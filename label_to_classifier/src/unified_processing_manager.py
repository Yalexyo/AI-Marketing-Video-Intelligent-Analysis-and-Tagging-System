#!/usr/bin/env python3
"""
ğŸš€ ç»Ÿä¸€å¤„ç†ç®¡ç†å™¨
æ•´åˆä¸»æ ‡ç­¾åˆ†ç±»å’ŒäºŒçº§èšç±»åŠŸèƒ½ï¼Œå®ç°å†…å­˜ä¸­çš„æµå¼æ•°æ®å¤„ç†
æ¶æ„ä¼˜åŒ–ç‰ˆï¼šé¿å…ä¸­é—´æ–‡ä»¶è¯»å†™ï¼Œæå‡æ€§èƒ½
"""

import json
import logging
import shutil
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Any
from collections import defaultdict

from .slice_file_manager import SliceFileManager
from .primary_ai_classifier import PrimaryAIClassifier
from .secondary_ai_classifier import SecondaryAIClassifier
from .tag_system_manager import TagSystemManager
from .data_classes import EnhancedClusterInfo, EnhancedClusterResult
from .unified_config_manager import get_unified_config_manager

# è®¾ç½®æ—¥å¿—
logger = logging.getLogger(__name__)


class UnifiedProcessingManager:
    """ğŸš€ ç»Ÿä¸€å¤„ç†ç®¡ç†å™¨ - å†…å­˜ä¸­æµå¼å¤„ç†ï¼Œé¿å…ä¸­é—´æ–‡ä»¶è¯»å†™"""
    
    def __init__(self, slice_base_dir: Optional[str] = None):
        """åˆå§‹åŒ–ç»Ÿä¸€å¤„ç†ç®¡ç†å™¨"""
        # ğŸ”§ åŠ è½½ç»Ÿä¸€é…ç½®
        self.config_manager = get_unified_config_manager()
        self.config = self.config_manager.get_config()
        
        # ä½¿ç”¨é…ç½®è®¾ç½®ç›®å½•
        self.slice_base_dir = Path(slice_base_dir or self.config.processing.slice_base_dir)
        
        # ğŸ”§ åˆå§‹åŒ–æ ¸å¿ƒç»„ä»¶
        try:
            self.file_manager = SliceFileManager(str(self.slice_base_dir))
            self.primary_classifier = PrimaryAIClassifier()
            self.secondary_classifier = SecondaryAIClassifier()
            self.tag_manager = TagSystemManager()
            
            logger.info("âœ… ç»Ÿä¸€å¤„ç†ç®¡ç†å™¨ç»„ä»¶åˆå§‹åŒ–æˆåŠŸ")
            logger.info(f"ğŸ“ ä½¿ç”¨åˆ‡ç‰‡ç›®å½•: {self.slice_base_dir}")
            logger.info(f"âš™ï¸ ç½®ä¿¡åº¦é˜ˆå€¼: {self.config.processing.min_confidence_threshold}")
        except Exception as e:
            logger.error(f"âŒ ç»„ä»¶åˆå§‹åŒ–å¤±è´¥: {e}")
            raise
        
        # 4å¤§ä¸»æ¨¡å—æ˜ å°„
        self.MAIN_MODULES = {
            "ğŸª é’©å­": {
                "description": "å®å®å“­é—¹ã€å®¶é•¿ç„¦è™‘ã€å–‚å…»å›°æ‰°ã€ä¸“å®¶å»ºè®®ã€é—®é¢˜è§£å†³",
                "folder_name": "ğŸª_é’©å­"
            },
            "ğŸ¼ äº§å“ä»‹ç»_è•´æ·³": {
                "description": "å¯èµ‹è•´æ·³äº§å“å±•ç¤ºã€HMOæ¯ä¹³ä½èšç³–ã€OPNæ´»æ€§è›‹ç™½ã€è¥å…»ç§‘å­¦ã€æƒ æ°èƒŒæ™¯",
                "folder_name": "ğŸ¼_äº§å“ä»‹ç»_è•´æ·³"
            },
            "ğŸ¼ äº§å“ä»‹ç»_æ°´å¥¶": {
                "description": "å¯èµ‹æ°´å¥¶å±•ç¤ºã€ä¾¿æºè£…ç‰¹æ€§ã€A2å¥¶æºã€å³é¥®æ–¹ä¾¿ã€æ–°é²œå“è´¨",
                "folder_name": "ğŸ¼_äº§å“ä»‹ç»_æ°´å¥¶"
            },
            "ğŸ¼ äº§å“ä»‹ç»_è“é’»": {
                "description": "å¯èµ‹è“é’»é«˜ç«¯ç³»åˆ—ã€å‡çº§é…æ–¹ã€é¡¶çº§å“è´¨ã€æ——èˆ°äº§å“",
                "folder_name": "ğŸ¼_äº§å“ä»‹ç»_è“é’»"
            },
            "ğŸŒŸ ä½¿ç”¨æ•ˆæœ": {
                "description": "å®å®æ´»æ³¼ã€æ•ˆæœå±•ç¤ºã€æ»¡æ„åé¦ˆã€å¥åº·å‘è‚²ã€å¿«ä¹ç©è€",
                "folder_name": "ğŸŒŸ_ä½¿ç”¨æ•ˆæœ"
            },
            "ğŸ ä¿ƒé”€æœºåˆ¶": {
                "description": "äº²å­äº’åŠ¨ã€æ¸©é¦¨åœºæ™¯ã€å®¶åº­å’Œè°ã€æƒ…æ„Ÿè¿æ¥ã€æ¨èå¼•å¯¼",
                "folder_name": "ğŸ_ä¿ƒé”€æœºåˆ¶"
            }
        }
        
        # å¤„ç†ç»Ÿè®¡
        self.processing_stats = {
            "total_files": 0,
            "classified_by_primary": 0,
            "classified_by_secondary": 0,
            "unclassified": 0,
            "failed": 0,
            "skipped": 0
        }
        
        # AIåˆ†æç»Ÿè®¡
        self.ai_analysis_stats = {
            "primary_ai_calls": 0,
            "secondary_ai_calls": 0,
            "successful_primary_classifications": 0,
            "successful_secondary_classifications": 0,
            "failed_classifications": 0,
            "low_confidence_classifications": 0
        }
        
        logger.info("âœ… ç»Ÿä¸€å¤„ç†ç®¡ç†å™¨åˆå§‹åŒ–å®Œæˆ - å†…å­˜æµå¼å¤„ç†æ¨¡å¼")
    
    def perform_unified_classification_and_clustering(self, 
                                                     force_reprocess: bool = False,
                                                     output_base_dir: Optional[Path] = None) -> EnhancedClusterResult:
        """
        ğŸš€ ç»Ÿä¸€æ™ºèƒ½åˆ†ç±»å’Œèšç±» - å†…å­˜ä¸­æµå¼å¤„ç†
        ç¬¬ä¸€å±‚ï¼šä¸»æ ‡ç­¾AIåˆ†ç±» + ç¬¬äºŒå±‚ï¼šæ™ºèƒ½å­ç±»åˆ«èšç±»
        """
        try:
            if not output_base_dir:
                base_output_dir = Path(self.config.processing.output_base_dir)
                output_base_dir = base_output_dir / f"ç»Ÿä¸€AIåˆ†ç±»v4_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
            
            output_base_dir.mkdir(parents=True, exist_ok=True)
            logger.info(f"ğŸš€ å¼€å§‹ç»Ÿä¸€æ™ºèƒ½åˆ†ç±»å¤„ç†ï¼Œè¾“å‡ºç›®å½•: {output_base_dir}")
            
            # ç¬¬ä¸€æ­¥ï¼šæ”¶é›†æ‰€æœ‰åˆ‡ç‰‡æ•°æ®
            all_slice_data = self._collect_all_slice_data_for_processing()
            
            if not all_slice_data:
                logger.warning("âš ï¸ æ²¡æœ‰æ‰¾åˆ°ä»»ä½•åˆ‡ç‰‡æ•°æ®")
                return EnhancedClusterResult({}, {}, [], [], {}, {}, {})
            
            logger.info(f"ğŸ“Š å…±æ”¶é›†åˆ° {len(all_slice_data)} ä¸ªåˆ‡ç‰‡æ–‡ä»¶")
            
            # ç¬¬äºŒæ­¥ï¼šğŸ¤– ä¸»æ ‡ç­¾AIåˆ†ç±»ï¼ˆå†…å­˜ä¸­å¤„ç†ï¼‰
            classified_data, unclassified_data = self._perform_primary_classification_in_memory(
                all_slice_data, force_reprocess
            )
            
            logger.info(f"ğŸ“Š ä¸»æ ‡ç­¾åˆ†ç±»å®Œæˆ: åˆ†ç±» {len(classified_data)} ä¸ªï¼Œæœªåˆ†ç±» {len(unclassified_data)} ä¸ª")
            
            # ç¬¬ä¸‰æ­¥ï¼šå¤„ç†æœªåˆ†ç±»æ•°æ®
            if unclassified_data:
                self._create_unclassified_folder(unclassified_data, output_base_dir)
            
            # ç¬¬å››æ­¥ï¼šæŒ‰ä¸»æ ‡ç­¾åˆ†ç»„
            main_tag_groups = self._group_by_main_tag(classified_data)
            
            # ç¬¬äº”æ­¥ï¼šğŸ¤– äºŒçº§AIæ™ºèƒ½èšç±»ï¼ˆå†…å­˜ä¸­å¤„ç†ï¼‰
            main_modules = {}
            cluster_mapping = {}
            
            for main_tag, slices in main_tag_groups.items():
                if not slices:
                    continue
                
                logger.info(f"ğŸ¤– å¤„ç†ä¸»æ¨¡å—äºŒçº§åˆ†ç±»: {main_tag} ({len(slices)} ä¸ªåˆ‡ç‰‡)")
                
                # åˆ›å»ºä¸»æ¨¡å—æ–‡ä»¶å¤¹
                main_folder = output_base_dir / self.MAIN_MODULES[main_tag]["folder_name"]
                main_folder.mkdir(parents=True, exist_ok=True)
                
                # ğŸ¤– è¿›è¡ŒäºŒçº§AIæ™ºèƒ½èšç±»ï¼ˆå†…å­˜ä¸­å¤„ç†ï¼‰
                sub_clusters = self._perform_secondary_classification_in_memory(
                    main_tag, slices, main_folder
                )
                
                main_modules[main_tag] = sub_clusters
                
                # è®°å½•æ˜ å°„å…³ç³»
                for cluster in sub_clusters:
                    for slice_data in slices:
                        slice_file = slice_data.get("file_path", "")
                        if slice_file:
                            cluster_mapping[slice_file] = cluster.cluster_id
            
            # ç¬¬å…­æ­¥ï¼šç”Ÿæˆå…ƒæ•°æ®å’ŒæŠ¥å‘Š
            metadata = self._generate_unified_metadata(main_modules, output_base_dir, classified_data, unclassified_data)
            
            # åˆ›å»ºç»“æœ
            cluster_result = EnhancedClusterResult(
                main_modules=main_modules,
                cluster_mapping=cluster_mapping,
                unclustered_slices=[],
                unclassified_slices=unclassified_data,
                metadata=metadata,
                processing_stats=self.processing_stats,
                ai_analysis_stats=self.ai_analysis_stats
            )
            
            # å¯¼å‡ºæŠ¥å‘Š
            self._export_unified_report(cluster_result, output_base_dir)
            
            logger.info(f"âœ… ç»Ÿä¸€æ™ºèƒ½åˆ†ç±»å®Œæˆï¼Œå…±ç”Ÿæˆ {len(main_modules)} ä¸ªä¸»æ¨¡å—")
            logger.info(f"ğŸ¤– AIåˆ†æç»Ÿè®¡: {self.ai_analysis_stats}")
            return cluster_result
            
        except Exception as e:
            logger.error(f"âŒ ç»Ÿä¸€æ™ºèƒ½åˆ†ç±»å¤±è´¥: {e}")
            return EnhancedClusterResult({}, {}, [], [], {}, {}, {})
    
    def _collect_all_slice_data_for_processing(self) -> List[Dict[str, Any]]:
        """æ”¶é›†æ‰€æœ‰åˆ‡ç‰‡æ•°æ®ï¼ŒåŒ…å«æ–‡ä»¶è·¯å¾„å’ŒJSONæ•°æ®ï¼Œå¹¶è¿‡æ»¤æ— æ•ˆæ–‡ä»¶"""
        all_data = []
        
        try:
            video_dirs = self.file_manager.get_all_video_directories()
            
            for video_name in video_dirs:
                json_files = self.file_manager.get_slice_json_files(video_name)
                
                for json_file in json_files:
                    json_data = self.file_manager.read_json_file(json_file)
                    if json_data:
                        # ğŸš¨ æ–°å¢ï¼šè´¨é‡è¿‡æ»¤é€»è¾‘
                        if self._should_filter_file(json_file, json_data):
                            logger.debug(f"ğŸš« è¿‡æ»¤æ–‡ä»¶: {json_file.name} (è´¨é‡é—®é¢˜)")
                            continue
                        
                        # ğŸ”§ ä¿®å¤ï¼šä»…å¯¹æœ‰æ•ˆæ–‡ä»¶ä½¿ç”¨æ™ºèƒ½è·¯å¾„è§£æ
                        resolved_file_path = self.file_manager._resolve_valid_video_file_path(json_file, json_data)
                        
                        # æ„å»ºå®Œæ•´çš„åˆ‡ç‰‡æ•°æ®ç»“æ„
                        slice_data = {
                            "slice_name": json_file.stem.replace("_analysis", ""),
                            "video_name": video_name,
                            "file_path": resolved_file_path,
                            "analysis_file": str(json_file),
                            "json_data": json_data,  # å†…å­˜ä¸­ä¿æŒJSONæ•°æ®
                            "labels": self.file_manager.extract_labels_for_classification(json_data),
                            "duration": json_data.get("duration", 0),
                            "object": json_data.get("object", ""),
                            "scene": json_data.get("scene", ""),
                            "emotion": json_data.get("emotion", ""),
                            "brand_elements": json_data.get("brand_elements", "")
                        }
                        all_data.append(slice_data)
                        self.processing_stats["total_files"] += 1
            
            logger.info(f"ğŸ“Š æ”¶é›†å®Œæˆ: å…± {len(all_data)} ä¸ªæœ‰æ•ˆåˆ‡ç‰‡æ–‡ä»¶")
            return all_data
            
        except Exception as e:
            logger.error(f"âŒ æ•°æ®æ”¶é›†å¤±è´¥: {e}")
            return []
    
    def _should_filter_file(self, json_file: Path, json_data: Dict[str, Any]) -> bool:
        """
        åˆ¤æ–­æ–‡ä»¶æ˜¯å¦éœ€è¦è¿‡æ»¤ï¼ˆä¾‹å¦‚ï¼Œå¸¦âŒå‰ç¼€ï¼Œæˆ–quality_statusä¸ºfailedï¼‰
        
        Args:
            json_file: JSONæ–‡ä»¶è·¯å¾„
            json_data: JSONæ•°æ®å†…å®¹
            
        Returns:
            bool: æ˜¯å¦éœ€è¦è¿‡æ»¤
        """
        # ğŸ¯ ç”¨æˆ·åé¦ˆï¼šå¤šé•œå¤´è§†é¢‘ä¹Ÿåº”è¯¥è¢«åˆ†æï¼Œåªè¿‡æ»¤çœŸæ­£å¤±è´¥çš„æ–‡ä»¶
        # åªè¿‡æ»¤âŒå‰ç¼€çš„æ–‡ä»¶ï¼ˆåˆ†æå¤±è´¥ï¼‰ï¼Œâ™»ï¸æ–‡ä»¶å…è®¸æ­£å¸¸åˆ†æ
        if json_file.stem.startswith("âŒ"):
            return True
        
        # æ£€æŸ¥quality_statusæ˜¯å¦ä¸ºfailed
        if json_data.get("quality_status") == "failed":
            return True
        
        # æ£€æŸ¥successå­—æ®µæ˜¯å¦ä¸ºfalse
        if json_data.get("success") == False:
            return True
        
        # æ£€æŸ¥æ–‡ä»¶è·¯å¾„æ˜¯å¦åŒ…å«âŒæ ‡è®°ï¼ˆâ™»ï¸æ ‡è®°å…è®¸é€šè¿‡ï¼‰
        file_path = json_data.get("file_path", "")
        if isinstance(file_path, str) and "âŒ" in file_path:
            return True
        
        return False
    
    def _perform_primary_classification_in_memory(self, 
                                                all_slice_data: List[Dict], 
                                                force_reprocess: bool = False) -> Tuple[List[Dict], List[Dict]]:
        """ğŸ¤– åœ¨å†…å­˜ä¸­æ‰§è¡Œä¸»æ ‡ç­¾åˆ†ç±»ï¼Œé¿å…æ–‡ä»¶è¯»å†™"""
        classified_data = []
        unclassified_data = []
        
        for slice_data in all_slice_data:
            try:
                json_data = slice_data["json_data"]
                
                # æ£€æŸ¥æ˜¯å¦å·²å¤„ç†ï¼ˆé™¤éå¼ºåˆ¶é‡æ–°å¤„ç†ï¼‰
                if not force_reprocess and self.file_manager.check_if_already_processed(json_data):
                    # å¦‚æœå·²ç»æœ‰ä¸»æ ‡ç­¾ï¼Œç›´æ¥ä½¿ç”¨
                    if json_data.get("main_tag"):
                        slice_data["main_tag"] = json_data["main_tag"]
                        slice_data["confidence"] = json_data.get("confidence", 0.0)
                        classified_data.append(slice_data)
                        self.processing_stats["skipped"] += 1
                        continue
                
                # æå–æ ‡ç­¾è¿›è¡Œåˆ†ç±»
                labels_text = slice_data["labels"]
                
                if not labels_text or labels_text.strip() == "":
                    # æ ‡è®°ä¸ºæœªåˆ†ç±»
                    slice_data["unclassified_reason"] = "è§†è§‰åˆ†ææ•°æ®ä¸ºç©ºæˆ–æ— æ•ˆ"
                    unclassified_data.append(slice_data)
                    self.processing_stats["unclassified"] += 1
                    continue
                
                # ğŸ¤– è°ƒç”¨ä¸»æ ‡ç­¾AIåˆ†ç±»å™¨
                classification_result = self.primary_classifier.classify_single_item({
                    "labels": labels_text,
                    "slice_name": slice_data["slice_name"]
                })
                
                self.ai_analysis_stats["primary_ai_calls"] += 1
                
                if classification_result and classification_result.get("success"):
                    main_tag = classification_result.get("main_tag", "")
                    confidence = classification_result.get("confidence", 0.0)
                    
                    # æ ‡å‡†åŒ–ä¸»æ ‡ç­¾
                    normalized_tag = self.tag_manager.normalize_main_tag(main_tag)
                    
                    if normalized_tag and confidence >= self.config.processing.min_confidence_threshold:
                        # æˆåŠŸåˆ†ç±»
                        slice_data["main_tag"] = normalized_tag
                        slice_data["confidence"] = confidence
                        slice_data["analysis"] = classification_result.get("analysis", {})
                        
                        # ğŸ”„ åŒæ—¶æ›´æ–°JSONæ–‡ä»¶ï¼ˆä¿æŒæ•°æ®ä¸€è‡´æ€§ï¼‰
                        self._update_json_file_with_classification(slice_data, classification_result)
                        
                        classified_data.append(slice_data)
                        self.ai_analysis_stats["successful_primary_classifications"] += 1
                        self.processing_stats["classified_by_primary"] += 1
                        
                        logger.debug(f"âœ… åˆ†ç±»æˆåŠŸ: {slice_data['slice_name']} -> {normalized_tag} ({confidence:.2f})")
                    else:
                        # ç½®ä¿¡åº¦ä¸è¶³
                        reason = f"ç½®ä¿¡åº¦ä¸è¶³ ({confidence:.2f})"
                        slice_data["unclassified_reason"] = reason
                        unclassified_data.append(slice_data)
                        self.ai_analysis_stats["low_confidence_classifications"] += 1
                        self.processing_stats["unclassified"] += 1
                else:
                    # AIåˆ†ç±»å¤±è´¥
                    slice_data["unclassified_reason"] = "AIåˆ†ç±»å™¨è°ƒç”¨å¤±è´¥"
                    unclassified_data.append(slice_data)
                    self.ai_analysis_stats["failed_classifications"] += 1
                    self.processing_stats["failed"] += 1
                
            except Exception as e:
                logger.error(f"âŒ å¤„ç†åˆ‡ç‰‡å¤±è´¥ {slice_data['slice_name']}: {e}")
                slice_data["unclassified_reason"] = f"å¤„ç†å¼‚å¸¸: {str(e)}"
                unclassified_data.append(slice_data)
                self.processing_stats["failed"] += 1
        
        logger.info(f"ğŸ“Š ä¸»æ ‡ç­¾åˆ†ç±»å®Œæˆ: æˆåŠŸ {len(classified_data)} ä¸ªï¼Œå¤±è´¥ {len(unclassified_data)} ä¸ª")
        return classified_data, unclassified_data
    
    def _group_by_main_tag(self, classified_data: List[Dict]) -> Dict[str, List[Dict]]:
        """æŒ‰ä¸»æ ‡ç­¾åˆ†ç»„æ•°æ®"""
        main_tag_groups = defaultdict(list)
        
        for slice_data in classified_data:
            main_tag = slice_data.get("main_tag", "")
            if main_tag in self.MAIN_MODULES:
                main_tag_groups[main_tag].append(slice_data)
            else:
                logger.warning(f"âš ï¸ æœªçŸ¥ä¸»æ ‡ç­¾: {main_tag}")
        
        return dict(main_tag_groups)
    
    def _perform_secondary_classification_in_memory(self, 
                                                  main_tag: str, 
                                                  slices: List[Dict], 
                                                  main_folder: Path) -> List[EnhancedClusterInfo]:
        """ğŸ¤– åœ¨å†…å­˜ä¸­æ‰§è¡ŒäºŒçº§AIåˆ†ç±»ï¼Œé¿å…æ–‡ä»¶è¯»å†™"""
        if not slices:
            return []
        
        try:
            # ğŸ¤– ä½¿ç”¨äºŒçº§AIåˆ†ç±»å™¨è¿›è¡Œæ‰¹é‡åˆ†ç±»
            enriched_slices = self.secondary_classifier.batch_classify_secondary(
                slices, main_tag, min_confidence=0.5
            )
            
            self.ai_analysis_stats["secondary_ai_calls"] += len(slices)
            
            # æŒ‰äºŒçº§åˆ†ç±»ç»“æœåˆ†ç»„
            secondary_groups = defaultdict(list)
            for slice_data in enriched_slices:
                secondary_category = slice_data.get("secondary_category", "")
                secondary_confidence = slice_data.get("secondary_confidence", 0.0)
                
                if secondary_confidence >= 0.5 and secondary_category:
                    self.ai_analysis_stats["successful_secondary_classifications"] += 1
                    secondary_groups[secondary_category].append(slice_data)
                    
                    # ğŸ”„ æ›´æ–°JSONæ–‡ä»¶
                    self._update_json_file_with_secondary_classification(slice_data)
                else:
                    # ä½ç½®ä¿¡åº¦åˆ†ç±»
                    self.ai_analysis_stats["low_confidence_classifications"] += 1
                    secondary_groups["ä½ç½®ä¿¡åº¦åˆ†ç±»"].append(slice_data)
            
            # ä¸ºæ¯ä¸ªäºŒçº§åˆ†ç±»åˆ›å»ºèšç±»ä¿¡æ¯
            sub_clusters = []
            for secondary_category, category_slices in secondary_groups.items():
                if category_slices:
                    cluster_info = self._create_cluster_info_from_memory(
                        main_tag, secondary_category, category_slices, main_folder
                    )
                    sub_clusters.append(cluster_info)
                    
                    # ç»„ç»‡æ–‡ä»¶åˆ°æ–‡ä»¶å¤¹
                    self._organize_cluster_files(category_slices, main_folder, secondary_category)
            
            logger.info(f"âœ… {main_tag} äºŒçº§åˆ†ç±»å®Œæˆ: {len(sub_clusters)} ä¸ªå­ç±»åˆ«")
            return sub_clusters
            
        except Exception as e:
            logger.error(f"âŒ {main_tag} äºŒçº§åˆ†ç±»å¤±è´¥: {e}")
            return self._fallback_to_simple_grouping(main_tag, slices, main_folder)
    
    def _update_json_file_with_classification(self, slice_data: Dict, classification_result: Dict):
        """æ›´æ–°JSONæ–‡ä»¶çš„ä¸»æ ‡ç­¾åˆ†ç±»ç»“æœ"""
        try:
            analysis_file = Path(slice_data["analysis_file"])
            if analysis_file.exists():
                json_data = slice_data["json_data"]
                
                # æ›´æ–°ä¸»æ ‡ç­¾å­—æ®µ
                json_data.update({
                    "main_tag": classification_result.get("main_tag", ""),
                    "confidence": classification_result.get("confidence", 0.0),
                    "analysis": classification_result.get("analysis", {}),
                    "main_tag_processed_at": datetime.now().isoformat()
                })
                
                # å†™å›æ–‡ä»¶
                with open(analysis_file, 'w', encoding='utf-8') as f:
                    json.dump(json_data, f, ensure_ascii=False, indent=2)
                    
        except Exception as e:
            logger.warning(f"âš ï¸ æ›´æ–°JSONæ–‡ä»¶å¤±è´¥ {slice_data['slice_name']}: {e}")
    
    def _update_json_file_with_secondary_classification(self, slice_data: Dict):
        """æ›´æ–°JSONæ–‡ä»¶çš„äºŒçº§åˆ†ç±»ç»“æœ"""
        try:
            analysis_file = Path(slice_data["analysis_file"])
            if analysis_file.exists():
                with open(analysis_file, 'r', encoding='utf-8') as f:
                    json_data = json.load(f)
                
                # æ›´æ–°äºŒçº§åˆ†ç±»å­—æ®µ
                json_data.update({
                    "secondary_category": slice_data.get("secondary_category", ""),
                    "secondary_confidence": slice_data.get("secondary_confidence", 0.0),
                    "secondary_reasoning": slice_data.get("secondary_reasoning", ""),
                    "secondary_features": slice_data.get("secondary_features", []),
                    "secondary_processed_at": datetime.now().isoformat()
                })
                
                # å†™å›æ–‡ä»¶
                with open(analysis_file, 'w', encoding='utf-8') as f:
                    json.dump(json_data, f, ensure_ascii=False, indent=2)
                    
        except Exception as e:
            logger.warning(f"âš ï¸ æ›´æ–°äºŒçº§åˆ†ç±»JSONæ–‡ä»¶å¤±è´¥ {slice_data['slice_name']}: {e}")
    
    def _create_cluster_info_from_memory(self, 
                                       main_tag: str, 
                                       secondary_category: str, 
                                       slices: List[Dict], 
                                       main_folder: Path) -> EnhancedClusterInfo:
        """ä»å†…å­˜æ•°æ®åˆ›å»ºèšç±»ä¿¡æ¯"""
        cluster_id = f"{main_tag}_{secondary_category}_{len(slices)}"
        
        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        total_duration = sum(s.get("duration", 0) for s in slices)
        avg_confidence = sum(s.get("confidence", 0) for s in slices) / len(slices)
        avg_secondary_confidence = sum(s.get("secondary_confidence", 0) for s in slices) / len(slices)
        
        # æå–ä»£è¡¨æ€§æ ‡ç­¾
        representative_tags = []
        for slice_data in slices[:3]:  # å–å‰3ä¸ªä½œä¸ºä»£è¡¨
            if slice_data.get("object"):
                representative_tags.append(f"ç‰©ä½“: {slice_data['object']}")
            if slice_data.get("scene"):
                representative_tags.append(f"åœºæ™¯: {slice_data['scene']}")
            if slice_data.get("emotion"):
                representative_tags.append(f"æƒ…æ„Ÿ: {slice_data['emotion']}")
        
        # AIæ¨ç†ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
        ai_reasoning = ""
        if slices and slices[0].get("secondary_reasoning"):
            ai_reasoning = slices[0]["secondary_reasoning"]
        
        return EnhancedClusterInfo(
            cluster_id=cluster_id,
            cluster_name=secondary_category,
            main_category=main_tag,
            sub_category=secondary_category,
            slice_count=len(slices),
            total_duration=total_duration,
            avg_confidence=avg_confidence,
            avg_secondary_confidence=avg_secondary_confidence,
            representative_tags=representative_tags,
            folder_path=str(main_folder),
            source_files=[s.get("file_path", "") for s in slices],
            ai_reasoning=ai_reasoning
        )
    
    def _organize_cluster_files(self, slices: List[Dict], main_folder: Path, category_name: str):
        """ç»„ç»‡èšç±»æ–‡ä»¶åˆ°æ–‡ä»¶å¤¹ - æ”¯æŒè¯­ä¹‰åŒ–å‘½åï¼Œç›´æ¥æ”¾ç½®åœ¨ä¸»ç›®å½•ä¸‹"""
        try:
            # ä¸åˆ›å»ºå­ç›®å½•ï¼Œç›´æ¥ä½¿ç”¨ä¸»ç›®å½•
            # ç”¨äºå¤„ç†æ–‡ä»¶åå†²çªçš„è®¡æ•°å™¨
            filename_counter = {}
            
            for slice_data in slices:
                # ç”Ÿæˆè¯­ä¹‰åŒ–æ–‡ä»¶å
                secondary_tag = slice_data.get("secondary_category", category_name)
                object_desc = slice_data.get("object", "è§†é¢‘ç‰‡æ®µ")
                
                # æ¸…ç†æ–‡ä»¶åä¸­çš„ç‰¹æ®Šå­—ç¬¦
                clean_tag = self._clean_filename(secondary_tag)
                clean_desc = self._clean_filename(object_desc)
                
                # é™åˆ¶æè¿°é•¿åº¦
                if len(clean_desc) > 30:
                    clean_desc = clean_desc[:27] + "..."
                
                # ç”ŸæˆåŸºç¡€æ–‡ä»¶å
                base_filename = f"{clean_tag}_{clean_desc}"
                
                # å¤„ç†æ–‡ä»¶åå†²çª
                if base_filename in filename_counter:
                    filename_counter[base_filename] += 1
                    final_filename = f"{base_filename}_{filename_counter[base_filename]}"
                else:
                    filename_counter[base_filename] = 0
                    final_filename = base_filename
                
                # å¤åˆ¶è§†é¢‘æ–‡ä»¶ - ç›´æ¥æ”¾åˆ°ä¸»ç›®å½•ä¸‹
                source_video = Path(slice_data.get("file_path", ""))
                if source_video.exists():
                    target_video = main_folder / f"{final_filename}.mp4"
                    shutil.copy2(source_video, target_video)
                    logger.debug(f"âœ… æ–‡ä»¶å·²å¤åˆ¶: {source_video.name} â†’ {final_filename}.mp4")
                
                # å¤åˆ¶åˆ†ææ–‡ä»¶ - ç›´æ¥æ”¾åˆ°ä¸»ç›®å½•ä¸‹
                source_analysis = Path(slice_data.get("analysis_file", ""))
                if source_analysis.exists():
                    target_analysis = main_folder / f"{final_filename}_analysis.json"
                    shutil.copy2(source_analysis, target_analysis)
                    
        except Exception as e:
            logger.warning(f"âš ï¸ ç»„ç»‡æ–‡ä»¶å¤±è´¥ {category_name}: {e}")
    
    def _clean_filename(self, text: str) -> str:
        """æ¸…ç†æ–‡ä»¶åä¸­çš„ç‰¹æ®Šå­—ç¬¦"""
        if not text:
            return "æœªçŸ¥"
        
        # ç§»é™¤æˆ–æ›¿æ¢ç‰¹æ®Šå­—ç¬¦
        import re
        # æ›¿æ¢Windowså’ŒUnixæ–‡ä»¶ç³»ç»Ÿä¸æ”¯æŒçš„å­—ç¬¦
        cleaned = re.sub(r'[<>:"/\\|?*]', '_', text)
        # ç§»é™¤å¤šä½™çš„ç©ºæ ¼å’Œæ ‡ç‚¹
        cleaned = re.sub(r'\s+', '_', cleaned.strip())
        # ç§»é™¤å¼€å¤´å’Œç»“å°¾çš„ä¸‹åˆ’çº¿
        cleaned = cleaned.strip('_')
        
        return cleaned if cleaned else "æœªçŸ¥"
    
    def _create_unclassified_folder(self, unclassified_data: List[Dict], output_dir: Path):
        """åˆ›å»ºæœªåˆ†ç±»æ–‡ä»¶å¤¹"""
        if not unclassified_data:
            return
        
        misc_folder = output_dir / "ğŸ§«å…¶ä»–"
        misc_folder.mkdir(parents=True, exist_ok=True)
        
        logger.info(f"ğŸ“ åˆ›å»ºæœªåˆ†ç±»æ–‡ä»¶å¤¹: ğŸ§«å…¶ä»– ({len(unclassified_data)} ä¸ªåˆ‡ç‰‡)")
        
        # å¤åˆ¶æ–‡ä»¶åˆ°æœªåˆ†ç±»æ–‡ä»¶å¤¹
        for slice_data in unclassified_data:
            self._organize_cluster_files([slice_data], output_dir, "ğŸ§«å…¶ä»–")
        
        # ç”Ÿæˆæœªåˆ†ç±»æ¦‚è§ˆ
        self._generate_unclassified_overview(unclassified_data, misc_folder)
    
    def _generate_unclassified_overview(self, unclassified_data: List[Dict], folder: Path):
        """ç”Ÿæˆæœªåˆ†ç±»æ¦‚è§ˆæ–‡ä»¶"""
        overview_file = folder / "ğŸ“‹_æœªåˆ†ç±»åŸå› åˆ†æ.json"
        
        # ç»Ÿè®¡æœªåˆ†ç±»åŸå› 
        reason_stats = defaultdict(int)
        for slice_data in unclassified_data:
            reason = slice_data.get("unclassified_reason", "æœªçŸ¥åŸå› ")
            reason_stats[reason] += 1
        
        overview_data = {
            'category_name': 'æœªåˆ†ç±»ç‰‡æ®µ',
            'slice_count': len(unclassified_data),
            'created_at': datetime.now().isoformat(),
            'reason_statistics': dict(reason_stats),
            'processing_stats': self.processing_stats,
            'ai_analysis_stats': self.ai_analysis_stats,
            'slices': [
                {
                    'slice_name': s.get("slice_name", ""),
                    'video_name': s.get("video_name", ""),
                    'file_path': s.get("file_path", ""),
                    'unclassified_reason': s.get("unclassified_reason", ""),
                    'duration': s.get("duration", 0),
                    'labels': s.get("labels", "")
                }
                for s in unclassified_data
            ],
            'recommendations': [
                "æ£€æŸ¥è§†è§‰åˆ†ææ•°æ®çš„å®Œæ•´æ€§",
                "ç¡®è®¤æ–‡ä»¶æ˜¯å¦æŸåæˆ–æ ¼å¼å¼‚å¸¸", 
                "è€ƒè™‘é‡æ–°è¿è¡Œä¸»æ ‡ç­¾åˆ†ç±»",
                "æ£€æŸ¥æ˜¯å¦éœ€è¦æ–°å¢ä¸»æ ‡ç­¾ç±»åˆ«"
            ]
        }
        
        with open(overview_file, 'w', encoding='utf-8') as f:
            json.dump(overview_data, f, ensure_ascii=False, indent=2)
        
        logger.info(f"ğŸ“Š æœªåˆ†ç±»åŸå› åˆ†æå·²ç”Ÿæˆ: {overview_file}")
    
    def _fallback_to_simple_grouping(self, main_tag: str, slices: List[Dict], main_folder: Path) -> List[EnhancedClusterInfo]:
        """å›é€€åˆ°ç®€å•åˆ†ç»„"""
        logger.warning(f"âš ï¸ {main_tag} ä½¿ç”¨ç®€å•åˆ†ç»„å›é€€æ–¹æ¡ˆ")
        
        cluster_info = self._create_cluster_info_from_memory(
            main_tag, "é€šç”¨åˆ†ç»„", slices, main_folder
        )
        
        # ç»„ç»‡æ–‡ä»¶
        self._organize_cluster_files(slices, main_folder, "é€šç”¨åˆ†ç»„")
        
        return [cluster_info]
    
    def _generate_unified_metadata(self, main_modules: Dict, output_dir: Path, 
                                 classified_data: List[Dict], unclassified_data: List[Dict]) -> Dict:
        """ç”Ÿæˆç»Ÿä¸€å¤„ç†å…ƒæ•°æ®"""
        return {
            "processing_mode": "unified_in_memory",
            "created_at": datetime.now().isoformat(),
            "output_directory": str(output_dir),
            "total_main_modules": len(main_modules),
            "total_classified_slices": len(classified_data),
            "total_unclassified_slices": len(unclassified_data),
            "processing_stats": self.processing_stats,
            "ai_analysis_stats": self.ai_analysis_stats,
            "main_modules_summary": {
                tag: {
                    "cluster_count": len(clusters),
                    "total_slices": sum(c.slice_count for c in clusters)
                }
                for tag, clusters in main_modules.items()
            }
        }
    
    def _export_unified_report(self, cluster_result: EnhancedClusterResult, output_dir: Path):
        """å¯¼å‡ºç»Ÿä¸€å¤„ç†æŠ¥å‘Š"""
        report_file = output_dir / "ğŸ“Š_ç»Ÿä¸€å¤„ç†æŠ¥å‘Š.json"
        
        report_data = {
            "processing_mode": "unified_in_memory",
            "created_at": datetime.now().isoformat(),
            "summary": {
                "total_main_modules": len(cluster_result.main_modules),
                "total_clusters": sum(len(clusters) for clusters in cluster_result.main_modules.values()),
                "total_slices": sum(c.slice_count for clusters in cluster_result.main_modules.values() for c in clusters),
                "unclassified_slices": len(cluster_result.unclassified_slices)
            },
            "processing_stats": cluster_result.processing_stats,
            "ai_analysis_stats": cluster_result.ai_analysis_stats,
            "main_modules": {
                tag: [
                    {
                        "cluster_name": c.cluster_name,
                        "slice_count": c.slice_count,
                        "avg_confidence": c.avg_confidence,
                        "avg_secondary_confidence": c.avg_secondary_confidence,
                        "representative_tags": c.representative_tags
                    }
                    for c in clusters
                ]
                for tag, clusters in cluster_result.main_modules.items()
            }
        }
        
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(report_data, f, ensure_ascii=False, indent=2)
        
        logger.info(f"ğŸ“Š ç»Ÿä¸€å¤„ç†æŠ¥å‘Šå·²ç”Ÿæˆ: {report_file}") 