#!/usr/bin/env python3
"""
AI Video Master 5.0 - å¹¶è¡Œæ‰¹é‡è§†é¢‘å¤„ç†å™¨ (ç²¾ç®€ç‰ˆ)
ä¸“æ³¨äºå¹¶è¡Œå¤„ç†ï¼Œç§»é™¤æ‰€æœ‰ä¸²è¡Œå¤„ç†ä»£ç 

ä¸»è¦ç‰¹æ€§:
1. å¼‚æ­¥å¹¶è¡Œå¤„ç†å¤šä¸ªè§†é¢‘æ–‡ä»¶
2. ä¿¡å·é‡æ§åˆ¶å¹¶å‘æ•°é‡ï¼ˆéµå¾ªAPIé…é¢é™åˆ¶ï¼‰
3. FFmpegå¹¶è¡Œåˆ‡ç‰‡ä¼˜åŒ–
4. å®æ—¶è¿›åº¦ç›‘æ§å’Œé”™è¯¯å¤„ç†
5. é‡è¯•æœºåˆ¶å’Œå®¹é”™å¤„ç†
6. è¯¦ç»†çš„æ€§èƒ½ç»Ÿè®¡æŠ¥å‘Š
7. æœ¬åœ°è½¬åœºæ£€æµ‹ï¼ˆæ— éœ€äº‘æœåŠ¡ï¼‰

æ·»åŠ è¯­ä¹‰åˆå¹¶åŠŸèƒ½ï¼Œæ”¯æŒå°†ç›¸å…³æ€§å¼ºçš„ç‰‡æ®µè¿›è¡Œæ™ºèƒ½æ•´åˆ
"""

import asyncio
import json
import logging
import os
import sys
import time
import argparse
import subprocess
import tempfile
from datetime import datetime
from pathlib import Path
from typing import List, Dict, Any, Optional
from concurrent.futures import ThreadPoolExecutor, as_completed

from tenacity import retry, wait_random_exponential, stop_after_attempt

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler('parallel_video_slice.log', encoding='utf-8')
    ]
)
logger = logging.getLogger(__name__)

try:
    from google_video_analyzer import GoogleVideoAnalyzer
    from parallel_video_slicer import ParallelVideoSlicer
    from semantic_segment_merger import SemanticSegmentMerger
except ImportError as e:
    logger.error(f"ä¾èµ–æ¨¡å—å¯¼å…¥å¤±è´¥: {e}")
    logger.error("è¯·ç¡®ä¿æ‰€æœ‰ä¾èµ–æ–‡ä»¶åœ¨åŒä¸€ç›®å½•ä¸‹")
    sys.exit(1)


class LocalSceneDetector:
    """æœ¬åœ°è½¬åœºæ£€æµ‹å™¨ - åŸºäºFFmpegå’Œå›¾åƒå·®å¼‚åˆ†æ"""
    
    def __init__(self, threshold: float = 0.4, min_scene_duration: float = 2.0):
        """
        åˆå§‹åŒ–æœ¬åœ°è½¬åœºæ£€æµ‹å™¨
        
        Args:
            threshold: è½¬åœºæ£€æµ‹é˜ˆå€¼ (0.1-1.0ï¼Œè¶Šå°è¶Šæ•æ„Ÿ)
            min_scene_duration: æœ€å°é•œå¤´æ—¶é•¿ï¼ˆç§’ï¼‰
        """
        self.threshold = threshold
        self.min_scene_duration = min_scene_duration
        
    def detect_scenes(self, video_path: str, progress_callback: Optional[callable] = None) -> List[Dict[str, Any]]:
        """
        æ£€æµ‹è§†é¢‘ä¸­çš„è½¬åœºç‚¹
        
        Args:
            video_path: è§†é¢‘æ–‡ä»¶è·¯å¾„
            progress_callback: è¿›åº¦å›è°ƒå‡½æ•°
            
        Returns:
            é•œå¤´åˆ—è¡¨ï¼Œæ¯ä¸ªåŒ…å«start_time, end_time, durationç­‰ä¿¡æ¯
        """
        try:
            if progress_callback:
                progress_callback(10, "å¼€å§‹æœ¬åœ°è½¬åœºæ£€æµ‹...")
            
            # 1. è·å–è§†é¢‘æ—¶é•¿
            duration = self._get_video_duration(video_path)
            if duration <= 0:
                logger.error(f"æ— æ³•è·å–è§†é¢‘æ—¶é•¿: {video_path}")
                return []
            
            if progress_callback:
                progress_callback(20, f"è§†é¢‘æ—¶é•¿: {duration:.1f}ç§’ï¼Œåˆ†æè½¬åœº...")
            
            # 2. ä½¿ç”¨FFmpegçš„sceneæ£€æµ‹æ»¤é•œ
            scene_changes = self._detect_with_ffmpeg(video_path, progress_callback)
            
            if not scene_changes:
                logger.warning("FFmpegè½¬åœºæ£€æµ‹æœªæ‰¾åˆ°åˆ‡æ¢ç‚¹ï¼Œä½¿ç”¨æ™ºèƒ½é»˜è®¤åˆ‡ç‰‡")
                return self._create_smart_default_shots(video_path, duration)
            
            if progress_callback:
                progress_callback(80, f"æ£€æµ‹åˆ° {len(scene_changes)} ä¸ªè½¬åœºç‚¹ï¼Œç”Ÿæˆé•œå¤´...")
            
            # 3. è½¬æ¢ä¸ºé•œå¤´åˆ—è¡¨
            shots = self._convert_to_shots(scene_changes, duration)
            
            # 4. åˆå¹¶è¿‡çŸ­çš„é•œå¤´
            shots = self._merge_short_scenes(shots)
            
            if progress_callback:
                progress_callback(100, f"è½¬åœºæ£€æµ‹å®Œæˆ: {len(shots)} ä¸ªé•œå¤´")
            
            logger.info(f"âœ… æœ¬åœ°è½¬åœºæ£€æµ‹å®Œæˆ: {len(shots)} ä¸ªé•œå¤´")
            return shots
            
        except Exception as e:
            logger.error(f"æœ¬åœ°è½¬åœºæ£€æµ‹å¤±è´¥: {e}")
            # å…œåº•æ–¹æ¡ˆï¼šæ™ºèƒ½é»˜è®¤åˆ‡ç‰‡
            duration = self._get_video_duration(video_path)
            return self._create_smart_default_shots(video_path, duration)
    
    def _get_video_duration(self, video_path: str) -> float:
        """è·å–è§†é¢‘æ—¶é•¿"""
        try:
            cmd = [
                "ffprobe", "-v", "quiet", "-show_entries", "format=duration",
                "-of", "csv=p=0", video_path
            ]
            result = subprocess.run(cmd, capture_output=True, text=True, timeout=30)
            
            if result.returncode == 0:
                return float(result.stdout.strip())
            else:
                logger.error(f"ffprobeè·å–æ—¶é•¿å¤±è´¥: {result.stderr}")
                return 0
                
        except Exception as e:
            logger.error(f"è·å–è§†é¢‘æ—¶é•¿å¼‚å¸¸: {e}")
            return 0
    
    def _detect_with_ffmpeg(self, video_path: str, progress_callback: Optional[callable] = None) -> List[float]:
        """ä½¿ç”¨FFmpegçš„sceneæ£€æµ‹æ»¤é•œ"""
        try:
            # åˆ›å»ºä¸´æ—¶æ–‡ä»¶ä¿å­˜æ£€æµ‹ç»“æœ
            with tempfile.NamedTemporaryFile(mode='w+', suffix='.txt', delete=False) as temp_file:
                temp_path = temp_file.name
            
            # FFmpeg sceneæ£€æµ‹å‘½ä»¤
            cmd = [
                "ffmpeg", "-i", video_path,
                "-filter:v", f"select='gt(scene,{self.threshold})',showinfo",
                "-f", "null", "-",
                "-v", "info"
            ]
            
            if progress_callback:
                progress_callback(30, "æ‰§è¡ŒFFmpegè½¬åœºåˆ†æ...")
            
            # æ‰§è¡ŒFFmpegå‘½ä»¤
            result = subprocess.run(
                cmd, 
                capture_output=True, 
                text=True, 
                timeout=300  # 5åˆ†é’Ÿè¶…æ—¶
            )
            
            if progress_callback:
                progress_callback(60, "è§£æè½¬åœºæ£€æµ‹ç»“æœ...")
            
            # è§£æè¾“å‡ºä¸­çš„æ—¶é—´æˆ³
            scene_times = []
            for line in result.stderr.split('\n'):
                if 'pts_time:' in line:
                    try:
                        # æå–æ—¶é—´æˆ³: pts_time:12.345
                        time_str = line.split('pts_time:')[1].split()[0]
                        scene_time = float(time_str)
                        scene_times.append(scene_time)
                    except (IndexError, ValueError):
                        continue
            
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            try:
                os.unlink(temp_path)
            except:
                pass
            
            # å»é‡å¹¶æ’åº
            scene_times = sorted(list(set(scene_times)))
            
            logger.info(f"FFmpegæ£€æµ‹åˆ° {len(scene_times)} ä¸ªè½¬åœºç‚¹")
            return scene_times
            
        except subprocess.TimeoutExpired:
            logger.error("FFmpegè½¬åœºæ£€æµ‹è¶…æ—¶")
            return []
        except Exception as e:
            logger.error(f"FFmpegè½¬åœºæ£€æµ‹å¼‚å¸¸: {e}")
            return []
    
    def _convert_to_shots(self, scene_times: List[float], total_duration: float) -> List[Dict[str, Any]]:
        """å°†è½¬åœºæ—¶é—´ç‚¹è½¬æ¢ä¸ºé•œå¤´åˆ—è¡¨"""
        shots = []
        
        # æ·»åŠ å¼€å§‹æ—¶é—´
        times = [0.0] + scene_times + [total_duration]
        times = sorted(list(set(times)))  # å»é‡æ’åº
        
        for i in range(len(times) - 1):
            start_time = times[i]
            end_time = times[i + 1]
            duration = end_time - start_time
            
            if duration > 1.5:  # è‡³å°‘1.5ç§’
                shots.append({
                    'index': len(shots) + 1,
                    'start_time': start_time,
                    'end_time': end_time,
                    'duration': duration,
                    'type': f'é•œå¤´{len(shots) + 1}',
                    'confidence': 0.8
                })
        
        return shots
    
    def _merge_short_scenes(self, shots: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """æ”¹è¿›çš„é•œå¤´åˆå¹¶ç®—æ³• - æ ¹æ®min_scene_durationå†³å®šæ˜¯å¦åˆå¹¶"""
        if not shots:
            return shots
        
        # å¦‚æœmin_scene_durationå¾ˆå°(<=0.5)ï¼Œè¯´æ˜ç”¨æˆ·æƒ³è¦ä¸¥æ ¼æŒ‰è½¬åœºç‚¹åˆ‡åˆ†ï¼Œè·³è¿‡åˆå¹¶
        if self.min_scene_duration <= 0.5:
            logger.info(f"ä¸¥æ ¼è½¬åœºæ¨¡å¼: ä¿æŒ {len(shots)} ä¸ªåŸå§‹é•œå¤´ï¼Œä¸è¿›è¡Œåˆå¹¶")
            # åªé‡æ–°ç¼–å·ï¼Œä¸åˆå¹¶
            for i, shot in enumerate(shots):
                shot['index'] = i + 1
                shot['type'] = f"é•œå¤´{i + 1}"
            return shots
        
        # æ­£å¸¸åˆå¹¶é€»è¾‘
        merged_shots = []
        i = 0
        
        while i < len(shots):
            current_shot = shots[i].copy()
            
            # å‘å‰åˆå¹¶æ‰€æœ‰ç›¸é‚»çš„çŸ­é•œå¤´
            while (i + 1 < len(shots) and 
                   (current_shot['duration'] < self.min_scene_duration or
                    shots[i + 1]['duration'] < self.min_scene_duration)):
                
                next_shot = shots[i + 1]
                current_shot['end_time'] = next_shot['end_time']
                current_shot['duration'] = current_shot['end_time'] - current_shot['start_time']
                i += 1
                
                # å¦‚æœåˆå¹¶åçš„ç‰‡æ®µå·²ç»å¤Ÿé•¿ï¼Œå°±åœæ­¢åˆå¹¶
                if current_shot['duration'] >= self.min_scene_duration * 1.5:
                    break
            
            # é‡æ–°ç¼–å·å’Œå‘½å
            current_shot['index'] = len(merged_shots) + 1
            current_shot['type'] = f"é•œå¤´{len(merged_shots) + 1}"
            merged_shots.append(current_shot)
            i += 1
        
        logger.info(f"é•œå¤´åˆå¹¶: {len(shots)} -> {len(merged_shots)} ä¸ªé•œå¤´")
        return merged_shots
    
    def _create_smart_default_shots(self, video_path: str, duration: float) -> List[Dict[str, Any]]:
        """åˆ›å»ºæ™ºèƒ½é»˜è®¤åˆ‡ç‰‡ï¼ˆæ¯”å›ºå®šæ—¶é—´åˆ‡ç‰‡æ›´åˆç†ï¼‰"""
        shots = []
        
        if duration <= 0:
            return shots
        
        # æ ¹æ®è§†é¢‘é•¿åº¦åŠ¨æ€è°ƒæ•´ç‰‡æ®µæ—¶é•¿
        if duration <= 30:
            segment_duration = 5.0    # çŸ­è§†é¢‘ï¼š5ç§’ä¸€æ®µ
        elif duration <= 120:
            segment_duration = 10.0   # ä¸­ç­‰è§†é¢‘ï¼š10ç§’ä¸€æ®µ
        elif duration <= 300:
            segment_duration = 15.0   # é•¿è§†é¢‘ï¼š15ç§’ä¸€æ®µ
        else:
            segment_duration = 20.0   # è¶…é•¿è§†é¢‘ï¼š20ç§’ä¸€æ®µ
        
        current_time = 0
        index = 1
        
        while current_time < duration:
            end_time = min(current_time + segment_duration, duration)
            
            shots.append({
                'index': index,
                'start_time': current_time,
                'end_time': end_time,
                'duration': end_time - current_time,
                'type': f'ç‰‡æ®µ{index}',
                'confidence': 0.6  # è¾ƒä½ç½®ä¿¡åº¦ï¼Œè¡¨ç¤ºæ˜¯é»˜è®¤åˆ‡ç‰‡
            })
            
            current_time = end_time
            index += 1
        
        logger.info(f"æ™ºèƒ½é»˜è®¤åˆ‡ç‰‡: {len(shots)} ä¸ªç‰‡æ®µï¼Œæ¯ä¸ªçº¦ {segment_duration} ç§’")
        return shots


class ParallelBatchProcessor:
    """å¹¶è¡Œæ‰¹å¤„ç†å™¨ - æ”¯æŒè¯­ä¹‰ç‰‡æ®µåˆå¹¶"""
    
    def __init__(self, 
                 output_dir: str = "./data/output",
                 temp_dir: str = "./data/temp", 
                 max_concurrent: int = 3,
                 ffmpeg_workers: int = 4,
                 enable_semantic_merge: bool = True,
                 similarity_threshold: float = 0.92,  # æé«˜åˆ°92%ï¼ˆéå¸¸ä¸¥æ ¼ï¼‰
                 max_merge_duration: float = 25.0):   # é™ä½åˆ°25ç§’ï¼ˆæ›´ä¸¥æ ¼ï¼‰
        """
        åˆå§‹åŒ–å¹¶è¡Œæ‰¹å¤„ç†å™¨
        
        Args:
            output_dir: è¾“å‡ºç›®å½•
            temp_dir: ä¸´æ—¶ç›®å½•
            max_concurrent: æœ€å¤§è§†é¢‘å¹¶å‘æ•°
            ffmpeg_workers: FFmpegå·¥ä½œçº¿ç¨‹æ•°
            enable_semantic_merge: æ˜¯å¦å¯ç”¨è¯­ä¹‰åˆå¹¶
            similarity_threshold: è¯­ä¹‰ç›¸ä¼¼åº¦é˜ˆå€¼
            max_merge_duration: æœ€å¤§åˆå¹¶æ—¶é•¿
        """
        self.output_dir = Path(output_dir)
        self.temp_dir = Path(temp_dir)
        self.max_concurrent = max_concurrent
        self.ffmpeg_workers = ffmpeg_workers
        
        # è¯­ä¹‰åˆå¹¶é…ç½®
        self.enable_semantic_merge = enable_semantic_merge
        self.similarity_threshold = similarity_threshold
        self.max_merge_duration = max_merge_duration

        # åˆ›å»ºç›®å½•
        self.output_dir.mkdir(parents=True, exist_ok=True)
        self.temp_dir.mkdir(parents=True, exist_ok=True)

        # åˆå§‹åŒ–ç»„ä»¶
        self.video_analyzer = GoogleVideoAnalyzer()
        self.video_slicer = ParallelVideoSlicer(max_workers=ffmpeg_workers)
        # æ ¹æ®è¯­ä¹‰åˆå¹¶è®¾ç½®è°ƒæ•´æœ¬åœ°æ£€æµ‹å™¨è¡Œä¸º
        if self.enable_semantic_merge:
            self.local_scene_detector = LocalSceneDetector(threshold=0.4, min_scene_duration=3.0)  # å¯ç”¨åŸºç¡€åˆå¹¶
        else:
            self.local_scene_detector = LocalSceneDetector(threshold=0.4, min_scene_duration=0.1)  # å‡ ä¹ä¸åˆå¹¶ï¼Œä¸¥æ ¼æŒ‰è½¬åœºç‚¹åˆ‡åˆ†
        
        # åˆå§‹åŒ–è¯­ä¹‰åˆå¹¶å™¨
        if self.enable_semantic_merge:
            self.semantic_merger = SemanticSegmentMerger(
                similarity_threshold=similarity_threshold,
                max_merge_duration=max_merge_duration
            )
            logger.info("âœ… è¯­ä¹‰åˆå¹¶åŠŸèƒ½å·²å¯ç”¨")
        else:
            self.semantic_merger = None
            logger.info("âš ï¸  è¯­ä¹‰åˆå¹¶åŠŸèƒ½å·²ç¦ç”¨")

        logger.info(f"å¹¶è¡Œæ‰¹å¤„ç†å™¨åˆå§‹åŒ–å®Œæˆ")
        logger.info(f"è§†é¢‘å¹¶å‘æ•°: {max_concurrent}")
        logger.info(f"FFmpegçº¿ç¨‹æ•°: {ffmpeg_workers}")
        logger.info(f"âœ… æœ¬åœ°è½¬åœºæ£€æµ‹å™¨å·²å°±ç»ª")
    
    def _validate_video_file(self, video_path: str) -> bool:
        """éªŒè¯è§†é¢‘æ–‡ä»¶"""
        try:
            if not os.path.exists(video_path):
                return False
            
            file_size = os.path.getsize(video_path)
            if file_size == 0:
                return False
            
            # ç®€å•çš„æ–‡ä»¶æ ¼å¼æ£€æŸ¥
            valid_extensions = {'.mp4', '.avi', '.mov', '.mkv', '.wmv', '.flv'}
            if Path(video_path).suffix.lower() not in valid_extensions:
                return False
            
            return True
        except Exception:
            return False
    
    def _create_default_shots(self, video_path: str, segment_duration: float = 10.0) -> List[Dict[str, Any]]:
        """
        åˆ›å»ºæœ¬åœ°è½¬åœºæ£€æµ‹åˆ‡ç‰‡ï¼ˆå½“Google Cloudåˆ†æå¤±è´¥æ—¶çš„å…œåº•æ–¹æ¡ˆï¼‰
        ä½¿ç”¨FFmpeg sceneæ£€æµ‹ç®—æ³•è¿›è¡Œæ™ºèƒ½é•œå¤´è¯†åˆ«ï¼Œä¿æŒé•œå¤´å®Œæ•´æ€§
        
        Args:
            video_path: è§†é¢‘æ–‡ä»¶è·¯å¾„
            segment_duration: å…œåº•å›ºå®šæ—¶é•¿ï¼ˆç§’ï¼‰- ä»…åœ¨è½¬åœºæ£€æµ‹ä¹Ÿå¤±è´¥æ—¶ä½¿ç”¨
            
        Returns:
            æ™ºèƒ½åˆ‡ç‰‡åˆ—è¡¨
        """
        logger.info("ğŸ¬ ä½¿ç”¨æœ¬åœ°è½¬åœºæ£€æµ‹å™¨è¿›è¡Œæ™ºèƒ½åˆ‡ç‰‡...")
        
        # ä½¿ç”¨æœ¬åœ°è½¬åœºæ£€æµ‹å™¨
        shots = self.local_scene_detector.detect_scenes(video_path)
        
        if shots:
            logger.info(f"âœ… æœ¬åœ°è½¬åœºæ£€æµ‹æˆåŠŸ: {len(shots)} ä¸ªé•œå¤´")
            return shots
        else:
            logger.warning("âš ï¸  è½¬åœºæ£€æµ‹å¤±è´¥ï¼Œä½¿ç”¨æ™ºèƒ½é»˜è®¤åˆ‡ç‰‡")
            # æœ€åçš„å…œåº•æ–¹æ¡ˆ
            return self.local_scene_detector._create_smart_default_shots(
                video_path, 
                self.local_scene_detector._get_video_duration(video_path)
            )
    
    def _validate_slice_quality(self, slices: List[Dict[str, Any]], video_name: str) -> Dict[str, Any]:
        """éªŒè¯åˆ‡ç‰‡è´¨é‡"""
        if not slices:
            return {
                "passed": False,
                "error": "æ²¡æœ‰ç”Ÿæˆä»»ä½•åˆ‡ç‰‡",
                "details": {}
            }
        
        # åŸºæœ¬è´¨é‡æ£€æŸ¥
        total_slices = len(slices)
        valid_slices = 0
        total_duration = 0
        
        for slice_info in slices:
            if 'file_path' in slice_info and os.path.exists(slice_info['file_path']):
                file_size = os.path.getsize(slice_info['file_path'])
                if file_size > 1024:  # è‡³å°‘1KB
                    valid_slices += 1
                    total_duration += slice_info.get('duration', 0)
        
        success_rate = (valid_slices / total_slices) * 100 if total_slices > 0 else 0
        
        return {
            "passed": success_rate >= 80,  # 80%æˆåŠŸç‡ä¸ºé€šè¿‡
            "success_rate": success_rate,
            "total_slices": total_slices,
            "valid_slices": valid_slices,
            "total_duration": total_duration,
            "error": f"æˆåŠŸç‡è¿‡ä½: {success_rate:.1f}%" if success_rate < 80 else None,
            "details": {
                "video_name": video_name,
                "quality_threshold": 80,
                "check_time": datetime.now().isoformat()
            }
        }
    
    def process_single_video(self, 
                           video_path: str,
                           features: List[str] = None,
                           progress_callback: Optional[callable] = None,
                           default_segment_duration: float = 10.0,
                           analysis_mode: str = "auto") -> Dict[str, Any]:
        """
        å¤„ç†å•ä¸ªè§†é¢‘æ–‡ä»¶ - æ”¯æŒGoogle Cloudå¤±è´¥æ—¶çš„æœ¬åœ°å…œåº•æ–¹æ¡ˆ
        
        Args:
            video_path: è§†é¢‘æ–‡ä»¶è·¯å¾„
            features: åˆ†æåŠŸèƒ½åˆ—è¡¨
            progress_callback: è¿›åº¦å›è°ƒå‡½æ•°
            default_segment_duration: é»˜è®¤ç‰‡æ®µæ—¶é•¿ï¼ˆå½“äº‘åˆ†æå¤±è´¥æ—¶ä½¿ç”¨ï¼‰
            
        Returns:
            å¤„ç†ç»“æœå­—å…¸
        """
        start_time = time.time()
        video_name = Path(video_path).stem
        
        logger.info(f"ğŸ” å¼€å§‹åˆ†æè§†é¢‘: {video_name}")
        
        # éªŒè¯è§†é¢‘æ–‡ä»¶
        if not self._validate_video_file(video_path):
            return {
                "success": False,
                "video_name": video_name,
                "error": f"è§†é¢‘æ–‡ä»¶æ— æ•ˆæˆ–ä¸å­˜åœ¨: {video_path}",
                "processing_time": time.time() - start_time
            }
        
        if progress_callback:
            progress_callback(10, f"å¼€å§‹åˆ†æè§†é¢‘: {video_name}")
        
        try:
            # æ ¹æ®åˆ†ææ¨¡å¼é€‰æ‹©å¤„ç†æ–¹å¼
            if analysis_mode == "local":
                # å¼ºåˆ¶ä½¿ç”¨æœ¬åœ°è½¬åœºæ£€æµ‹
                logger.info(f"ğŸ”§ å¼ºåˆ¶ä½¿ç”¨æœ¬åœ°è½¬åœºæ£€æµ‹æ¨¡å¼")
                analysis_result = {"success": False, "error": "å¼ºåˆ¶ä½¿ç”¨æœ¬åœ°æ¨¡å¼"}
            elif analysis_mode == "google":
                # å¼ºåˆ¶ä½¿ç”¨Google Cloudåˆ†æ
                logger.info(f"â˜ï¸  å¼ºåˆ¶ä½¿ç”¨Google Cloudåˆ†ææ¨¡å¼")
                analysis_result = self.video_analyzer.analyze_video(
                    video_path=video_path,
                    features=features or ["shot_detection"],
                    progress_callback=lambda p, m: progress_callback(10 + p * 0.5, m) if progress_callback else None,
                    auto_cleanup_storage=True,  # è‡ªåŠ¨æ¸…ç†ä¸´æ—¶æ–‡ä»¶
                    bucket_name="video-slice-bucket"  # æŒ‡å®šå­˜å‚¨æ¡¶
                )
            else:  # autoæ¨¡å¼
                # è‡ªåŠ¨é€‰æ‹©ï¼šé¦–å…ˆå°è¯•Google Cloudåˆ†æ
                logger.info(f"ğŸ¤– è‡ªåŠ¨æ¨¡å¼ï¼šå°è¯•Google Cloudåˆ†æ")
                analysis_result = self.video_analyzer.analyze_video(
                    video_path=video_path,
                    features=features or ["shot_detection"],
                    progress_callback=lambda p, m: progress_callback(10 + p * 0.5, m) if progress_callback else None,
                    auto_cleanup_storage=True,  # è‡ªåŠ¨æ¸…ç†ä¸´æ—¶æ–‡ä»¶
                    bucket_name="video-slice-bucket"  # æŒ‡å®šå­˜å‚¨æ¡¶
                )
            
            if not analysis_result["success"]:
                logger.warning(f"Google Cloudåˆ†æå¤±è´¥: {analysis_result.get('error', 'æœªçŸ¥é”™è¯¯')}")
                logger.info(f"ğŸ”„ åˆ‡æ¢åˆ°æœ¬åœ°è½¬åœºæ£€æµ‹æ–¹æ¡ˆ (FFmpegæ™ºèƒ½è¯†åˆ«é•œå¤´åˆ‡æ¢)")
                
                # ä½¿ç”¨æœ¬åœ°é»˜è®¤åˆ‡ç‰‡æ–¹æ¡ˆ
                shots = self._create_default_shots(video_path, default_segment_duration)
                if not shots:
                    return {
                        "success": False,
                        "video_name": video_name,
                        "error": "æ— æ³•åˆ›å»ºé»˜è®¤åˆ‡ç‰‡æ–¹æ¡ˆ",
                        "processing_time": time.time() - start_time
                    }

                analysis_result = {
                    "success": True,
                    "shots": shots,
                    "fallback_mode": True
                }
            else:
                # Google Cloudåˆ†ææˆåŠŸï¼Œæå–shotsæ•°æ®
                shots = self.video_analyzer.extract_shots(analysis_result)
                if not shots:
                    logger.warning("Google Cloudåˆ†ææˆåŠŸä½†æœªæ£€æµ‹åˆ°é•œå¤´ï¼Œä½¿ç”¨æœ¬åœ°å…œåº•æ–¹æ¡ˆ")
                    shots = self._create_default_shots(video_path, default_segment_duration)
                    if not shots:
                        return {
                            "success": False,
                            "video_name": video_name,
                            "error": "Google Cloudå’Œæœ¬åœ°æ–¹æ¡ˆéƒ½æ— æ³•åˆ›å»ºåˆ‡ç‰‡",
                            "processing_time": time.time() - start_time
                        }
                    analysis_result["fallback_mode"] = True
                
                # æ›´æ–°analysis_resultä»¥åŒ…å«shotsæ•°æ®
                analysis_result["shots"] = shots
            
            # å®‰å…¨è·å–shotsï¼Œç¡®ä¿é”®å­˜åœ¨
            shots = analysis_result.get("shots", [])
            if not shots:
                logger.error("åˆ†æç»“æœä¸­æ²¡æœ‰æ‰¾åˆ°shotsæ•°æ®")
                return {
                    "success": False,
                    "video_name": video_name,
                    "error": "åˆ†æç»“æœä¸­æ²¡æœ‰shotsæ•°æ®",
                    "processing_time": time.time() - start_time
                }

            if progress_callback:
                progress_callback(60, f"æ£€æµ‹åˆ° {len(shots)} ä¸ªç‰‡æ®µï¼Œå¼€å§‹åˆ‡ç‰‡...")
            
            logger.info(f"ğŸ“Š æ£€æµ‹åˆ° {len(shots)} ä¸ªè§†é¢‘ç‰‡æ®µ")
            
            # åˆ›å»ºè§†é¢‘è¾“å‡ºç›®å½•
            video_output_dir = self.output_dir / video_name
            video_output_dir.mkdir(parents=True, exist_ok=True)

            # æ‰§è¡Œè§†é¢‘åˆ‡ç‰‡
            slices = self.video_slicer.create_slices_from_shots(
                video_path=video_path,
                shots=shots,
                video_name=video_name,
                output_dir=str(self.output_dir),
                progress_callback=lambda p, m: progress_callback(60 + p * 0.3, m) if progress_callback else None
            )

            if progress_callback:
                progress_callback(90, f"åˆ‡ç‰‡å®Œæˆï¼ŒéªŒè¯è´¨é‡...")
            
            # éªŒè¯åˆ‡ç‰‡è´¨é‡
            quality_check = self._validate_slice_quality(slices, video_name)
            
            # ä¿å­˜åˆ‡ç‰‡ä¿¡æ¯
            slices_file = video_output_dir / f"{video_name}_slices.json"
            slice_info = {
                    "video_name": video_name,
                "video_path": video_path,
                "total_slices": len(slices),
                "fallback_mode": analysis_result.get("fallback_mode", False),
                "processing_time": time.time() - start_time,
                "quality_check": quality_check,
                "slices": slices
            }
            
            with open(slices_file, 'w', encoding='utf-8') as f:
                json.dump(slice_info, f, ensure_ascii=False, indent=2)
            
            # è¯­ä¹‰åˆå¹¶ï¼ˆå¦‚æœå¯ç”¨ï¼‰- æ”¯æŒæœ¬åœ°è½¬åœºæ£€æµ‹
            if self.enable_semantic_merge and self.semantic_merger:
                if progress_callback:
                    progress_callback(95, f"æ‰§è¡Œè¯­ä¹‰åˆå¹¶...")
                
                try:
                    logger.info(f"ğŸ”— å¼€å§‹è¯­ä¹‰åˆå¹¶: {len(slices)} ä¸ªç‰‡æ®µ")
                    # è¯­ä¹‰åˆå¹¶åº”è¯¥åœ¨slicesç›®å½•ä¸‹è¿›è¡Œï¼Œç›´æ¥æ›¿æ¢åŸå§‹åˆ‡ç‰‡
                    slices_dir = video_output_dir / "slices"
                    merge_result = self.semantic_merger.merge_segments(
                        slices, video_name, str(slices_dir)
                    )
                    
                    if merge_result.get("success") and merge_result.get("segments"):
                        # ğŸ”§ æ ¹æœ¬æ€§ä¿®å¤ï¼šç”¨åˆå¹¶åçš„åˆ‡ç‰‡æ›¿æ¢åŸå§‹åˆ‡ç‰‡ï¼Œç¡®ä¿ç»Ÿä¸€æ•°æ®æº
                        merged_segments = merge_result["segments"]
                        slice_info["original_slices"] = slice_info["slices"]  # å¤‡ä»½åŸå§‹æ•°æ®
                        slice_info["slices"] = merged_segments  # æ›¿æ¢ä¸ºåˆå¹¶åçš„æœ€ç»ˆç»“æœ
                        slice_info["total_slices"] = len(merged_segments)  # æ›´æ–°æ€»æ•°
                        slice_info["merged_from_count"] = len(slice_info["original_slices"])  # è®°å½•åŸå§‹æ•°é‡
                        slice_info["merge_applied"] = True  # æ ‡è®°å·²åº”ç”¨åˆå¹¶
                        slice_info["merge_compression_ratio"] = merge_result.get("compression_ratio", 1.0)  # è®°å½•å‹ç¼©æ¯”
                        
                        with open(slices_file, 'w', encoding='utf-8') as f:
                            json.dump(slice_info, f, ensure_ascii=False, indent=2)
                    
                        logger.info(f"ğŸ”— è¯­ä¹‰åˆå¹¶å®Œæˆ: {len(slices)} -> {len(merged_segments)} ä¸ªç‰‡æ®µ")
                    else:
                        logger.info("ğŸ”— è¯­ä¹‰åˆå¹¶ï¼šæœªæ‰¾åˆ°å¯åˆå¹¶çš„ç‰‡æ®µ")
                    
                except Exception as e:
                    logger.warning(f"è¯­ä¹‰åˆå¹¶å¤±è´¥: {e}")
            elif self.enable_semantic_merge:
                logger.warning("è¯­ä¹‰åˆå¹¶å·²å¯ç”¨ä½†åˆå¹¶å™¨æœªåˆå§‹åŒ–")

            if progress_callback:
                progress_callback(100, f"å¤„ç†å®Œæˆ: {len(slices)} ä¸ªåˆ‡ç‰‡")
            
            processing_time = time.time() - start_time

            logger.info(f"âœ… è§†é¢‘å¤„ç†å®Œæˆ: {video_name}")
            logger.info(f"ğŸ“Š ç”Ÿæˆåˆ‡ç‰‡: {len(slices)} ä¸ª")
            logger.info(f"â±ï¸  å¤„ç†æ—¶é—´: {processing_time:.1f}ç§’")

            return {
                "success": True,
                "video_name": video_name,
                "video_path": video_path,
                "total_slices": len(slices),
                "fallback_mode": analysis_result.get("fallback_mode", False),
                "processing_time": processing_time,
                "quality_passed": quality_check["passed"],
                "slices": slices
            }

        except Exception as e:
            error_msg = f"è§†é¢‘å¤„ç†å¼‚å¸¸: {str(e)}"
            logger.error(f"âŒ {video_name}: {error_msg}")
            
            return {
                "success": False,
                "video_name": video_name,
                "error": error_msg,
                "processing_time": time.time() - start_time
            }
    
    @retry(
        wait=wait_random_exponential(multiplier=1, max=120),
        stop=stop_after_attempt(3)
    )
    async def async_process_video(self, video_path: str, features: List[str] = None) -> Dict[str, Any]:
        """
        å¼‚æ­¥å¤„ç†å•ä¸ªè§†é¢‘æ–‡ä»¶
        
        Args:
            video_path: è§†é¢‘æ–‡ä»¶è·¯å¾„
            features: åˆ†æåŠŸèƒ½åˆ—è¡¨
            
        Returns:
            å¤„ç†ç»“æœå­—å…¸
        """
        async with self.semaphore:  # é™åˆ¶å¹¶å‘æ•°
            video_name = Path(video_path).stem
            
            try:
                logger.info(f"ğŸ¬ å¼€å§‹å¼‚æ­¥å¤„ç†è§†é¢‘: {video_name}")
                start_time = time.time()
                
                # ä½¿ç”¨çº¿ç¨‹æ± æ‰§è¡ŒåŒæ­¥çš„è§†é¢‘å¤„ç†
                loop = asyncio.get_event_loop()
                result = await loop.run_in_executor(
                    None, 
                    self.process_single_video, 
                    video_path, 
                    features
                )
                
                end_time = time.time()
                duration = end_time - start_time
                
                if result.get("success"):
                    logger.info(f"âœ… è§†é¢‘å¤„ç†å®Œæˆ: {video_name} ({duration:.1f}ç§’)")
                else:
                    logger.error(f"âŒ è§†é¢‘å¤„ç†å¤±è´¥: {video_name}")
                
                return result
                
            except Exception as e:
                error_msg = f"å¼‚æ­¥å¤„ç†è§†é¢‘å¤±è´¥ {video_name}: {str(e)}"
                logger.error(error_msg)
                return {
                    "success": False,
                    "video_name": video_name,
                    "error": error_msg,
                    "slices_count": 0,
                    "slices": []
                }
    
    async def parallel_batch_process(self, video_files: List[str], 
                                   features: List[str] = None,
                                   progress_callback: Optional[callable] = None) -> Dict[str, Any]:
        """
        å¹¶è¡Œæ‰¹é‡å¤„ç†è§†é¢‘æ–‡ä»¶
        
        Args:
            video_files: è§†é¢‘æ–‡ä»¶è·¯å¾„åˆ—è¡¨
            features: åˆ†æåŠŸèƒ½åˆ—è¡¨
            progress_callback: è¿›åº¦å›è°ƒå‡½æ•°
            
        Returns:
            æ‰¹å¤„ç†ç»“æœ
        """
        total_videos = len(video_files)
        self.stats["total_videos"] = total_videos
        
        logger.info(f"ğŸš€ å¼€å§‹å¹¶è¡Œå¤„ç† {total_videos} ä¸ªè§†é¢‘æ–‡ä»¶ (æœ€å¤§å¹¶å‘: {self.max_concurrent})")
        
        if progress_callback:
            progress_callback(0, f"å¼€å§‹å¹¶è¡Œå¤„ç† {total_videos} ä¸ªè§†é¢‘...")
        
        start_time = time.time()
        
        # åˆ›å»ºå¼‚æ­¥ä»»åŠ¡åˆ—è¡¨
        tasks = []
        for i, video_file in enumerate(video_files):
            task = self.async_process_video(str(video_file), features)
            tasks.append(task)
        
        # å¹¶è¡Œæ‰§è¡Œæ‰€æœ‰ä»»åŠ¡ï¼Œä½¿ç”¨as_completedè·å–è¿›åº¦
        results = []
        completed = 0
        
        for coro in asyncio.as_completed(tasks):
            try:
                result = await coro
                results.append(result)
                completed += 1
                
                # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
                if result.get("success"):
                    self.stats["processed_videos"] += 1
                    self.stats["total_slices"] += result.get("slices_count", 0)
                else:
                    self.stats["failed_videos"] += 1
                    self.stats["processing_errors"].append({
                        "video": result.get("video_name", "unknown"),
                        "error": result.get("error", "unknown error")
                    })
                
                # è¿›åº¦å›è°ƒ
                progress = int((completed / total_videos) * 100)
                if progress_callback:
                    progress_callback(
                        progress, 
                        f"å·²å®Œæˆ {completed}/{total_videos} ä¸ªè§†é¢‘ "
                        f"(æˆåŠŸ: {self.stats['processed_videos']}, "
                        f"å¤±è´¥: {self.stats['failed_videos']})"
                    )
                
                logger.info(f"ğŸ“Š è¿›åº¦: {completed}/{total_videos} ({progress}%)")
                
            except Exception as e:
                logger.error(f"å¤„ç†ä»»åŠ¡æ—¶å‘ç”Ÿå¼‚å¸¸: {e}")
                results.append({
                    "success": False,
                    "video_name": "unknown",
                    "error": str(e),
                    "slices_count": 0,
                    "slices": []
                })
                completed += 1
        
        end_time = time.time()
        total_duration = end_time - start_time
        
        # ç”Ÿæˆè¯¦ç»†æŠ¥å‘Š
        report_data = {
            'batch_stats': self.stats.copy(),
            'processing_results': results,
            'parallel_info': {
                'max_concurrent': self.max_concurrent,
                'total_duration_seconds': total_duration,
                'average_time_per_video': total_duration / total_videos if total_videos > 0 else 0,
                'estimated_sequential_time': sum([r.get('duration', 94) for r in results if r.get('success')]),
                'time_saved_percentage': 0
            },
            'generated_at': datetime.now().isoformat()
        }
        
        # è®¡ç®—æ—¶é—´èŠ‚çœ
        estimated_sequential = report_data['parallel_info']['estimated_sequential_time']
        if estimated_sequential > 0:
            time_saved = max(0, (estimated_sequential - total_duration) / estimated_sequential * 100)
            report_data['parallel_info']['time_saved_percentage'] = time_saved
        
        # ä¿å­˜æŠ¥å‘Š
        report_file = self.output_dir / "parallel_batch_processing_report.json"
        
        # æ¸…ç†æ‰¹å¤„ç†æŠ¥å‘Šä¸­çš„numpyæ•°ç»„
        cleaned_batch_report = self._clean_for_json_serialization(report_data)
        
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(cleaned_batch_report, f, indent=2, ensure_ascii=False)
        
        logger.info(f"ğŸ‰ å¹¶è¡Œæ‰¹å¤„ç†å®Œæˆ!")
        logger.info(f"ğŸ“Š å¤„ç†ç»Ÿè®¡: æˆåŠŸ {self.stats['processed_videos']}/{total_videos} ä¸ªè§†é¢‘")
        logger.info(f"ğŸ¬ æ€»è®¡ç”Ÿæˆ: {self.stats['total_slices']} ä¸ªè§†é¢‘åˆ‡ç‰‡")
        logger.info(f"â±ï¸  æ€»è€—æ—¶: {total_duration:.1f}ç§’")
        logger.info(f"ğŸ“„ è¯¦ç»†æŠ¥å‘Š: {report_file}")
        
        if report_data['parallel_info']['time_saved_percentage'] > 0:
            logger.info(f"ğŸš€ æ€§èƒ½æå‡: èŠ‚çœäº† {report_data['parallel_info']['time_saved_percentage']:.1f}% çš„æ—¶é—´!")
        
        return {
            "success": True,
            "stats": self.stats,
            "results": results,
            "report_file": str(report_file),
            "total_duration": total_duration,
            "parallel_info": report_data['parallel_info']
        }
    
    def process_batch_sync(self, input_dir: str, file_patterns: List[str] = None, 
                          features: List[str] = None,
                          default_segment_duration: float = 10.0,
                          analysis_mode: str = "auto") -> Dict[str, Any]:
        """
        åŒæ­¥æ‰¹å¤„ç†è§†é¢‘æ–‡ä»¶ï¼ˆæ”¯æŒè¯­ä¹‰åˆå¹¶ï¼‰
        
        Args:
            input_dir: è¾“å…¥ç›®å½•
            file_patterns: æ–‡ä»¶åŒ¹é…æ¨¡å¼
            features: åˆ†æåŠŸèƒ½åˆ—è¡¨
            
        Returns:
            æ‰¹å¤„ç†ç»“æœ
        """
        if not file_patterns:
            file_patterns = ["*.mp4", "*.MP4", "*.avi", "*.AVI", "*.mov", "*.MOV", "*.mkv", "*.MKV"]
        if not features:
            features = ["shot_detection"]

        input_path = Path(input_dir)
        
        if not input_path.exists():
            logger.error(f"è¾“å…¥ç›®å½•ä¸å­˜åœ¨: {input_dir}")
            return {
                "success": False,
                "error": f"Input directory does not exist: {input_dir}"
            }

        # æ”¶é›†è§†é¢‘æ–‡ä»¶
        video_files = []
        for pattern in file_patterns:
            video_files.extend(input_path.glob(pattern))

        if not video_files:
            logger.warning(f"æœªæ‰¾åˆ°åŒ¹é…çš„è§†é¢‘æ–‡ä»¶: {file_patterns}")
            return {
                "success": False,
                "error": f"No video files found matching patterns: {file_patterns}"
            }

        logger.info(f"ğŸ¬ å‘ç° {len(video_files)} ä¸ªè§†é¢‘æ–‡ä»¶")
        logger.info(f"ğŸ”§ è¯­ä¹‰åˆå¹¶: {'å¯ç”¨' if self.enable_semantic_merge else 'ç¦ç”¨'}")
        if self.enable_semantic_merge:
            logger.info(f"ğŸ“Š ç›¸ä¼¼åº¦é˜ˆå€¼: {self.similarity_threshold}")
            logger.info(f"â±ï¸  æœ€å¤§åˆå¹¶æ—¶é•¿: {self.max_merge_duration}ç§’")

        batch_start_time = time.time()
        results = []
        
        # ä½¿ç”¨çº¿ç¨‹æ± è¿›è¡Œå¹¶è¡Œå¤„ç†
        with ThreadPoolExecutor(max_workers=self.max_concurrent) as executor:
            # æäº¤æ‰€æœ‰ä»»åŠ¡
            future_to_video = {
                executor.submit(self.process_single_video, str(video_file), features, None, default_segment_duration, analysis_mode): video_file
                for video_file in video_files
            }

            # æ”¶é›†ç»“æœ
            for future in as_completed(future_to_video):
                video_file = future_to_video[future]
                try:
                    result = future.result()
                    results.append(result)
                    
                    if result['success']:
                        merge_info = ""
                        if self.enable_semantic_merge and result.get('compression_ratio', 1.0) > 1.0:
                            merge_info = f" (å‹ç¼©æ¯”: {result['compression_ratio']:.1f}x)"
                        logger.info(f"âœ… å®Œæˆ {len(results)}/{len(video_files)}: {result['video_name']}{merge_info}")
                    else:
                        logger.error(f"âŒ å¤±è´¥ {len(results)}/{len(video_files)}: {video_file.name} - {result.get('error', 'Unknown error')}")
                        
                except Exception as e:
                    logger.error(f"âŒ å¤„ç†å¼‚å¸¸: {video_file.name} - {e}")
                    results.append({
                        "success": False,
                        "video_name": video_file.stem,
                        "error": str(e),
                        "duration": 0
                    })

        # ç”Ÿæˆæ‰¹å¤„ç†æŠ¥å‘Š
        batch_end_time = time.time()
        batch_duration = batch_end_time - batch_start_time

        successful_results = [r for r in results if r['success']]
        failed_results = [r for r in results if not r['success']]

        # è®¡ç®—è¯­ä¹‰åˆå¹¶ç»Ÿè®¡
        total_original_slices = sum(r.get('original_slices', 0) for r in successful_results)
        total_final_slices = sum(r.get('slices_count', 0) for r in successful_results)
        total_compression_ratio = total_original_slices / max(1, total_final_slices)

        batch_report = {
            "timestamp": datetime.now().isoformat(),
            "input_directory": str(input_path),
            "output_directory": str(self.output_dir),
            "file_patterns": file_patterns,
            "features": features,
            "semantic_merge_enabled": self.enable_semantic_merge,
            "similarity_threshold": self.similarity_threshold,
            "max_merge_duration": self.max_merge_duration,
            "total_duration": batch_duration,
            "stats": {
                "total_videos": len(video_files),
                "processed_videos": len(successful_results),
                "failed_videos": len(failed_results),
                "total_slices": total_final_slices,
                "original_slices": total_original_slices,
                "compression_ratio": total_compression_ratio,
                "average_time_per_video": batch_duration / max(1, len(video_files))
            },
            "parallel_info": {
                "max_concurrent_videos": self.max_concurrent,
                "ffmpeg_workers": self.ffmpeg_workers,
                "estimated_sequential_time": sum(r.get('duration', 0) for r in results),
                "actual_parallel_time": batch_duration,
                "time_saved_percentage": max(0, (sum(r.get('duration', 0) for r in results) - batch_duration) / max(batch_duration, 0.001) * 100),
                "average_time_per_video": batch_duration / max(1, len(results))
            },
            "results": results
        }

        # ä¿å­˜æ‰¹å¤„ç†æŠ¥å‘Š
        report_file = self.output_dir / "parallel_batch_processing_report.json"
        
        # æ¸…ç†æ‰¹å¤„ç†æŠ¥å‘Šä¸­çš„numpyæ•°ç»„
        cleaned_batch_report = self._clean_for_json_serialization(batch_report)
        
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(cleaned_batch_report, f, indent=2, ensure_ascii=False)

        # è¾“å‡ºç»Ÿè®¡ä¿¡æ¯
        logger.info(f"ğŸ‰ æ‰¹å¤„ç†å®Œæˆ!")
        logger.info(f"ğŸ“Š æˆåŠŸå¤„ç†: {len(successful_results)}/{len(video_files)} ä¸ªè§†é¢‘")
        if failed_results:
            logger.info(f"âŒ å¤±è´¥: {len(failed_results)} ä¸ªè§†é¢‘")
        logger.info(f"ğŸ¬ æ€»åˆ‡ç‰‡æ•°: {total_final_slices}")
        if self.enable_semantic_merge and total_compression_ratio > 1.0:
            logger.info(f"ğŸ§  è¯­ä¹‰å‹ç¼©æ¯”: {total_compression_ratio:.1f}x")
        logger.info(f"â±ï¸  æ€»æ—¶é—´: {batch_duration:.1f}ç§’")
        logger.info(f"ğŸ“„ æŠ¥å‘Šæ–‡ä»¶: {report_file}")

        return {
            "success": len(successful_results) > 0,
            "stats": batch_report["stats"],
            "parallel_info": batch_report["parallel_info"],
            "total_duration": batch_duration,
            "report_file": str(report_file),
            "results": results
        }

    def _clean_for_json_serialization(self, obj):
        """
        é€’å½’æ¸…ç†å¯¹è±¡ä»¥å‡†å¤‡JSONåºåˆ—åŒ–ï¼Œç§»é™¤æ‰€æœ‰numpyæ•°ç»„å’Œä¸å¯åºåˆ—åŒ–çš„å¯¹è±¡
        
        Args:
            obj: è¦æ¸…ç†çš„å¯¹è±¡
            
        Returns:
            æ¸…ç†åçš„å¯åºåˆ—åŒ–å¯¹è±¡
        """
        import numpy as np
        
        if isinstance(obj, np.ndarray):
            # numpyæ•°ç»„è½¬æ¢ä¸ºåˆ—è¡¨
            return obj.tolist()
        elif isinstance(obj, dict):
            # é€’å½’æ¸…ç†å­—å…¸
            cleaned_dict = {}
            for key, value in obj.items():
                # è·³è¿‡ç‰¹å®šçš„ä¸å¯åºåˆ—åŒ–å­—æ®µ
                if key in ['clip_features', 'features', 'model_output']:
                    continue
                try:
                    cleaned_dict[key] = self._clean_for_json_serialization(value)
                except (TypeError, ValueError):
                    # å¦‚æœå€¼ä¸èƒ½åºåˆ—åŒ–ï¼Œå°±è·³è¿‡
                    continue
            return cleaned_dict
        elif isinstance(obj, list):
            # é€’å½’æ¸…ç†åˆ—è¡¨
            return [self._clean_for_json_serialization(item) for item in obj]
        elif hasattr(obj, 'tolist') and callable(getattr(obj, 'tolist')):
            # æ”¯æŒtolistçš„numpyç±»å‹
            try:
                return obj.tolist()
            except:
                return str(obj)
        elif hasattr(obj, '__dict__'):
            # è‡ªå®šä¹‰å¯¹è±¡è½¬æ¢ä¸ºå­—å…¸
            return self._clean_for_json_serialization(obj.__dict__)
        else:
            # åŸºæœ¬ç±»å‹ç›´æ¥è¿”å›
            return obj


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="AI Video Master 5.0 - å¹¶è¡Œæ‰¹é‡è§†é¢‘åˆ‡ç‰‡å·¥å…·")
    parser.add_argument("input_dir", help="è¾“å…¥è§†é¢‘ç›®å½•")
    parser.add_argument("-o", "--output", default="./data/output", help="è¾“å‡ºç›®å½•")
    parser.add_argument("-t", "--temp", default="./data/temp", help="ä¸´æ—¶ç›®å½•")
    parser.add_argument("-f", "--features", nargs="+", 
                       choices=["shot_detection", "label_detection", "face_detection", "text_detection"],
                       default=["shot_detection"],
                       help="åˆ†æåŠŸèƒ½ (é»˜è®¤ä»…é•œå¤´æ£€æµ‹ï¼Œæ€§èƒ½æœ€ä½³)")
    parser.add_argument("-c", "--concurrent", type=int, default=3,
                       help="è§†é¢‘çº§æœ€å¤§å¹¶å‘æ•° (é»˜è®¤3ï¼Œå»ºè®®ä¸è¶…è¿‡3ä»¥éµå¾ªAPIé…é¢)")
    parser.add_argument("-w", "--ffmpeg-workers", type=int, default=4,
                       help="FFmpegå¹¶è¡Œåˆ‡ç‰‡å·¥ä½œçº¿ç¨‹æ•° (é»˜è®¤4ï¼Œå»ºè®®2-8)")
    parser.add_argument("--patterns", nargs="+", 
                       default=["*.mp4", "*.MP4", "*.avi", "*.AVI", "*.mov", "*.MOV", "*.mkv", "*.MKV"],
                       help="æ–‡ä»¶åŒ¹é…æ¨¡å¼(æ”¯æŒå¤§å°å†™)")
    parser.add_argument("-v", "--verbose", action="store_true", help="è¯¦ç»†è¾“å‡º")
    
    args = parser.parse_args()
    
    # è®¾ç½®æ—¥å¿—çº§åˆ«
    if args.verbose:
        logging.getLogger().setLevel(logging.DEBUG)
    
    # æ£€æŸ¥ç¯å¢ƒå˜é‡
    if not os.getenv("GOOGLE_APPLICATION_CREDENTIALS"):
        if not os.path.exists("google_credentials.json"):
            logger.error("è¯·è®¾ç½® GOOGLE_APPLICATION_CREDENTIALS ç¯å¢ƒå˜é‡")
            logger.error("æˆ–å°†Google Cloudå‡­æ®æ–‡ä»¶æ”¾åœ¨å½“å‰ç›®å½•ä¸‹å¹¶å‘½åä¸º google_credentials.json")
            return 1
    
    try:
        # åˆ›å»ºå¹¶è¡Œå¤„ç†å™¨
        processor = ParallelBatchProcessor(
            output_dir=args.output,
            temp_dir=args.temp,
            max_concurrent=args.concurrent,
            ffmpeg_workers=args.ffmpeg_workers
        )
        
        # æ‰§è¡Œå¹¶è¡Œæ‰¹å¤„ç†
        result = processor.process_batch_sync(
            input_dir=args.input_dir,
            file_patterns=args.patterns,
            features=args.features
        )
        
        if result["success"]:
            print(f"\nâœ… å¹¶è¡Œæ‰¹å¤„ç†å®Œæˆ!")
            print(f"ğŸ“Š å¤„ç†ç»Ÿè®¡: {result['stats']['processed_videos']}/{result['stats']['total_videos']} ä¸ªè§†é¢‘æˆåŠŸ")
            print(f"ğŸ¬ æ€»è®¡ç”Ÿæˆ: {result['stats']['total_slices']} ä¸ªè§†é¢‘åˆ‡ç‰‡")
            print(f"â±ï¸  æ€»è€—æ—¶: {result['total_duration']:.1f}ç§’")
            print(f"ğŸ“„ è¯¦ç»†æŠ¥å‘Š: {result['report_file']}")
            
            if result['parallel_info']['time_saved_percentage'] > 0:
                print(f"ğŸš€ æ€§èƒ½æå‡: èŠ‚çœäº† {result['parallel_info']['time_saved_percentage']:.1f}% çš„æ—¶é—´!")
            
            return 0
        else:
            print(f"\nâŒ å¹¶è¡Œæ‰¹å¤„ç†å¤±è´¥: {result['error']}")
            return 1
            
    except KeyboardInterrupt:
        logger.info("ç”¨æˆ·ä¸­æ–­å¤„ç†")
        return 130
    except Exception as e:
        logger.error(f"ç¨‹åºå¼‚å¸¸: {e}")
        return 1


if __name__ == "__main__":
    sys.exit(main()) 