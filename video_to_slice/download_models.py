#!/usr/bin/env python3
"""
CLIPæ¨¡å‹ä¸‹è½½è„šæœ¬
ä¸‹è½½å¹¶ç¼“å­˜CLIPæ¨¡å‹åˆ°æœ¬åœ°ï¼Œç”¨äºç¦»çº¿ä½¿ç”¨
"""

import os
import sys
from pathlib import Path
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def download_clip_model():
    """ä¸‹è½½CLIPæ¨¡å‹åˆ°æœ¬åœ°ç¼“å­˜"""
    try:
        from transformers import CLIPProcessor, CLIPModel
        import torch
        
        print("ğŸ”„ å¼€å§‹ä¸‹è½½CLIPæ¨¡å‹...")
        
        model_name = "openai/clip-vit-base-patch32"
        cache_dir = Path("./cache/clip").resolve()
        cache_dir.mkdir(parents=True, exist_ok=True)
        
        print(f"ğŸ“ ç¼“å­˜ç›®å½•: {cache_dir}")
        
        # ä¸‹è½½æ¨¡å‹
        print("ğŸ“¦ ä¸‹è½½CLIPæ¨¡å‹...")
        model = CLIPModel.from_pretrained(
            model_name,
            cache_dir=str(cache_dir),
            force_download=False  # å¦‚æœå·²å­˜åœ¨åˆ™ä¸é‡å¤ä¸‹è½½
        )
        
        print("ğŸ“¦ ä¸‹è½½CLIPå¤„ç†å™¨...")
        processor = CLIPProcessor.from_pretrained(
            model_name,
            cache_dir=str(cache_dir),
            force_download=False
        )
        
        # æµ‹è¯•æ¨¡å‹æ˜¯å¦æ­£å¸¸å·¥ä½œ
        print("ğŸ§ª æµ‹è¯•æ¨¡å‹...")
        model.eval()
        
        # æ£€æŸ¥GPUå¯ç”¨æ€§
        if torch.cuda.is_available():
            print("ğŸ® æ£€æµ‹åˆ°GPUï¼Œæµ‹è¯•GPUæ¨¡å¼...")
            model = model.to('cuda')
            print("âœ… GPUæ¨¡å¼æ­£å¸¸")
        else:
            print("ğŸ’» ä½¿ç”¨CPUæ¨¡å¼")
        
        print("âœ… CLIPæ¨¡å‹ä¸‹è½½å¹¶æµ‹è¯•æˆåŠŸ!")
        print(f"ğŸ“‚ æ¨¡å‹å·²ä¿å­˜åˆ°: {cache_dir}")
        
        # æ˜¾ç¤ºç¼“å­˜ç›®å½•å†…å®¹
        print("\nğŸ“‹ ç¼“å­˜æ–‡ä»¶:")
        for item in cache_dir.rglob("*"):
            if item.is_file():
                size_mb = item.stat().st_size / (1024 * 1024)
                print(f"  {item.relative_to(cache_dir)} ({size_mb:.1f} MB)")
        
        return True
        
    except ImportError as e:
        print(f"âŒ ä¾èµ–ç¼ºå¤±: {e}")
        print("è¯·å…ˆå®‰è£…: uv add torch transformers pillow")
        return False
        
    except Exception as e:
        print(f"âŒ ä¸‹è½½å¤±è´¥: {e}")
        return False

def check_offline_model():
    """æ£€æŸ¥ç¦»çº¿æ¨¡å‹æ˜¯å¦å¯ç”¨"""
    try:
        from transformers import CLIPProcessor, CLIPModel
        
        model_name = "openai/clip-vit-base-patch32"
        cache_dir = Path("./cache/clip").resolve()
        
        if not cache_dir.exists():
            print("âŒ ç¼“å­˜ç›®å½•ä¸å­˜åœ¨")
            return False
        
        print("ğŸ” æ£€æŸ¥ç¦»çº¿æ¨¡å‹...")
        
        # å°è¯•ä»æœ¬åœ°åŠ è½½
        model = CLIPModel.from_pretrained(
            model_name,
            cache_dir=str(cache_dir),
            local_files_only=True  # åªä½¿ç”¨æœ¬åœ°æ–‡ä»¶
        )
        
        processor = CLIPProcessor.from_pretrained(
            model_name,
            cache_dir=str(cache_dir),
            local_files_only=True
        )
        
        print("âœ… ç¦»çº¿æ¨¡å‹åŠ è½½æˆåŠŸ!")
        return True
        
    except Exception as e:
        print(f"âŒ ç¦»çº¿æ¨¡å‹æ£€æŸ¥å¤±è´¥: {e}")
        return False

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ¬ AI Video Master 5.0 - CLIPæ¨¡å‹ç®¡ç†å·¥å…·")
    print("=" * 50)
    
    # æ£€æŸ¥æ˜¯å¦å·²æœ‰ç¦»çº¿æ¨¡å‹
    if check_offline_model():
        print("\nâœ… ç¦»çº¿æ¨¡å‹å·²å­˜åœ¨ä¸”å¯ç”¨!")
        choice = input("æ˜¯å¦é‡æ–°ä¸‹è½½? [y/N]: ").strip().lower()
        if choice not in ['y', 'yes']:
            print("ğŸ‘ ä½¿ç”¨ç°æœ‰ç¦»çº¿æ¨¡å‹")
            return
    
    # ä¸‹è½½æ¨¡å‹
    print("\nğŸš€ å¼€å§‹ä¸‹è½½æµç¨‹...")
    success = download_clip_model()
    
    if success:
        print("\nğŸ‰ æ¨¡å‹å‡†å¤‡å®Œæˆ!")
        print("ç°åœ¨å¯ä»¥ä½¿ç”¨ç¦»çº¿æ¨¡å¼è¿è¡Œç¨‹åºäº†")
    else:
        print("\nâŒ æ¨¡å‹å‡†å¤‡å¤±è´¥")
        sys.exit(1)

if __name__ == "__main__":
    main() 