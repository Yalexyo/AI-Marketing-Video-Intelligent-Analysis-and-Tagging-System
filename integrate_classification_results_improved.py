#!/usr/bin/env python3
"""
ğŸ“ç”Ÿæˆç»“æœæ•´åˆè„šæœ¬ - æ”¹è¿›ç‰ˆ
ä½¿ç”¨å¤šç»´åº¦å»é‡é€»è¾‘ï¼šæ–‡ä»¶å + æ–‡ä»¶å¤§å° + æ—¶é•¿
"""

import os
import shutil
from pathlib import Path
import json
from collections import defaultdict
import datetime
import hashlib
try:
    from moviepy.editor import VideoFileClip  # type: ignore
except ImportError as e:
    print(f"âŒ ç¼ºå°‘ä¾èµ–åŒ…: {e}")
    print("ğŸ’¡ è¯·è¿è¡Œ: uv add moviepy")
    VideoFileClip = None

class ImprovedClassificationIntegrator:
    def __init__(self, source_dir="ğŸ“ç”Ÿæˆç»“æœ", target_dir="ğŸ“ç”Ÿæˆç»“æœ/ã€æ€»å½’ç±»ã€‘"):
        self.source_dir = Path(source_dir)
        self.target_dir = Path(target_dir)
        self.video_files = defaultdict(list)
        self.duplicate_files = []
        self.duplicate_groups = []  # é‡å¤æ–‡ä»¶ç»„
        
    def get_video_fingerprint(self, video_path):
        """è·å–è§†é¢‘æŒ‡çº¹ï¼šæ–‡ä»¶å¤§å° + æ—¶é•¿"""
        try:
            # è·å–æ–‡ä»¶å¤§å°
            file_size = video_path.stat().st_size
            
            # è·å–è§†é¢‘æ—¶é•¿ï¼ˆæ›´å‡†ç¡®çš„åˆ¤æ–­ä¾æ®ï¼‰
            if VideoFileClip is not None:
                with VideoFileClip(str(video_path)) as clip:
                    duration = round(clip.duration, 2)
            else:
                print(f"  âš ï¸ MoviePyæœªå®‰è£…ï¼Œæ— æ³•è·å–è§†é¢‘æ—¶é•¿: {video_path.name}")
                duration = 0
            
            # ç”ŸæˆæŒ‡çº¹
            fingerprint = f"{file_size}_{duration}"
            
            return {
                'size': file_size,
                'duration': duration,
                'fingerprint': fingerprint
            }
        except Exception as e:
            print(f"  âš ï¸ è·å–è§†é¢‘ä¿¡æ¯å¤±è´¥: {video_path.name} - {str(e)}")
            # fallbackï¼šä»…ä½¿ç”¨æ–‡ä»¶å¤§å°
            return {
                'size': video_path.stat().st_size,
                'duration': 0,
                'fingerprint': f"{video_path.stat().st_size}_0"
            }
    
    def is_duplicate_video(self, new_file_info, existing_files):
        """åˆ¤æ–­æ˜¯å¦ä¸ºé‡å¤è§†é¢‘"""
        new_fingerprint = new_file_info['fingerprint']
        
        for existing in existing_files:
            # 1. æ–‡ä»¶åå®Œå…¨ç›¸åŒ
            if new_file_info['name'] == existing['name']:
                return True, existing, "æ–‡ä»¶åç›¸åŒ"
            
            # 2. æŒ‡çº¹ç›¸åŒï¼ˆæ–‡ä»¶å¤§å° + æ—¶é•¿ï¼‰
            if new_fingerprint == existing['fingerprint']:
                return True, existing, "æ–‡ä»¶å¤§å°å’Œæ—¶é•¿ç›¸åŒ"
            
            # 3. æ–‡ä»¶åç›¸ä¼¼ä¸”å¤§å°æ¥è¿‘ï¼ˆå¯èƒ½æ˜¯åŒä¸€è§†é¢‘çš„ä¸åŒç‰ˆæœ¬ï¼‰
            if self.is_similar_filename(new_file_info['name'], existing['name']):
                size_diff = abs(new_file_info['size'] - existing['size'])
                if size_diff < 100 * 1024:  # å°äº100KBå·®å¼‚
                    return True, existing, "æ–‡ä»¶åç›¸ä¼¼ä¸”å¤§å°æ¥è¿‘"
        
        return False, None, ""
    
    def is_similar_filename(self, name1, name2):
        """åˆ¤æ–­æ–‡ä»¶åæ˜¯å¦ç›¸ä¼¼"""
        # ç§»é™¤æ‰©å±•å
        stem1 = Path(name1).stem
        stem2 = Path(name2).stem
        
        # ç§»é™¤å¸¸è§çš„åç¼€ï¼ˆå¦‚ _1, _2, _copyç­‰ï¼‰
        clean_stem1 = stem1.split('_')[0]
        clean_stem2 = stem2.split('_')[0]
        
        return clean_stem1 == clean_stem2
    
    def scan_classification_results(self):
        """æ‰«ææ‰€æœ‰åˆ†ç±»ç»“æœæ–‡ä»¶å¤¹"""
        print("ğŸ” æ‰«æåˆ†ç±»ç»“æœæ–‡ä»¶å¤¹...")
        
        timestamp_folders = [d for d in self.source_dir.iterdir() 
                           if d.is_dir() and not d.name.startswith("ã€")]
        
        print(f"ğŸ“ å‘ç° {len(timestamp_folders)} ä¸ªåˆ†ç±»ç»“æœæ–‡ä»¶å¤¹")
        
        primary_tags = {
            "ğŸ¼_äº§å“ä»‹ç»_è•´æ·³": "ğŸ¼_äº§å“ä»‹ç»_è•´æ·³",
            "ğŸ¼_äº§å“ä»‹ç»_æ°´å¥¶": "ğŸ¼_äº§å“ä»‹ç»_æ°´å¥¶", 
            "ğŸ¼_äº§å“ä»‹ç»_è“é’»": "ğŸ¼_äº§å“ä»‹ç»_è“é’»",
            "ğŸŒŸ_ä½¿ç”¨æ•ˆæœ": "ğŸŒŸ_ä½¿ç”¨æ•ˆæœ",
            "ğŸ_ä¿ƒé”€æœºåˆ¶": "ğŸ_ä¿ƒé”€æœºåˆ¶",
            "ğŸª_é’©å­": "ğŸª_é’©å­"
        }
        
        # æ‰«ææ¯ä¸ªæ—¶é—´æˆ³æ–‡ä»¶å¤¹
        for folder in sorted(timestamp_folders, key=lambda x: x.name):
            print(f"  ğŸ“‚ æ‰«æ: {folder.name}")
            
            for tag_folder in folder.iterdir():
                if tag_folder.is_dir() and tag_folder.name in primary_tags:
                    tag_name = primary_tags[tag_folder.name]
                    
                    for file in tag_folder.iterdir():
                        if file.suffix.lower() == '.mp4':
                            print(f"    ğŸ¬ åˆ†æè§†é¢‘: {file.name}")
                            
                            # è·å–è§†é¢‘æŒ‡çº¹
                            video_info = self.get_video_fingerprint(file)
                            
                            file_info = {
                                'name': file.name,
                                'path': file,
                                'folder': folder.name,
                                'tag': tag_name,
                                'size': video_info['size'],
                                'duration': video_info['duration'],
                                'fingerprint': video_info['fingerprint']
                            }
                            
                            # æ£€æŸ¥æ˜¯å¦é‡å¤
                            is_dup, existing_file, reason = self.is_duplicate_video(
                                file_info, self.video_files[tag_name]
                            )
                            
                            if is_dup and existing_file is not None:
                                self.duplicate_files.append({
                                    'name': file.name,
                                    'tag': tag_name,
                                    'folder': folder.name,
                                    'existing_folder': existing_file['folder'],
                                    'reason': reason,
                                    'new_size': file_info['size'],
                                    'new_duration': file_info['duration'],
                                    'existing_size': existing_file['size'],
                                    'existing_duration': existing_file['duration']
                                })
                                print(f"    ğŸ”„ é‡å¤æ–‡ä»¶: {reason}")
                            else:
                                self.video_files[tag_name].append(file_info)
                                print(f"    âœ… æ–°æ–‡ä»¶: {file.name}")
        
        # ç»Ÿè®¡ä¿¡æ¯
        print(f"\nğŸ“Š æ‰«æå®Œæˆç»Ÿè®¡:")
        total_files = sum(len(files) for files in self.video_files.values())
        print(f"  âœ… å”¯ä¸€è§†é¢‘æ–‡ä»¶: {total_files} ä¸ª")
        print(f"  ğŸ”„ é‡å¤æ–‡ä»¶: {len(self.duplicate_files)} ä¸ª")
        
        for tag, files in self.video_files.items():
            print(f"  ğŸ“‹ {tag}: {len(files)} ä¸ªæ–‡ä»¶")
    
    def create_integrated_folder(self):
        """åˆ›å»ºã€æ€»å½’ç±»ã€‘æ–‡ä»¶å¤¹"""
        print(f"\nğŸ“ åˆ›å»ºã€æ€»å½’ç±»ã€‘æ–‡ä»¶å¤¹: {self.target_dir}")
        
        if self.target_dir.exists():
            shutil.rmtree(self.target_dir)
        
        self.target_dir.mkdir(parents=True, exist_ok=True)
        
        for tag in self.video_files.keys():
            tag_folder = self.target_dir / tag
            tag_folder.mkdir(exist_ok=True)
            print(f"  ğŸ“‚ åˆ›å»ºåˆ†ç±»æ–‡ä»¶å¤¹: {tag}")
    
    def copy_files_with_analysis(self):
        """å¤åˆ¶è§†é¢‘æ–‡ä»¶å’Œåˆ†ææ–‡ä»¶"""
        print(f"\nğŸ“‹ å¤åˆ¶æ–‡ä»¶åˆ°ã€æ€»å½’ç±»ã€‘æ–‡ä»¶å¤¹...")
        
        copied_files = 0
        failed_files = 0
        
        for tag, files in self.video_files.items():
            tag_folder = self.target_dir / tag
            
            for file_info in files:
                try:
                    # å¤åˆ¶è§†é¢‘æ–‡ä»¶
                    source_video = file_info['path']
                    target_video = tag_folder / file_info['name']
                    
                    shutil.copy2(source_video, target_video)
                    
                    # å¤åˆ¶å¯¹åº”çš„JSONåˆ†ææ–‡ä»¶
                    json_name = file_info['name'].replace('.mp4', '_analysis.json')
                    source_json = source_video.parent / json_name
                    target_json = tag_folder / json_name
                    
                    if source_json.exists():
                        shutil.copy2(source_json, target_json)
                    
                    copied_files += 1
                    print(f"  âœ… å¤åˆ¶: {file_info['name']} â†’ {tag}")
                    
                except Exception as e:
                    failed_files += 1
                    print(f"  âŒ å¤åˆ¶å¤±è´¥: {file_info['name']} - {str(e)}")
        
        print(f"\nğŸ“Š å¤åˆ¶å®Œæˆ:")
        print(f"  âœ… æˆåŠŸå¤åˆ¶: {copied_files} ä¸ªæ–‡ä»¶")
        print(f"  âŒ å¤åˆ¶å¤±è´¥: {failed_files} ä¸ªæ–‡ä»¶")
    
    def generate_detailed_report(self):
        """ç”Ÿæˆè¯¦ç»†æŠ¥å‘Š"""
        print(f"\nğŸ“„ ç”Ÿæˆè¯¦ç»†æŠ¥å‘Š...")
        
        report = {
            "integration_time": datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            "total_unique_files": sum(len(files) for files in self.video_files.values()),
            "duplicate_files": len(self.duplicate_files),
            "classifications": {},
            "duplicate_analysis": {}
        }
        
        # åˆ†ç±»ç»Ÿè®¡
        for tag, files in self.video_files.items():
            report["classifications"][tag] = {
                "count": len(files),
                "files": [
                    {
                        "name": f['name'],
                        "size_mb": round(f['size'] / 1024 / 1024, 2),
                        "duration": f['duration'],
                        "source_folder": f['folder']
                    }
                    for f in files
                ]
            }
        
        # é‡å¤æ–‡ä»¶åˆ†æ
        duplicate_reasons = defaultdict(int)
        for dup in self.duplicate_files:
            duplicate_reasons[dup['reason']] += 1
        
        report["duplicate_analysis"] = {
            "by_reason": dict(duplicate_reasons),
            "details": self.duplicate_files
        }
        
        # ä¿å­˜æŠ¥å‘Š
        report_path = self.target_dir / "ğŸ“Š_è¯¦ç»†æ•´åˆæŠ¥å‘Š.json"
        with open(report_path, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
        
        print(f"  âœ… è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜: {report_path}")
        
        # ç”Ÿæˆäººç±»å¯è¯»çš„æ±‡æ€»
        summary_path = self.target_dir / "ğŸ“‹_å»é‡åˆ†ææ±‡æ€».txt"
        with open(summary_path, 'w', encoding='utf-8') as f:
            f.write("ğŸ“ç”Ÿæˆç»“æœ - æ™ºèƒ½å»é‡åˆ†ææ±‡æ€»\n")
            f.write("=" * 60 + "\n\n")
            f.write(f"æ•´åˆæ—¶é—´: {report['integration_time']}\n")
            f.write(f"å”¯ä¸€è§†é¢‘æ–‡ä»¶: {report['total_unique_files']} ä¸ª\n")
            f.write(f"é‡å¤æ–‡ä»¶: {report['duplicate_files']} ä¸ª\n\n")
            
            f.write("ğŸ“Š åˆ†ç±»ç»Ÿè®¡:\n")
            for tag, info in report["classifications"].items():
                f.write(f"  {tag}: {info['count']} ä¸ªæ–‡ä»¶\n")
                total_size = sum(f['size_mb'] for f in info['files'])
                total_duration = sum(f['duration'] for f in info['files'])
                f.write(f"    æ€»å¤§å°: {total_size:.1f} MB\n")
                f.write(f"    æ€»æ—¶é•¿: {total_duration:.1f} ç§’\n\n")
            
            f.write("ğŸ”„ é‡å¤æ–‡ä»¶åˆ†æ:\n")
            for reason, count in duplicate_reasons.items():
                f.write(f"  {reason}: {count} ä¸ª\n")
            f.write("\n")
            
            f.write("ğŸ“‹ é‡å¤æ–‡ä»¶è¯¦æƒ…:\n")
            for i, dup in enumerate(self.duplicate_files, 1):
                f.write(f"{i}. {dup['name']} ({dup['tag']})\n")
                f.write(f"   åŸå› : {dup['reason']}\n")
                f.write(f"   ä¿ç•™ç‰ˆæœ¬: {dup['existing_folder']} ({dup['existing_size']/1024/1024:.1f}MB, {dup['existing_duration']:.1f}s)\n")
                f.write(f"   è·³è¿‡ç‰ˆæœ¬: {dup['folder']} ({dup['new_size']/1024/1024:.1f}MB, {dup['new_duration']:.1f}s)\n\n")
        
        print(f"  âœ… å»é‡åˆ†ææ±‡æ€»å·²ä¿å­˜: {summary_path}")
    
    def run(self):
        """æ‰§è¡Œæ•´åˆæµç¨‹"""
        print("ğŸš€ å¼€å§‹æ™ºèƒ½å»é‡æ•´åˆğŸ“ç”Ÿæˆç»“æœ...")
        print("=" * 60)
        
        try:
            # 1. æ‰«æåˆ†ç±»ç»“æœ
            self.scan_classification_results()
            
            # 2. åˆ›å»ºæ€»å½’ç±»æ–‡ä»¶å¤¹
            self.create_integrated_folder()
            
            # 3. å¤åˆ¶æ–‡ä»¶
            self.copy_files_with_analysis()
            
            # 4. ç”Ÿæˆè¯¦ç»†æŠ¥å‘Š
            self.generate_detailed_report()
            
            print("\n" + "=" * 60)
            print("ğŸ‰ æ™ºèƒ½å»é‡æ•´åˆå®Œæˆï¼")
            print(f"ğŸ“ æ€»å½’ç±»æ–‡ä»¶å¤¹: {self.target_dir}")
            print(f"ğŸ“Š å”¯ä¸€è§†é¢‘æ–‡ä»¶: {sum(len(files) for files in self.video_files.values())}")
            print(f"ğŸ”„ é‡å¤æ–‡ä»¶: {len(self.duplicate_files)}")
            
        except Exception as e:
            print(f"âŒ æ•´åˆå¤±è´¥: {str(e)}")
            import traceback
            traceback.print_exc()

if __name__ == "__main__":
    integrator = ImprovedClassificationIntegrator()
    integrator.run() 