#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ‰¹é‡è§†é¢‘åˆ‡ç‰‡æ ‡ç­¾æå–å™¨ - åŒå±‚è¯†åˆ«æœºåˆ¶ç‰ˆæœ¬

åŸºäºä¸»ç¨‹åºçš„åŒå±‚è¯†åˆ«æœºåˆ¶ï¼š
1. ç¬¬ä¸€å±‚ï¼ˆAI-Bï¼‰ï¼šé€šç”¨ç‰©ä½“/åœºæ™¯/æƒ…ç»ªè¯†åˆ« + ä¸»è°“å®¾åŠ¨ä½œè¯†åˆ«  
2. ç¬¬äºŒå±‚ï¼ˆAI-Aï¼‰ï¼šæ¡ä»¶è§¦å‘çš„å“ç‰Œä¸“ç”¨æ£€æµ‹

å‚è€ƒæ¶æ„ï¼šbatch_video_to_srt.py
"""

import os
import sys
import json
import logging
import argparse
import time
from pathlib import Path
from typing import List, Dict, Any, Optional
from datetime import datetime

# æ·»åŠ å½“å‰ç›®å½•åˆ°è·¯å¾„
current_dir = Path(__file__).parent
sys.path.insert(0, str(current_dir))

from config.slice_config import validate_config, get_output_config, get_quality_control
from src.ai_analyzers import DualStageAnalyzer, BatchSliceAnalyzer
from utils.file_utils import scan_video_files, ensure_output_directory

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(sys.stdout),
        logging.FileHandler('slice_to_label.log', encoding='utf-8')
    ]
)
logger = logging.getLogger(__name__)

class BatchSliceToLabelProcessor:
    """
    æ‰¹é‡åˆ‡ç‰‡æ ‡ç­¾æå–å¤„ç†å™¨
    
    åŸºäºä¸»ç¨‹åºçš„åŒå±‚è¯†åˆ«æœºåˆ¶å®ç°
    å‚è€ƒï¼šbatch_video_to_srt.py çš„æ¶æ„æ¨¡å¼
    """
    
    def __init__(self):
        """åˆå§‹åŒ–å¤„ç†å™¨"""
        self.output_config = get_output_config()
        self.quality_control = get_quality_control()
        
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        for dir_key, dir_path in self.output_config.items():
            if dir_key.endswith("_dir"):
                Path(dir_path).mkdir(parents=True, exist_ok=True)
        
        # åˆå§‹åŒ–åˆ†æå™¨
        self.dual_analyzer = DualStageAnalyzer()
        self.batch_analyzer = BatchSliceAnalyzer()
        
        logger.info("âœ… æ‰¹é‡åˆ‡ç‰‡æ ‡ç­¾æå–å¤„ç†å™¨åˆå§‹åŒ–å®Œæˆ")
        logger.info("ğŸ¯ åŒå±‚è¯†åˆ«æœºåˆ¶ï¼šAI-Bé€šç”¨è¯†åˆ« + AI-Aæ¡ä»¶å“ç‰Œæ£€æµ‹")
    
    def process_directory(
        self, 
        input_dir: str, 
        output_dir: Optional[str] = None,
        file_pattern: str = "*.mp4"
    ) -> Dict[str, Any]:
        """
        å¤„ç†æ•´ä¸ªç›®å½•çš„è§†é¢‘æ–‡ä»¶
        
        Args:
            input_dir: è¾“å…¥ç›®å½•è·¯å¾„
            output_dir: è¾“å‡ºç›®å½•è·¯å¾„ï¼ˆå¯é€‰ï¼‰
            file_pattern: æ–‡ä»¶åŒ¹é…æ¨¡å¼
            
        Returns:
            å¤„ç†ç»“æœå­—å…¸
        """
        logger.info(f"ğŸ¯ å¼€å§‹å¤„ç†ç›®å½•: {input_dir}")
        logger.info(f"ğŸ“ æ–‡ä»¶æ¨¡å¼: {file_pattern}")
        
        # æ‰«æè§†é¢‘æ–‡ä»¶
        video_files = scan_video_files(input_dir)
        
        if not video_files:
            logger.warning(f"æœªæ‰¾åˆ°åŒ¹é…çš„è§†é¢‘æ–‡ä»¶: {input_dir}")
            return {
                "success": False,
                "error": "æœªæ‰¾åˆ°åŒ¹é…çš„è§†é¢‘æ–‡ä»¶",
                "input_dir": input_dir,
                "file_pattern": file_pattern
            }
        
        logger.info(f"ğŸ“¹ å‘ç° {len(video_files)} ä¸ªè§†é¢‘æ–‡ä»¶")
        
        # ç¡®å®šè¾“å‡ºç›®å½•
        if not output_dir:
            output_dir = self.output_config["output_dir"]
        
        # ç¡®ä¿output_diræ˜¯å­—ç¬¦ä¸²ç±»å‹
        if output_dir is None:
            output_dir = str(Path.cwd() / "output")
        
        ensure_output_directory(output_dir)
        
        # æ‰¹é‡å¤„ç†
        batch_result = self._process_file_list(video_files, output_dir)
        
        # æ·»åŠ è¾“å…¥ç›®å½•ä¿¡æ¯åˆ°ç»“æœä¸­
        batch_result["input_directory"] = input_dir
        
        return batch_result
    
    def process_single_file(self, video_path: str, analysis_type: str = "full") -> Dict[str, Any]:
        """
        å¤„ç†å•ä¸ªè§†é¢‘æ–‡ä»¶
        
        Args:
            video_path: è§†é¢‘æ–‡ä»¶è·¯å¾„
            analysis_type: åˆ†æç±»å‹ ("visual", "audio", "full")
            
        Returns:
            å¤„ç†ç»“æœå­—å…¸
        """
        try:
            logger.info(f"ğŸ¯ å¼€å§‹å¤„ç†å•ä¸ªæ–‡ä»¶: {video_path}")
            logger.info(f"ğŸ“Š åˆ†æç±»å‹: {analysis_type}")
            
            # éªŒè¯æ–‡ä»¶
            if not Path(video_path).exists():
                return {"error": f"æ–‡ä»¶ä¸å­˜åœ¨: {video_path}", "success": False}
            
            # æ‰§è¡Œåˆ†æ
            result = self.dual_analyzer.analyze_video_slice(video_path, analysis_type)
            
            if result.get("success"):
                # æ·»åŠ æ–‡ä»¶ä¿¡æ¯
                result["file_path"] = str(video_path)
                result["file_name"] = Path(video_path).name
                result["file_size_mb"] = Path(video_path).stat().st_size / (1024 * 1024)
                result["analysis_type"] = analysis_type
                result["processed_at"] = time.time()
                
                logger.info(f"âœ… å•æ–‡ä»¶å¤„ç†æˆåŠŸ: {Path(video_path).name}")
                return result
            else:
                logger.error(f"âŒ å•æ–‡ä»¶å¤„ç†å¤±è´¥: {result.get('error', 'æœªçŸ¥é”™è¯¯')}")
                return result
                
        except Exception as e:
            logger.error(f"å•æ–‡ä»¶å¤„ç†å¼‚å¸¸: {str(e)}")
            return {"error": str(e), "success": False}
    
    def process_batch(self, input_dir: str, analysis_type: str = "full", max_files: Optional[int] = None) -> Dict[str, Any]:
        """
        æ‰¹é‡å¤„ç†è§†é¢‘æ–‡ä»¶
        
        Args:
            input_dir: è¾“å…¥ç›®å½•
            analysis_type: åˆ†æç±»å‹ ("visual", "audio", "full")
            max_files: æœ€å¤§å¤„ç†æ–‡ä»¶æ•°ï¼ŒNoneä¸ºæ— é™åˆ¶
            
        Returns:
            æ‰¹é‡å¤„ç†ç»“æœ
        """
        try:
            logger.info(f"ğŸ¯ å¼€å§‹æ‰¹é‡å¤„ç†")
            logger.info(f"ğŸ“‚ è¾“å…¥ç›®å½•: {input_dir}")
            logger.info(f"ğŸ“Š åˆ†æç±»å‹: {analysis_type}")
            logger.info(f"ğŸ“‹ æœ€å¤§æ–‡ä»¶æ•°: {max_files or 'æ— é™åˆ¶'}")
            
            # æ‰«æè§†é¢‘æ–‡ä»¶
            video_files = self._scan_video_files(input_dir)
            
            if not video_files:
                return {"error": "æœªæ‰¾åˆ°è§†é¢‘æ–‡ä»¶", "success": False}
            
            # é™åˆ¶æ–‡ä»¶æ•°é‡
            if max_files:
                video_files = video_files[:max_files]
            
            logger.info(f"ğŸ“‹ æ‰¾åˆ° {len(video_files)} ä¸ªè§†é¢‘æ–‡ä»¶")
            
            # æ‰¹é‡å¤„ç†
            results = []
            failed_files = []
            
            for i, video_file in enumerate(video_files, 1):
                logger.info(f"ğŸ¬ å¤„ç†è¿›åº¦: {i}/{len(video_files)} - {Path(video_file).name}")
                
                try:
                    result = self.process_single_file(video_file, analysis_type)
                    
                    if result.get("success"):
                        results.append(result)
                    else:
                        failed_files.append({
                            "file": video_file,
                            "error": result.get("error", "æœªçŸ¥é”™è¯¯")
                        })
                        
                except Exception as e:
                    logger.error(f"å¤„ç†æ–‡ä»¶å¤±è´¥: {video_file}, é”™è¯¯: {e}")
                    failed_files.append({
                        "file": video_file,
                        "error": str(e)
                    })
                
                # ç®€å•çš„è¿›åº¦æ˜¾ç¤º
                if i % 10 == 0:
                    logger.info(f"ğŸ“Š å·²å¤„ç† {i} ä¸ªæ–‡ä»¶ï¼ŒæˆåŠŸ {len(results)} ä¸ª")
            
            # ç”Ÿæˆæ‰¹é‡å¤„ç†æŠ¥å‘Š
            batch_report = self._generate_batch_report(results, failed_files, analysis_type)
            
            # ä¿å­˜ç»“æœ
            output_file = self._save_batch_results(results, analysis_type)
            batch_report["output_file"] = output_file
            
            logger.info(f"âœ… æ‰¹é‡å¤„ç†å®Œæˆï¼ŒæˆåŠŸ {len(results)} ä¸ªï¼Œå¤±è´¥ {len(failed_files)} ä¸ª")
            return batch_report
            
        except Exception as e:
            logger.error(f"æ‰¹é‡å¤„ç†å¼‚å¸¸: {str(e)}")
            return {"error": str(e), "success": False}
    
    def _process_file_list(
        self, 
        video_files: List[str], 
        output_dir: str
    ) -> Dict[str, Any]:
        """
        å¤„ç†æ–‡ä»¶åˆ—è¡¨
        
        Args:
            video_files: è§†é¢‘æ–‡ä»¶è·¯å¾„åˆ—è¡¨
            output_dir: è¾“å‡ºç›®å½•
            
        Returns:
            æ‰¹é‡å¤„ç†ç»“æœ
        """
        logger.info(f"ğŸš€ å¼€å§‹æ‰¹é‡åŒå±‚è¯†åˆ«åˆ†æï¼Œå…± {len(video_files)} ä¸ªæ–‡ä»¶")
        
        start_time = time.time()
        
        # æ‰§è¡Œæ‰¹é‡åˆ†æ
        batch_result = self.batch_analyzer.analyze_batch(
            video_files=video_files,
            progress_callback=self._progress_callback
        )
        
        # ä¿å­˜æ‰¹é‡ç»“æœ
        summary_file = self._save_batch_summary(batch_result, output_dir)
        batch_result["summary_file"] = summary_file
        
        # ä¿å­˜è¯¦ç»†ç»“æœ
        details_file = self._save_detailed_results(batch_result, output_dir)
        batch_result["details_file"] = details_file
        
        # ç”ŸæˆæŠ¥å‘Š
        report_file = self._generate_analysis_report(batch_result, output_dir)
        batch_result["report_file"] = report_file
        
        end_time = time.time()
        duration = end_time - start_time
        
        # æ·»åŠ å¤„ç†ä¿¡æ¯
        batch_result.update({
            "processing_duration": duration,
            "output_directory": output_dir,
            "processing_timestamp": datetime.now().isoformat()
        })
        
        self._print_processing_summary(batch_result)
        
        return batch_result
    
    def _progress_callback(self, message: str):
        """è¿›åº¦å›è°ƒå‡½æ•°"""
        logger.info(f"ğŸ“Š {message}")
    
    def _save_single_result(self, result: Dict[str, Any], output_dir: str) -> str:
        """ä¿å­˜å•ä¸ªåˆ†æç»“æœ - ä½¿ç”¨ç»“æ„åŒ–æ–‡ä»¶å"""
        file_path = Path(result["file_path"])
        file_name = file_path.stem
        
        # æ¸…ç†æ–‡ä»¶åï¼Œç¡®ä¿åªåŒ…å«å®‰å…¨å­—ç¬¦
        clean_name = "".join(c for c in file_name if c.isalnum() or c in ('_', '-'))
        
        # ä½¿ç”¨ç»“æ„åŒ–å‘½åï¼šè§†é¢‘æ–‡ä»¶å_analysis.json
        output_file = Path(output_dir) / f"{clean_name}_analysis.json"
        
        try:
            # æ„å»ºç»“æ„åŒ–ç»“æœæ•°æ®
            structured_result = {
                'file_info': {
                    'filename': file_path.name,
                    'file_path': str(file_path),
                    'file_size_mb': round(file_path.stat().st_size / (1024 * 1024), 2) if file_path.exists() else 0,
                    'directory': file_path.parent.name
                },
                'analysis_info': {
                    'analysis_time': datetime.now().isoformat(),
                    'analyzer_version': 'dual_stage_v1.0',
                    'analysis_method': result.get('analysis_method', 'dual_stage'),
                    'success': result.get('success', True)
                },
                'content_analysis': {
                    'interaction': result.get('object', 'æœªçŸ¥'),
                    'scene': result.get('scene', 'æœªçŸ¥'),
                    'emotion': result.get('emotion', 'æœªçŸ¥'),
                    'confidence': result.get('confidence', 0.8)
                },
                'brand_detection': {
                    'brand_elements': result.get('brand_elements', 'æ— '),
                    'brand_detected': result.get('brand_elements', 'æ— ') != 'æ— ',
                    'stage2_triggered': result.get('stage2_triggered', False)
                },
                                 'technical_details': {
                     'key_frames_extracted': result.get('key_frames_count', 0),
                     'processing_time_seconds': result.get('processing_time', 0),
                     'stage1_success': result.get('stage1_success', True),
                     'stage2_success': result.get('stage2_success', True) if result.get('stage2_triggered') else None
                 }
            }
            
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(structured_result, f, ensure_ascii=False, indent=2)
            
            logger.debug(f"ğŸ’¾ ç»“æ„åŒ–ç»“æœå·²ä¿å­˜: {output_file}")
            return str(output_file)
            
        except Exception as e:
            logger.error(f"ä¿å­˜å•ä¸ªç»“æœå¤±è´¥: {e}")
            return ""
    
    def _save_batch_summary(self, batch_result: Dict[str, Any], output_dir: str) -> str:
        """ä¿å­˜æ‰¹é‡åˆ†ææ±‡æ€» - ä½¿ç”¨ç»“æ„åŒ–å‘½å"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # ä»æ‰¹é‡ç»“æœä¸­è·å–è¾“å…¥ç›®å½•ä¿¡æ¯
        input_info = batch_result.get('input_directory', 'unknown')
        if isinstance(input_info, str):
            dir_name = Path(input_info).name
        else:
            dir_name = 'mixed'
        
        summary_file = Path(output_dir) / f"batch_summary_{dir_name}_{timestamp}.json"
        
        try:
            # å‡†å¤‡æ±‡æ€»æ•°æ®ï¼ˆä¸åŒ…å«è¯¦ç»†ç»“æœï¼‰
            summary_data = {
                "processing_info": {
                    "total_files": batch_result["total_files"],
                    "success_count": batch_result["success_count"],
                    "failed_count": batch_result["failed_count"],
                    "duration": batch_result["duration"],
                    "timestamp": datetime.now().isoformat()
                },
                "statistics": batch_result["statistics"],
                "dual_stage_metrics": {
                    "stage2_trigger_rate": batch_result["statistics"].get("stage2_trigger_rate", 0),
                    "average_confidence": batch_result["statistics"].get("average_confidence", 0),
                    "top_interactions": dict(list(batch_result["statistics"].get("interaction_frequency", {}).items())[:5]),
                    "top_brands": dict(list(batch_result["statistics"].get("brand_frequency", {}).items())[:5])
                }
            }
            
            with open(summary_file, 'w', encoding='utf-8') as f:
                json.dump(summary_data, f, ensure_ascii=False, indent=2)
            
            logger.info(f"ğŸ“„ æ‰¹é‡åˆ†ææ±‡æ€»å·²ä¿å­˜: {summary_file}")
            return str(summary_file)
            
        except Exception as e:
            logger.error(f"ä¿å­˜æ‰¹é‡æ±‡æ€»å¤±è´¥: {e}")
            return ""
    
    def _save_detailed_results(self, batch_result: Dict[str, Any], output_dir: str) -> str:
        """ä¿å­˜è¯¦ç»†åˆ†æç»“æœ - ä½¿ç”¨ç»“æ„åŒ–å‘½å"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # ä»æ‰¹é‡ç»“æœä¸­è·å–è¾“å…¥ç›®å½•ä¿¡æ¯
        input_info = batch_result.get('input_directory', 'unknown')
        if isinstance(input_info, str):
            dir_name = Path(input_info).name
        else:
            dir_name = 'mixed'
        
        details_file = Path(output_dir) / f"batch_details_{dir_name}_{timestamp}.json"
        
        try:
            with open(details_file, 'w', encoding='utf-8') as f:
                json.dump(batch_result, f, ensure_ascii=False, indent=2)
            
            logger.info(f"ğŸ“‹ è¯¦ç»†åˆ†æç»“æœå·²ä¿å­˜: {details_file}")
            return str(details_file)
            
        except Exception as e:
            logger.error(f"ä¿å­˜è¯¦ç»†ç»“æœå¤±è´¥: {e}")
            return ""
    
    def _generate_analysis_report(self, batch_result: Dict[str, Any], output_dir: str) -> str:
        """ç”Ÿæˆåˆ†ææŠ¥å‘Š - ä½¿ç”¨ç»“æ„åŒ–å‘½å"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # ä»æ‰¹é‡ç»“æœä¸­è·å–è¾“å…¥ç›®å½•ä¿¡æ¯
        input_info = batch_result.get('input_directory', 'unknown')
        if isinstance(input_info, str):
            dir_name = Path(input_info).name
        else:
            dir_name = 'mixed'
        
        report_file = Path(output_dir) / f"analysis_report_{dir_name}_{timestamp}.md"
        
        try:
            statistics = batch_result["statistics"]
            
            report_content = f"""# åŒå±‚è¯†åˆ«æœºåˆ¶åˆ†ææŠ¥å‘Š

## ğŸ“Š å¤„ç†æ¦‚å†µ

- **æ€»æ–‡ä»¶æ•°**: {batch_result["total_files"]}
- **æˆåŠŸå¤„ç†**: {batch_result["success_count"]}
- **å¤„ç†å¤±è´¥**: {batch_result["failed_count"]}
- **æˆåŠŸç‡**: {(batch_result["success_count"] / batch_result["total_files"] * 100):.1f}%
- **å¤„ç†æ—¶é•¿**: {batch_result["duration"]:.2f} ç§’
- **å¹³å‡ç½®ä¿¡åº¦**: {statistics.get("average_confidence", 0):.2f}

## ğŸ¯ åŒå±‚è¯†åˆ«æœºåˆ¶è¡¨ç°

### ç¬¬äºŒé˜¶æ®µè§¦å‘ç‡
- **å“ç‰Œæ£€æµ‹è§¦å‘ç‡**: {statistics.get("stage2_trigger_rate", 0):.1f}%
- è¯´æ˜ï¼šæ£€æµ‹åˆ°äº§å“ç›¸å…³äº¤äº’å¹¶è§¦å‘å“ç‰Œä¸“ç”¨æ£€æµ‹çš„æ¯”ä¾‹

### ç¬¬ä¸€é˜¶æ®µï¼šé€šç”¨è¯†åˆ«ç»“æœ

#### ğŸ­ äº¤äº’è¡Œä¸ºé¢‘æ¬¡ (ä¸»è°“å®¾ç»“æ„)
"""
            
            # æ·»åŠ äº¤äº’é¢‘æ¬¡ç»Ÿè®¡
            interaction_freq = statistics.get("interaction_frequency", {})
            if interaction_freq:
                for interaction, count in sorted(interaction_freq.items(), key=lambda x: x[1], reverse=True)[:10]:
                    report_content += f"- **{interaction}**: {count} æ¬¡\n"
            else:
                report_content += "- æš‚æ— äº¤äº’æ•°æ®\n"
            
            report_content += f"""
#### ğŸï¸ åœºæ™¯ç¯å¢ƒé¢‘æ¬¡
"""
            
            # æ·»åŠ åœºæ™¯é¢‘æ¬¡ç»Ÿè®¡
            scene_freq = statistics.get("scene_frequency", {})
            if scene_freq:
                for scene, count in sorted(scene_freq.items(), key=lambda x: x[1], reverse=True)[:10]:
                    report_content += f"- **{scene}**: {count} æ¬¡\n"
            else:
                report_content += "- æš‚æ— åœºæ™¯æ•°æ®\n"
            
            report_content += f"""
#### ğŸ˜Š æƒ…ç»ªè¡¨è¾¾é¢‘æ¬¡
"""
            
            # æ·»åŠ æƒ…ç»ªé¢‘æ¬¡ç»Ÿè®¡
            emotion_freq = statistics.get("emotion_frequency", {})
            if emotion_freq:
                for emotion, count in sorted(emotion_freq.items(), key=lambda x: x[1], reverse=True)[:10]:
                    report_content += f"- **{emotion}**: {count} æ¬¡\n"
            else:
                report_content += "- æš‚æ— æƒ…ç»ªæ•°æ®\n"
            
            report_content += f"""
### ç¬¬äºŒé˜¶æ®µï¼šå“ç‰Œä¸“ç”¨æ£€æµ‹ç»“æœ

#### ğŸ·ï¸ æ ¸å¿ƒå“ç‰Œè¯†åˆ«é¢‘æ¬¡
"""
            
            # æ·»åŠ å“ç‰Œé¢‘æ¬¡ç»Ÿè®¡
            brand_freq = statistics.get("brand_frequency", {})
            if brand_freq:
                for brand, count in sorted(brand_freq.items(), key=lambda x: x[1], reverse=True):
                    report_content += f"- **{brand}**: {count} æ¬¡\n"
            else:
                report_content += "- æœªæ£€æµ‹åˆ°æ ¸å¿ƒå“ç‰Œ\n"
            
            report_content += f"""
## ğŸ“ˆ æ ‡ç­¾ç»Ÿè®¡

### é«˜é¢‘æ ‡ç­¾ TOP 15
"""
            
            # æ·»åŠ é«˜é¢‘æ ‡ç­¾ç»Ÿè®¡
            tag_freq = statistics.get("tag_frequency", {})
            if tag_freq:
                for tag, count in sorted(tag_freq.items(), key=lambda x: x[1], reverse=True)[:15]:
                    report_content += f"- **{tag}**: {count} æ¬¡\n"
            else:
                report_content += "- æš‚æ— æ ‡ç­¾æ•°æ®\n"
            
            report_content += f"""
## ğŸ› ï¸ æŠ€æœ¯è¯´æ˜

### åŒå±‚è¯†åˆ«æœºåˆ¶
1. **ç¬¬ä¸€å±‚ï¼ˆAI-Bï¼‰**: ä¸“æ³¨é€šç”¨ç‰©ä½“/åœºæ™¯/æƒ…ç»ªè¯†åˆ«ï¼Œå¼ºè°ƒä¸»è°“å®¾åŠ¨ä½œæè¿°
2. **ç¬¬äºŒå±‚ï¼ˆAI-Aï¼‰**: ä»…åœ¨æ£€æµ‹åˆ°äº§å“ç›¸å…³äº¤äº’æ—¶è§¦å‘ï¼Œè¿›è¡Œæ ¸å¿ƒå“ç‰Œä¸“ç”¨æ£€æµ‹

### è¯†åˆ«ä¼˜åŠ¿
- **é˜²è¯¯è¯†åˆ«**: å“ç‰Œæ£€æµ‹ä¸åŸºç¡€è¯†åˆ«åˆ†ç¦»ï¼Œé¿å…å“ç‰Œè§„åˆ™å¹²æ‰°é€šç”¨è¯†åˆ«
- **ç²¾å‡†è§¦å‘**: åªæœ‰åœ¨æ£€æµ‹åˆ°äº§å“ç›¸å…³è¡Œä¸ºæ—¶æ‰å¯åŠ¨å“ç‰Œæ£€æµ‹
- **ä¸»è°“å®¾ç»“æ„**: ç¬¬ä¸€é˜¶æ®µå¼ºè°ƒè¡Œä¸º/äº¤äº’çš„å®Œæ•´æè¿°

---
*æŠ¥å‘Šç”Ÿæˆæ—¶é—´: {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}*
"""
            
            with open(report_file, 'w', encoding='utf-8') as f:
                f.write(report_content)
            
            logger.info(f"ğŸ“– åˆ†ææŠ¥å‘Šå·²ç”Ÿæˆ: {report_file}")
            return str(report_file)
            
        except Exception as e:
            logger.error(f"ç”Ÿæˆåˆ†ææŠ¥å‘Šå¤±è´¥: {e}")
            return ""
    
    def _print_processing_summary(self, batch_result: Dict[str, Any]):
        """æ‰“å°å¤„ç†æ‘˜è¦"""
        print("\n" + "="*60)
        print("ğŸ¯ åŒå±‚è¯†åˆ«æœºåˆ¶å¤„ç†å®Œæˆ")
        print("="*60)
        print(f"ğŸ“Š æ€»æ–‡ä»¶æ•°: {batch_result['total_files']}")
        print(f"âœ… æˆåŠŸå¤„ç†: {batch_result['success_count']}")
        print(f"âŒ å¤„ç†å¤±è´¥: {batch_result['failed_count']}")
        print(f"ğŸ“ˆ æˆåŠŸç‡: {(batch_result['success_count'] / batch_result['total_files'] * 100):.1f}%")
        print(f"â±ï¸ å¤„ç†æ—¶é•¿: {batch_result['duration']:.2f} ç§’")
        
        statistics = batch_result['statistics']
        print(f"ğŸ¯ ç¬¬äºŒé˜¶æ®µè§¦å‘ç‡: {statistics.get('stage2_trigger_rate', 0):.1f}%")
        print(f"ğŸ“Š å¹³å‡ç½®ä¿¡åº¦: {statistics.get('average_confidence', 0):.2f}")
        
        # æ˜¾ç¤ºé«˜é¢‘æ ‡ç­¾
        tag_freq = statistics.get('tag_frequency', {})
        if tag_freq:
            top_tags = sorted(tag_freq.items(), key=lambda x: x[1], reverse=True)[:5]
            print(f"ğŸ·ï¸ é«˜é¢‘æ ‡ç­¾: {', '.join([f'{tag}({count})' for tag, count in top_tags])}")
        
        # æ˜¾ç¤ºæ£€æµ‹åˆ°çš„å“ç‰Œ
        brand_freq = statistics.get('brand_frequency', {})
        if brand_freq:
            brands = list(brand_freq.keys())
            print(f"ğŸ æ£€æµ‹å“ç‰Œ: {', '.join(brands)}")
        else:
            print("ğŸ æ£€æµ‹å“ç‰Œ: æ— ")
        
        print(f"ğŸ“ è¾“å‡ºç›®å½•: {batch_result.get('output_directory', 'N/A')}")
        print("="*60)

    def _generate_batch_report(self, results: List[Dict], failed_files: List[Dict], analysis_type: str) -> Dict[str, Any]:
        """ç”Ÿæˆæ‰¹é‡å¤„ç†æŠ¥å‘Šï¼ˆä¼˜åŒ–ä¸ºç‰‡æ®µåˆ†æï¼‰"""
        try:
            total_files = len(results) + len(failed_files)
            success_rate = len(results) / total_files * 100 if total_files > 0 else 0
            
            # ç»Ÿè®¡åˆ†ææ–¹æ³•
            analysis_methods = {}
            for result in results:
                method = result.get("analysis_method", "unknown")
                analysis_methods[method] = analysis_methods.get(method, 0) + 1
            
            # ç»Ÿè®¡æ ‡ç­¾é¢‘æ¬¡ï¼ˆç‰‡æ®µçº§åˆ«ï¼‰
            tag_stats = self._generate_segment_tag_statistics(results)
            
            # è´¨é‡ç»Ÿè®¡
            quality_stats = self._generate_quality_statistics(results)
            
            # éŸ³é¢‘åˆ†æç»Ÿè®¡ï¼ˆå¦‚æœåŒ…å«éŸ³é¢‘åˆ†æï¼‰
            audio_stats = self._generate_audio_statistics(results) if analysis_type in ["audio", "full"] else {}
            
            report = {
                "success": True,
                "summary": {
                    "total_files": total_files,
                    "successful_files": len(results),
                    "failed_files": len(failed_files),
                    "success_rate": f"{success_rate:.1f}%",
                    "analysis_type": analysis_type
                },
                "analysis_methods": analysis_methods,
                "tag_statistics": tag_stats,
                "quality_statistics": quality_stats,
                "failed_files": failed_files[:10]  # åªæ˜¾ç¤ºå‰10ä¸ªå¤±è´¥æ–‡ä»¶
            }
            
            # æ·»åŠ éŸ³é¢‘ç»Ÿè®¡ï¼ˆå¦‚æœæœ‰ï¼‰
            if audio_stats:
                report["audio_statistics"] = audio_stats
            
            return report
            
        except Exception as e:
            logger.error(f"ç”Ÿæˆæ‰¹é‡æŠ¥å‘Šå¤±è´¥: {str(e)}")
            return {"error": str(e), "success": False}
    
    def _generate_segment_tag_statistics(self, results: List[Dict]) -> Dict[str, Any]:
        """ç”Ÿæˆç‰‡æ®µçº§åˆ«çš„æ ‡ç­¾ç»Ÿè®¡"""
        try:
            # ç»Ÿè®¡å„ç±»æ ‡ç­¾
            object_tags = {}
            scene_tags = {}
            emotion_tags = {}
            brand_tags = {}
            
            for result in results:
                # ç»Ÿè®¡objectæ ‡ç­¾
                objects = result.get("object", "").split(", ")
                for obj in objects:
                    if obj.strip():
                        object_tags[obj.strip()] = object_tags.get(obj.strip(), 0) + 1
                
                # ç»Ÿè®¡sceneæ ‡ç­¾
                scenes = result.get("scene", "").split(", ")
                for scene in scenes:
                    if scene.strip():
                        scene_tags[scene.strip()] = scene_tags.get(scene.strip(), 0) + 1
                
                # ç»Ÿè®¡emotionæ ‡ç­¾
                emotion = result.get("emotion", "").strip()
                if emotion:
                    emotion_tags[emotion] = emotion_tags.get(emotion, 0) + 1
                
                # ç»Ÿè®¡brandæ ‡ç­¾
                brands = result.get("brand_elements", "").split(", ")
                for brand in brands:
                    if brand.strip():
                        brand_tags[brand.strip()] = brand_tags.get(brand.strip(), 0) + 1
            
            return {
                "top_objects": dict(sorted(object_tags.items(), key=lambda x: x[1], reverse=True)[:10]),
                "top_scenes": dict(sorted(scene_tags.items(), key=lambda x: x[1], reverse=True)[:10]),
                "top_emotions": dict(sorted(emotion_tags.items(), key=lambda x: x[1], reverse=True)[:10]),
                "top_brands": dict(sorted(brand_tags.items(), key=lambda x: x[1], reverse=True)[:10]),
                "total_unique_objects": len(object_tags),
                "total_unique_scenes": len(scene_tags),
                "total_unique_emotions": len(emotion_tags),
                "total_unique_brands": len(brand_tags)
            }
            
        except Exception as e:
            logger.error(f"ç”Ÿæˆæ ‡ç­¾ç»Ÿè®¡å¤±è´¥: {str(e)}")
            return {}
    
    def _generate_audio_statistics(self, results: List[Dict]) -> Dict[str, Any]:
        """ç”ŸæˆéŸ³é¢‘åˆ†æç»Ÿè®¡"""
        try:
            audio_results = [r for r in results if r.get("transcription")]
            
            if not audio_results:
                return {"message": "æ— éŸ³é¢‘åˆ†æç»“æœ"}
            
            # è½¬å½•ç»Ÿè®¡
            total_transcriptions = len(audio_results)
            avg_transcription_length = sum(len(r.get("transcription", "")) for r in audio_results) / total_transcriptions
            
            # éŸ³é¢‘ç½®ä¿¡åº¦ç»Ÿè®¡
            audio_confidences = [r.get("transcription_confidence", 0) for r in audio_results if r.get("transcription_confidence")]
            avg_audio_confidence = sum(audio_confidences) / len(audio_confidences) if audio_confidences else 0
            
            return {
                "total_audio_analyzed": total_transcriptions,
                "avg_transcription_length": f"{avg_transcription_length:.1f} å­—ç¬¦",
                "avg_audio_confidence": f"{avg_audio_confidence:.2f}",
                "audio_success_rate": f"{total_transcriptions / len(results) * 100:.1f}%"
            }
            
        except Exception as e:
            logger.error(f"ç”ŸæˆéŸ³é¢‘ç»Ÿè®¡å¤±è´¥: {str(e)}")
            return {"error": str(e)}

    def _save_batch_results(self, results: List[Dict], analysis_type: str) -> str:
        """ä¿å­˜æ‰¹é‡å¤„ç†ç»“æœ"""
        try:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            output_file = self.output_config["output_dir"] / f"batch_analysis_{analysis_type}_{timestamp}.json"
            
            # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
            output_file.parent.mkdir(parents=True, exist_ok=True)
            
            # ä¿å­˜ç»“æœ
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(results, f, ensure_ascii=False, indent=2)
            
            logger.info(f"ğŸ“ æ‰¹é‡ç»“æœå·²ä¿å­˜: {output_file}")
            return str(output_file)
            
        except Exception as e:
            logger.error(f"ä¿å­˜æ‰¹é‡ç»“æœå¤±è´¥: {str(e)}")
            return ""
    
    def _generate_quality_statistics(self, results: List[Dict]) -> Dict[str, Any]:
        """ç”Ÿæˆè´¨é‡ç»Ÿè®¡"""
        try:
            if not results:
                return {}
            
            # ç½®ä¿¡åº¦ç»Ÿè®¡
            confidences = [r.get("confidence", 0) for r in results]
            avg_confidence = sum(confidences) / len(confidences)
            high_confidence_count = sum(1 for c in confidences if c > 0.7)
            
            # æ–‡ä»¶å¤§å°ç»Ÿè®¡
            file_sizes = [r.get("file_size_mb", 0) for r in results]
            avg_file_size = sum(file_sizes) / len(file_sizes) if file_sizes else 0
            
            return {
                "avg_confidence": f"{avg_confidence:.2f}",
                "high_confidence_rate": f"{high_confidence_count / len(results) * 100:.1f}%",
                "avg_file_size_mb": f"{avg_file_size:.2f}",
                "total_processed": len(results)
            }
            
        except Exception as e:
            logger.error(f"ç”Ÿæˆè´¨é‡ç»Ÿè®¡å¤±è´¥: {str(e)}")
            return {}

    def _scan_video_files(self, directory: str) -> List[str]:
        """æ‰«æç›®å½•ä¸­çš„è§†é¢‘æ–‡ä»¶ï¼Œå¹¶è¿‡æ»¤æ— æ•ˆæ–‡ä»¶"""
        video_extensions = ['.mp4', '.avi', '.mov', '.mkv', '.m4v', '.webm']
        video_files = []
        filtered_count = 0  # è¿‡æ»¤æ–‡ä»¶è®¡æ•°
        
        def _should_filter_video_file(file_path: Path) -> bool:
            """åˆ¤æ–­è§†é¢‘æ–‡ä»¶æ˜¯å¦åº”è¯¥è¢«è¿‡æ»¤"""
            # ğŸ¯ ç”¨æˆ·åé¦ˆï¼šå¤šé•œå¤´è§†é¢‘ä¹Ÿåº”è¯¥è¢«åˆ†æï¼Œåªè¿‡æ»¤çœŸæ­£å¤±è´¥çš„æ–‡ä»¶
            # åªè¿‡æ»¤âŒå‰ç¼€çš„æ–‡ä»¶ï¼ˆåˆ†æå¤±è´¥ï¼‰ï¼Œâ™»ï¸æ–‡ä»¶å…è®¸æ­£å¸¸åˆ†æ
            if file_path.stem.startswith("âŒ"):
                return True
            return False
        
        for root, dirs, files in os.walk(directory):
            for file in files:
                file_path = Path(root) / file
                if file_path.suffix.lower() in video_extensions:
                    # ğŸš¨ æ–°å¢ï¼šè¿‡æ»¤é€»è¾‘
                    if _should_filter_video_file(file_path):
                        filtered_count += 1
                        logger.debug(f"ğŸš« è¿‡æ»¤è§†é¢‘æ–‡ä»¶: {file_path.name} (è´¨é‡é—®é¢˜)")
                        continue
                    video_files.append(str(file_path))
        
        if filtered_count > 0:
            logger.info(f"ğŸš« æ‰¹é‡å¤„ç†è¿‡æ»¤äº† {filtered_count} ä¸ªè´¨é‡é—®é¢˜è§†é¢‘æ–‡ä»¶")
        
        return sorted(video_files)

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(
        description="æ‰¹é‡è§†é¢‘ç‰‡æ®µæ ‡ç­¾åˆ†æå·¥å…· - åŸºäºåŒå±‚è¯†åˆ«æœºåˆ¶",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ç¤ºä¾‹:
  # åŒå±‚è§†è§‰æœºåˆ¶ï¼ˆæ¨èï¼‰
  python batch_slice_to_label.py --input data/segments --type dual
  
  # åŒå±‚æœºåˆ¶+éŸ³é¢‘å¢å¼º
  python batch_slice_to_label.py --input data/segments --type enhanced
  
  # å•æ–‡ä»¶åˆ†æ
  python batch_slice_to_label.py --file segment_001.mp4 --type dual
  
  # æ‰¹é‡å¤„ç†ï¼ˆé™åˆ¶æ–‡ä»¶æ•°ï¼‰
  python batch_slice_to_label.py --input data/segments --type dual --max-files 50
        """
    )
    
    # è¾“å…¥å‚æ•°
    input_group = parser.add_mutually_exclusive_group(required=True)
    input_group.add_argument(
        "--input", "-i",
        type=str,
        help="è¾“å…¥ç›®å½•è·¯å¾„ï¼ˆæ‰¹é‡å¤„ç†ï¼‰"
    )
    input_group.add_argument(
        "--file", "-f", 
        type=str,
        help="å•ä¸ªè§†é¢‘æ–‡ä»¶è·¯å¾„"
    )
    
    # åˆ†æç±»å‹
    parser.add_argument(
        "--type", "-t",
        choices=["dual", "enhanced"],
        default="dual",
        help="åˆ†æç±»å‹ï¼šdual(åŒå±‚è§†è§‰æœºåˆ¶), enhanced(åŒå±‚+éŸ³é¢‘å¢å¼º) [é»˜è®¤: dual]"
    )
    
    # æ‰¹é‡å¤„ç†é€‰é¡¹
    parser.add_argument(
        "--max-files",
        type=int,
        help="æœ€å¤§å¤„ç†æ–‡ä»¶æ•°ï¼ˆä»…æ‰¹é‡å¤„ç†æ—¶æœ‰æ•ˆï¼‰"
    )
    
    # è¾“å‡ºé€‰é¡¹
    parser.add_argument(
        "--output", "-o",
        type=str,
        help="è¾“å‡ºç›®å½•è·¯å¾„ï¼ˆå¯é€‰ï¼Œé»˜è®¤ä½¿ç”¨é…ç½®ä¸­çš„è·¯å¾„ï¼‰"
    )
    
    # è°ƒè¯•é€‰é¡¹
    parser.add_argument(
        "--verbose", "-v",
        action="store_true",
        help="è¯¦ç»†è¾“å‡ºæ¨¡å¼"
    )
    
    args = parser.parse_args()
    
    # è®¾ç½®æ—¥å¿—çº§åˆ«
    if args.verbose:
        logging.getLogger().setLevel(logging.DEBUG)
        logger.info("ğŸ” è¯¦ç»†è¾“å‡ºæ¨¡å¼å·²å¯ç”¨")
    
    try:
        # éªŒè¯é…ç½®
        if not validate_config():
            logger.error("âŒ é…ç½®éªŒè¯å¤±è´¥ï¼Œè¯·æ£€æŸ¥APIå¯†é’¥å’Œé…ç½®")
            return 1
        
        logger.info("ğŸ¯ slice_to_label æ‰¹é‡åˆ†æå·¥å…·å¯åŠ¨")
        logger.info(f"ğŸ“Š åˆ†æç±»å‹: {args.type}")
        
        # åˆ›å»ºå¤„ç†å™¨
        processor = BatchSliceToLabelProcessor()
        
        # æ‰§è¡Œå¤„ç†
        if args.file:
            # å•æ–‡ä»¶å¤„ç†
            logger.info(f"ğŸ¬ å•æ–‡ä»¶æ¨¡å¼: {args.file}")
            result = processor.process_single_file(args.file, args.type)
            
            if result.get("success"):
                logger.info("âœ… å•æ–‡ä»¶å¤„ç†æˆåŠŸ")
                print(f"\nğŸ¯ åˆ†æç»“æœ:")
                print(f"ğŸ“ æ–‡ä»¶: {result.get('file_name', 'N/A')}")
                print(f"ğŸ“Š åˆ†ææ–¹æ³•: {result.get('analysis_method', 'N/A')}")
                print(f"ğŸ·ï¸ ç‰©ä½“æ ‡ç­¾: {result.get('object', 'æ— ')}")
                print(f"ğŸ¬ åœºæ™¯æ ‡ç­¾: {result.get('scene', 'æ— ')}")
                print(f"ğŸ˜Š æƒ…ç»ªæ ‡ç­¾: {result.get('emotion', 'æ— ')}")
                print(f"ğŸ¢ å“ç‰Œæ ‡ç­¾: {result.get('brand_elements', 'æ— ')}")
                print(f"ğŸ“ˆ ç½®ä¿¡åº¦: {result.get('confidence', 0):.2f}")
                
                if result.get('transcription'):
                    print(f"ğŸ¤ è¯­éŸ³è½¬å½•: {result.get('transcription', '')[:100]}...")
                
            else:
                logger.error(f"âŒ å•æ–‡ä»¶å¤„ç†å¤±è´¥: {result.get('error', 'æœªçŸ¥é”™è¯¯')}")
                return 1
                
        else:
            # æ‰¹é‡å¤„ç†
            logger.info(f"ğŸ“‚ æ‰¹é‡å¤„ç†æ¨¡å¼: {args.input}")
            result = processor.process_batch(args.input, args.type, args.max_files)
            
            if result.get("success"):
                summary = result.get("summary", {})
                logger.info("âœ… æ‰¹é‡å¤„ç†å®Œæˆ")
                print(f"\nğŸ“Š æ‰¹é‡å¤„ç†æ€»ç»“:")
                print(f"ğŸ“‹ æ€»æ–‡ä»¶æ•°: {summary.get('total_files', 0)}")
                print(f"âœ… æˆåŠŸæ–‡ä»¶: {summary.get('successful_files', 0)}")
                print(f"âŒ å¤±è´¥æ–‡ä»¶: {summary.get('failed_files', 0)}")
                print(f"ğŸ“ˆ æˆåŠŸç‡: {summary.get('success_rate', '0%')}")
                print(f"ğŸ“Š åˆ†æç±»å‹: {summary.get('analysis_type', 'N/A')}")
                
                if result.get("output_file"):
                    print(f"ğŸ“ ç»“æœæ–‡ä»¶: {result.get('output_file')}")
                
                # æ˜¾ç¤ºæ ‡ç­¾ç»Ÿè®¡
                tag_stats = result.get("tag_statistics", {})
                if tag_stats:
                    print(f"\nğŸ·ï¸ æ ‡ç­¾ç»Ÿè®¡:")
                    print(f"ğŸ” ç‰©ä½“ç±»å‹: {tag_stats.get('total_unique_objects', 0)} ç§")
                    print(f"ğŸ¬ åœºæ™¯ç±»å‹: {tag_stats.get('total_unique_scenes', 0)} ç§")
                    print(f"ğŸ˜Š æƒ…ç»ªç±»å‹: {tag_stats.get('total_unique_emotions', 0)} ç§")
                    print(f"ğŸ¢ å“ç‰Œç±»å‹: {tag_stats.get('total_unique_brands', 0)} ç§")
                
                # æ˜¾ç¤ºéŸ³é¢‘ç»Ÿè®¡ï¼ˆå¦‚æœæœ‰ï¼‰
                audio_stats = result.get("audio_statistics", {})
                if audio_stats and audio_stats.get("total_audio_analyzed"):
                    print(f"\nğŸ¤ éŸ³é¢‘åˆ†æç»Ÿè®¡:")
                    print(f"ğŸ“ è½¬å½•æ–‡ä»¶: {audio_stats.get('total_audio_analyzed', 0)} ä¸ª")
                    print(f"ğŸ“ å¹³å‡é•¿åº¦: {audio_stats.get('avg_transcription_length', 'N/A')}")
                    print(f"ğŸ“ˆ éŸ³é¢‘ç½®ä¿¡åº¦: {audio_stats.get('avg_audio_confidence', 'N/A')}")
                    print(f"âœ… éŸ³é¢‘æˆåŠŸç‡: {audio_stats.get('audio_success_rate', 'N/A')}")
                
            else:
                logger.error(f"âŒ æ‰¹é‡å¤„ç†å¤±è´¥: {result.get('error', 'æœªçŸ¥é”™è¯¯')}")
                return 1
        
        logger.info("ğŸ‰ åˆ†æä»»åŠ¡å®Œæˆ")
        return 0
        
    except KeyboardInterrupt:
        logger.info("â¹ï¸ ç”¨æˆ·ä¸­æ–­æ“ä½œ")
        return 1
    except Exception as e:
        logger.error(f"ğŸ’¥ ç¨‹åºå¼‚å¸¸: {str(e)}")
        return 1

if __name__ == "__main__":
    main() 