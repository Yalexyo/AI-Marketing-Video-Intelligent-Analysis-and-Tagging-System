"""
è§†é¢‘åˆ‡ç‰‡AIåˆ†æå™¨æ¨¡å—
ä¸“é—¨åˆ†æå·²æœ‰è§†é¢‘åˆ‡ç‰‡ï¼Œæå–æ ‡ç­¾å¹¶åˆ†ç±»åˆ°ä¸šåŠ¡æ¨¡å—

æ ¸å¿ƒåŠŸèƒ½ï¼š
- Qwenè§†è§‰åˆ†æï¼šç‰©ä½“ã€åœºæ™¯ã€æƒ…ç»ªã€å“ç‰Œè¯†åˆ«
- DashScopeéŸ³é¢‘è½¬å½•ï¼šé«˜ç²¾åº¦è¯­éŸ³è¯†åˆ«  
- DeepSeekè¯­ä¹‰åˆ†æï¼šè½¬å½•å†…å®¹ç†è§£å’Œä¸šåŠ¡åˆ†ç±»
"""

import os
import sys
import json
import logging
import tempfile
import requests
import base64
from typing import Dict, Any, List, Optional
from pathlib import Path
import time
try:
    import cv2  # type: ignore
    import numpy as np  # type: ignore
except ImportError as e:
    print(f"âŒ ç¼ºå°‘ä¾èµ–åŒ…: {e}")
    print("ğŸ’¡ è¯·è¿è¡Œ: uv add opencv-python numpy")
import re

# æ·»åŠ ä¸»é¡¹ç›®è·¯å¾„ï¼Œä»¥ä¾¿å¯¼å…¥ä¸»ç¨‹åºçš„åˆ†æå™¨
current_dir = Path(__file__).parent.parent.parent
sys.path.insert(0, str(current_dir / "streamlit_app"))

from config.slice_config import (
    get_api_keys, get_models_config, get_core_brands, 
    get_brand_trigger_keywords, get_analysis_prompts, get_quality_control, get_output_config,
    get_default_model_selection
)

# å¯¼å…¥ç»Ÿä¸€æç¤ºè¯ç®¡ç†
from config.prompt_templates import get_unified_prompt, get_prompt_manager

logger = logging.getLogger(__name__)

class QwenVideoAnalyzer:
    """Qwenè§†è§‰åˆ†æå™¨ - ç‹¬ç«‹å®ç°"""
    
    def __init__(self, api_key: str):
        self.api_key = api_key
        self.model_config = get_models_config()["qwen"]
        
    def analyze_video_frames(self, video_path: str, prompt: str) -> Dict[str, Any]:
        """åˆ†æè§†é¢‘å¸§ï¼Œè¾“å‡ºåŸå§‹æ ¼å¼ï¼Œä¸è¿›è¡Œå³æ—¶ç¿»è¯‘"""
        try:
            # ä½¿ç”¨ä¼˜åŒ–çš„å¸§æå–ç­–ç•¥
            frames = self._extract_frames_optimized(video_path)
            if not frames:
                return {"success": False, "error": "æ— æ³•æå–è§†é¢‘å¸§"}
            
            # ç¼–ç å¸§ä¸ºbase64
            encoded_frames = []
            for frame in frames:  # ä½¿ç”¨æ‰€æœ‰æå–çš„å¸§
                _, buffer = cv2.imencode('.jpg', frame, [cv2.IMWRITE_JPEG_QUALITY, 85])
                encoded_frame = base64.b64encode(buffer).decode('utf-8')
                encoded_frames.append(encoded_frame)
            
            logger.info(f"ğŸ“¸ æå–äº† {len(encoded_frames)} å¸§ç”¨äºåˆ†æ")
            
            # è°ƒç”¨Qwen API
            response = self._call_qwen_api(encoded_frames, prompt)
            
            if response.get("success"):
                raw_output = response["content"]
                
                # ğŸ”¥ å…³é”®ä¿®å¤ï¼šå¤„ç†Qwen APIè¿”å›çš„åˆ—è¡¨æ ¼å¼
                logger.info("ğŸ“‹ Qwenåˆ†æå®Œæˆï¼Œä¿ç•™åŸå§‹æ ¼å¼ï¼ˆç¿»è¯‘å°†åœ¨åå¤„ç†é˜¶æ®µç»Ÿä¸€è¿›è¡Œï¼‰")
                logger.info(f"ğŸ“‹ QwenåŸå§‹è¾“å‡º: {str(raw_output)[:200]}...")
                
                # æå–æ–‡æœ¬å†…å®¹
                extracted_text = self._extract_qwen_text_content(raw_output)
                
                # åªåšåŸºæœ¬æ ¼å¼æ¸…ç†ï¼Œä¸ç¿»è¯‘
                cleaned_output = self._clean_qwen_output(extracted_text)
                
                logger.info(f"ğŸ“‹ æ ¼å¼æ¸…ç†åè¾“å‡º: {cleaned_output[:200]}...")
                
                return {
                    "success": True,
                    "analysis_result": cleaned_output,  # è¿”å›æ¸…ç†åä½†æœªç¿»è¯‘çš„ç»“æœ
                    "raw_output": raw_output,           # ä¿ç•™åŸå§‹è¾“å‡ºç”¨äºè°ƒè¯•
                    "confidence": 0.85,
                    "model_used": "qwen-vl-max-latest",
                    "method_used": "multi_frame_raw_output"
                }
            else:
                return {"success": False, "error": response.get("error", "APIè°ƒç”¨å¤±è´¥")}
                
        except Exception as e:
            logger.error(f"Qwenè§†é¢‘åˆ†æå¤±è´¥: {str(e)}")
            return {"success": False, "error": str(e)}
    
    def _extract_qwen_text_content(self, raw_output) -> str:
        """æå–Qwen APIè¿”å›çš„æ–‡æœ¬å†…å®¹"""
        try:
            # å¦‚æœæ˜¯åˆ—è¡¨æ ¼å¼
            if isinstance(raw_output, list):
                if len(raw_output) > 0 and isinstance(raw_output[0], dict):
                    # æå–textå­—æ®µ
                    if 'text' in raw_output[0]:
                        return raw_output[0]['text']
                    elif 'content' in raw_output[0]:
                        return raw_output[0]['content']
                # å¦‚æœåˆ—è¡¨ä¸­æ˜¯å­—ç¬¦ä¸²
                elif len(raw_output) > 0 and isinstance(raw_output[0], str):
                    return raw_output[0]
            
            # å¦‚æœæ˜¯å­—å…¸æ ¼å¼
            elif isinstance(raw_output, dict):
                if 'text' in raw_output:
                    return raw_output['text']
                elif 'content' in raw_output:
                    return raw_output['content']
            
            # å¦‚æœå·²ç»æ˜¯å­—ç¬¦ä¸²
            elif isinstance(raw_output, str):
                return raw_output
            
            # é»˜è®¤æƒ…å†µï¼šè½¬æ¢ä¸ºå­—ç¬¦ä¸²
            logger.warning(f"âš ï¸ æœªçŸ¥çš„Qwenè¾“å‡ºæ ¼å¼ï¼Œè½¬æ¢ä¸ºå­—ç¬¦ä¸²: {type(raw_output)}")
            return str(raw_output)
                
        except Exception as e:
            logger.error(f"âŒ æå–Qwenæ–‡æœ¬å†…å®¹å¤±è´¥: {e}")
            return str(raw_output)
    
    def _clean_qwen_output(self, text: str) -> str:
        """æ¸…ç†Qwenè¾“å‡ºæ ¼å¼ï¼Œç§»é™¤æ–¹æ‹¬å·å’Œå¤šä½™ç¬¦å·"""
        if not text:
            return text
        
        # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼è¿›è¡Œæ ¼å¼æ¸…ç†
        import re
        
        try:
            # å…ˆå°è¯•è§£æJSONæ ¼å¼
            import json
            if text.strip().startswith('{') and text.strip().endswith('}'):
                try:
                    data = json.loads(text)
                    # æ¸…ç†æ‰€æœ‰å­—æ®µæ ¼å¼
                    for key, value in data.items():
                        if isinstance(value, str):
                            # å»é™¤æ–¹æ‹¬å·å’Œå¤šä½™ç¬¦å·
                            value = re.sub(r'^\[([^\]]+)\]$', r'\1', value.strip())
                            value = value.strip().strip('"').strip("'").strip()
                            data[key] = value
                    
                    # è¿”å›æ ¼å¼åŒ–çš„JSON
                    return json.dumps(data, ensure_ascii=False, separators=(',', ': '))
                except json.JSONDecodeError:
                    pass
        except:
            pass
        
        # å¦‚æœä¸æ˜¯JSONï¼ŒæŒ‰è¡Œå¤„ç†
        lines = text.split('\n')
        cleaned_lines = []
        
        for line in lines:
            if ':' in line:
                key, value = line.split(':', 1)
                key = key.strip()
                value = value.strip()
                
                # ç§»é™¤æ–¹æ‹¬å·å’Œå¼•å·
                value = value.replace('[', '').replace(']', '')
                value = value.replace('"', '').replace("'", '')
                
                # æ¸…ç†é€—å·
                if value.startswith(','):
                    value = value[1:]
                if value.endswith(','):
                    value = value[:-1]
                
                # æ¸…ç†ç©ºæ ¼
                value = ' '.join(value.split())
                
                # ç‰¹æ®Šå¤„ç†
                if value.lower().strip() in ['æ— ', 'none', 'null', '']:
                    value = 'æ— '
                
                cleaned_lines.append(f"{key}: {value}")
            else:
                cleaned_lines.append(line)
        
        return '\n'.join(cleaned_lines)
    
    def _get_deepseek_api_key(self) -> str:
        """è·å–DeepSeek APIå¯†é’¥"""
        try:
            import os
            # é¦–å…ˆå°è¯•ç¯å¢ƒå˜é‡
            api_key = os.getenv("DEEPSEEK_API_KEY")
            if api_key:
                return api_key
            
            # ç„¶åå°è¯•ä»é…ç½®æ–‡ä»¶è¯»å–
            config_path = Path(__file__).parent.parent / "config" / "env_config.txt"
            if config_path.exists():
                with open(config_path, 'r', encoding='utf-8') as f:
                    for line in f:
                        if line.startswith("DEEPSEEK_API_KEY="):
                            return line.split("=", 1)[1].strip()
            
            logger.error("âŒ æœªæ‰¾åˆ°DeepSeek APIå¯†é’¥")
            return ""
            
        except Exception as e:
            logger.error(f"âŒ è·å–DeepSeek APIå¯†é’¥å¤±è´¥: {e}")
            return ""
    
    def _extract_frames_optimized(self, video_path: str) -> List:
        """ä¼˜åŒ–çš„è§†é¢‘å¸§æå–ç­–ç•¥ - é’ˆå¯¹çŸ­è§†é¢‘ç‰‡æ®µä¼˜åŒ–"""
        try:
            cap = cv2.VideoCapture(video_path)
            frames = []
            
            # è·å–è§†é¢‘åŸºæœ¬ä¿¡æ¯
            total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
            fps = cap.get(cv2.CAP_PROP_FPS)
            duration = total_frames / fps if fps > 0 else 0
            
            logger.info(f"ğŸ“¹ è§†é¢‘ä¿¡æ¯: {total_frames}å¸§, {fps:.1f}fps, {duration:.1f}ç§’")
            
            if total_frames == 0:
                cap.release()
                return frames
            
            # æ ¹æ®è§†é¢‘é•¿åº¦åŠ¨æ€é€‰æ‹©å¸§æ•°å’Œç­–ç•¥
            if duration <= 3:  # æçŸ­è§†é¢‘ (â‰¤3ç§’)
                frames = self._extract_short_video_frames(cap, total_frames, fps)
            elif duration <= 10:  # çŸ­è§†é¢‘ (3-10ç§’)
                frames = self._extract_medium_video_frames(cap, total_frames, fps)
            else:  # è¾ƒé•¿è§†é¢‘ (>10ç§’)
                frames = self._extract_long_video_frames(cap, total_frames, fps)
            
            cap.release()
            
            # å¸§è´¨é‡è¯„ä¼°å’Œè¿‡æ»¤
            frames = self._filter_quality_frames(frames)
            
            logger.info(f"âœ… æœ€ç»ˆæå– {len(frames)} ä¸ªé«˜è´¨é‡å¸§")
            return frames
            
        except Exception as e:
            logger.error(f"ä¼˜åŒ–å¸§æå–å¤±è´¥: {str(e)}")
            return []
    
    def _extract_short_video_frames(self, cap, total_frames: int, fps: float) -> List:
        """æå–æçŸ­è§†é¢‘çš„å¸§ (â‰¤3ç§’) - å¯†é›†é‡‡æ ·"""
        frames = []
        
        # å¯¹äºæçŸ­è§†é¢‘ï¼Œæ¯éš”å‡ å¸§å°±é‡‡æ ·ä¸€æ¬¡ï¼Œç¡®ä¿æ•æ‰åˆ°æ‰€æœ‰é‡è¦å˜åŒ–
        step = max(1, int(total_frames / 8))  # æœ€å¤š8å¸§
        
        for i in range(0, total_frames, step):
            cap.set(cv2.CAP_PROP_POS_FRAMES, i)
            ret, frame = cap.read()
            if ret and frame is not None:
                frames.append(frame)
        
        # ç¡®ä¿åŒ…å«é¦–å°¾å¸§
        if total_frames > 1:
            cap.set(cv2.CAP_PROP_POS_FRAMES, total_frames - 1)
            ret, frame = cap.read()
            if ret and frame is not None:
                frames.append(frame)
        
        return frames[:6]  # æœ€å¤š6å¸§
    
    def _extract_medium_video_frames(self, cap, total_frames: int, fps: float) -> List:
        """æå–çŸ­è§†é¢‘çš„å¸§ (3-10ç§’) - å…³é”®æ—¶åˆ»é‡‡æ ·"""
        frames = []
        
        # å…³é”®æ—¶åˆ»ç‚¹ï¼šå¼€å§‹ã€1/4ã€1/2ã€3/4ã€ç»“æŸ
        key_positions = [0, 0.25, 0.5, 0.75, 1.0]
        
        for pos in key_positions:
            frame_idx = int(pos * (total_frames - 1))
            cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)
            ret, frame = cap.read()
            if ret and frame is not None:
                frames.append(frame)
        
        return frames
    
    def _extract_long_video_frames(self, cap, total_frames: int, fps: float) -> List:
        """æå–è¾ƒé•¿è§†é¢‘çš„å¸§ (>10ç§’) - å‡åŒ€é‡‡æ ·"""
        frames = []
        
        # å‡åŒ€é€‰æ‹©4ä¸ªå…³é”®å¸§
        indices = [int(i * total_frames / 4) for i in range(4)]
        
        for idx in indices:
            cap.set(cv2.CAP_PROP_POS_FRAMES, idx)
            ret, frame = cap.read()
            if ret and frame is not None:
                frames.append(frame)
        
        return frames
    
    def _filter_quality_frames(self, frames: List) -> List:
        """è¿‡æ»¤å’Œè¯„ä¼°å¸§è´¨é‡"""
        if not frames:
            return frames
        
        quality_frames = []
        
        for frame in frames:
            # æ£€æŸ¥å¸§æ˜¯å¦æœ‰æ•ˆ
            if frame is None or frame.size == 0:
                continue
            
            # è®¡ç®—å¸§çš„è´¨é‡æŒ‡æ ‡
            quality_score = self._calculate_frame_quality(frame)
            
            # åªä¿ç•™è´¨é‡è¶³å¤Ÿçš„å¸§
            if quality_score > 0.3:  # è´¨é‡é˜ˆå€¼
                quality_frames.append(frame)
        
        # å¦‚æœæ‰€æœ‰å¸§éƒ½è¢«è¿‡æ»¤æ‰äº†ï¼Œä¿ç•™åŸå§‹çš„ç¬¬ä¸€å¸§
        if not quality_frames and frames:
            quality_frames = [frames[0]]
        
        # é™åˆ¶æœ€å¤§å¸§æ•°ï¼ˆé¿å…APIè°ƒç”¨è¿‡å¤§ï¼‰
        return quality_frames[:5]
    
    def _calculate_frame_quality(self, frame) -> float:
        """è®¡ç®—å¸§çš„è´¨é‡åˆ†æ•°"""
        try:
            # è½¬æ¢ä¸ºç°åº¦å›¾
            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
            
            # è®¡ç®—æ‹‰æ™®æ‹‰æ–¯æ–¹å·®ï¼ˆæ¸…æ™°åº¦æŒ‡æ ‡ï¼‰
            laplacian_var = cv2.Laplacian(gray, cv2.CV_64F).var()
            
            # è®¡ç®—äº®åº¦åˆ†å¸ƒï¼ˆé¿å…è¿‡æš—æˆ–è¿‡äº®ï¼‰
            brightness = gray.mean()
            brightness_score = 1.0 - abs(brightness - 128) / 128
            
            # è®¡ç®—å¯¹æ¯”åº¦
            contrast = gray.std()
            contrast_score = min(contrast / 64, 1.0)
            
            # ç»¼åˆè´¨é‡åˆ†æ•°
            quality = (
                min(laplacian_var / 1000, 1.0) * 0.5 +  # æ¸…æ™°åº¦æƒé‡50%
                brightness_score * 0.3 +                 # äº®åº¦æƒé‡30%
                contrast_score * 0.2                     # å¯¹æ¯”åº¦æƒé‡20%
            )
            
            return quality
            
        except Exception as e:
            logger.warning(f"å¸§è´¨é‡è®¡ç®—å¤±è´¥: {e}")
            return 0.5  # é»˜è®¤ä¸­ç­‰è´¨é‡
    
    def _call_qwen_api(self, encoded_frames: List[str], prompt: str) -> Dict[str, Any]:
        """è°ƒç”¨Qwen API - æ”¯æŒé‡è¯•æœºåˆ¶"""
        import time
        
        url = "https://dashscope.aliyuncs.com/api/v1/services/aigc/multimodal-generation/generation"
        
        # ä»é…ç½®è·å–å‚æ•°
        max_retries = self.model_config.get("max_retries", 5)
        timeout = self.model_config.get("timeout", 90)
        retry_delay = self.model_config.get("retry_delay", 2)
        
        # æ„å»ºæ¶ˆæ¯å†…å®¹
        content = [{"text": prompt}]
        for frame in encoded_frames:
            content.append({
                "image": f"data:image/jpeg;base64,{frame}"
            })
        
        payload = {
            "model": self.model_config["model_name"],
            "input": {
                "messages": [
                    {
                        "role": "user",
                        "content": content
                    }
                ]
            },
            "parameters": {
                "max_tokens": 2000
            }
        }
        
        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json"
        }
        
        # å®ç°é‡è¯•æœºåˆ¶
        for attempt in range(max_retries):
            try:
                logger.info(f"ğŸ”„ Qwen APIè°ƒç”¨å°è¯• {attempt + 1}/{max_retries} (è¶…æ—¶: {timeout}s)")
                
                response = requests.post(url, json=payload, headers=headers, timeout=timeout)
                
                if response.status_code == 200:
                    result = response.json()
                    if result.get("output", {}).get("choices"):
                        content = result["output"]["choices"][0]["message"]["content"]
                        logger.info(f"âœ… Qwen APIè°ƒç”¨æˆåŠŸ (å°è¯•æ¬¡æ•°: {attempt + 1})")
                        return {"success": True, "content": content}
                    else:
                        logger.warning(f"âš ï¸ Qwen APIè¿”å›æ ¼å¼é”™è¯¯ (å°è¯• {attempt + 1})")
                        if attempt < max_retries - 1:
                            logger.info(f"â³ ç­‰å¾… {retry_delay} ç§’åé‡è¯•...")
                            time.sleep(retry_delay)
                            continue
                        return {"success": False, "error": "APIè¿”å›æ ¼å¼é”™è¯¯"}
                else:
                    logger.warning(f"âš ï¸ Qwen APIè°ƒç”¨å¤±è´¥: {response.status_code} (å°è¯• {attempt + 1})")
                    if attempt < max_retries - 1:
                        logger.info(f"â³ ç­‰å¾… {retry_delay} ç§’åé‡è¯•...")
                        time.sleep(retry_delay)
                        continue
                    return {"success": False, "error": f"APIè°ƒç”¨å¤±è´¥: {response.status_code}"}
                    
            except requests.exceptions.Timeout:
                logger.error(f"âŒ Qwen APIè¶…æ—¶ (å°è¯• {attempt + 1}/{max_retries}, è¶…æ—¶: {timeout}s)")
                if attempt < max_retries - 1:
                    logger.info(f"â³ ç½‘ç»œè¶…æ—¶ï¼Œç­‰å¾… {retry_delay * (attempt + 1)} ç§’åé‡è¯•...")
                    time.sleep(retry_delay * (attempt + 1))  # é€’å¢ç­‰å¾…æ—¶é—´
                    continue
                return {"success": False, "error": f"APIè¶…æ—¶ (å·²é‡è¯•{max_retries}æ¬¡)"}
                
            except Exception as e:
                logger.error(f"âŒ Qwen APIå¼‚å¸¸: {str(e)} (å°è¯• {attempt + 1}/{max_retries})")
                if attempt < max_retries - 1:
                    logger.info(f"â³ å¼‚å¸¸åç­‰å¾… {retry_delay * (attempt + 1)} ç§’åé‡è¯•...")
                    time.sleep(retry_delay * (attempt + 1))
                    continue
                return {"success": False, "error": str(e)}
        
        return {"success": False, "error": f"æ‰€æœ‰ {max_retries} æ¬¡é‡è¯•éƒ½å¤±è´¥"}
    
class GeminiVideoAnalyzer:
    """Google Gemini 2.5 Proè§†è§‰åˆ†æå™¨ - é€šè¿‡OpenRouter APIè°ƒç”¨"""
    
    def __init__(self, api_key: str):
        self.api_key = api_key
        self.model_config = get_models_config()["gemini"]
        self.gemini_model = self.model_config["model_name"]  # è®¾ç½®gemini_modelå±æ€§
        
        # ğŸ†• æ·»åŠ  OpenRouter API é…ç½®
        self.openrouter_api_key = os.getenv("OPENROUTER_API_KEY")
        if not self.openrouter_api_key:
            logger.warning("âš ï¸ OPENROUTER_API_KEY æœªé…ç½®ï¼Œå°†å›é€€åˆ°åŸæœ‰ Google API")
            self.use_openrouter = False
        else:
            self.use_openrouter = True
            logger.info("âœ… ä½¿ç”¨ OpenRouter API è°ƒç”¨ Gemini 2.5 Pro")
        
        # ğŸš€ æ–°å¢ï¼šæ™ºèƒ½å¸§æå–å™¨é…ç½®
        self.use_smart_extractor = self.model_config.get("use_smart_extractor", False)
        if self.use_smart_extractor:
            try:
                from smart_frame_extractor import SmartFrameExtractor
                self.smart_extractor = SmartFrameExtractor()
                logger.info("âœ… æ™ºèƒ½å¸§æå–å™¨å·²å¯ç”¨ï¼ˆå†…å®¹å˜åŒ–æ£€æµ‹ï¼‰")
            except ImportError:
                logger.warning("âš ï¸ æ™ºèƒ½å¸§æå–å™¨æ¨¡å—æœªæ‰¾åˆ°ï¼Œä½¿ç”¨æ ‡å‡†æå–æ–¹å¼")
                self.use_smart_extractor = False
                self.smart_extractor = None
        else:
            self.smart_extractor = None
            logger.info("ğŸ“Š ä½¿ç”¨æ ‡å‡†å¸§æå–æ–¹å¼ï¼ˆå·²ä¼˜åŒ–ï¼‰")
    
    def analyze_video_frames(self, video_path: str, prompt: str) -> Dict[str, Any]:
        """ä½¿ç”¨Geminiåˆ†æè§†é¢‘ï¼Œä¼˜å…ˆä½¿ç”¨OpenRouter API"""
        
        try:
            # ğŸ”§ **å…³é”®ä¿®æ­£**ï¼šä½¿ç”¨ä¼ å…¥çš„ç»Ÿä¸€promptï¼Œä¸å†ç¡¬ç¼–ç 
            logger.info(f"ğŸ¯ Geminiæ”¶åˆ°promptï¼Œé•¿åº¦: {len(prompt)}å­—ç¬¦")
            logger.info(f"ğŸ“‹ Geminiæ¥æ”¶çš„promptå¼€å¤´: {prompt[:300]}...")
            
            # ğŸ” **è°ƒè¯•**ï¼šæ£€æŸ¥æ˜¯å¦ä¸ºä¸“ç”¨prompt
            if "Geminiå¢å¼ºæŒ‡å¯¼" in prompt:
                logger.info("âœ… ç¡®è®¤æ”¶åˆ°Geminiä¸“ç”¨promptï¼ˆåŒ…å«å¢å¼ºæŒ‡å¯¼ï¼‰")
            elif "è¡Œä¸º/äº¤äº’" in prompt:
                logger.info("âš ï¸ æ”¶åˆ°é€šç”¨promptï¼ˆé€šç”¨ç‰ˆæœ¬ï¼‰")
            else:
                logger.warning("â“ æ”¶åˆ°æœªçŸ¥æ ¼å¼çš„prompt")
            
            logger.info(f"ğŸ“¹ Geminiå¼€å§‹åˆ†æè§†é¢‘: {video_path}")
            
            # ğŸ†• ä¼˜å…ˆä½¿ç”¨ OpenRouter API
            if self.use_openrouter:
                response = self._call_gemini_openrouter_api(video_path, prompt)
            else:
                # å›é€€åˆ°åŸæœ‰çš„ Google APIï¼ˆéœ€è¦å¯¼å…¥åº“ï¼‰
                response = self._call_gemini_google_api_fallback(video_path, prompt)
            
            if response.get("success"):
                raw_output = response["content"]
                
                # ğŸ”¥ ç®€åŒ–å¤„ç†ï¼šç›´æ¥è¿”å›åŸå§‹è¾“å‡ºï¼Œä¸åšå¤æ‚è§£æ
                logger.info("ğŸ“‹ Geminiåˆ†æå®Œæˆï¼Œä¿ç•™åŸå§‹æ ¼å¼ï¼ˆç¿»è¯‘å°†åœ¨åå¤„ç†é˜¶æ®µç»Ÿä¸€è¿›è¡Œï¼‰")
                logger.info(f"ğŸ“‹ GeminiåŸå§‹è¾“å‡º: {raw_output[:200]}...")
                
                # åªåšåŸºæœ¬æ ¼å¼æ¸…ç†ï¼Œä¿æŒåŸå§‹å†…å®¹
                cleaned_output = raw_output.strip()
                
                return {
                    "success": True,
                    "analysis_result": cleaned_output,  # ç›´æ¥è¿”å›æ¸…ç†åçš„ç»“æœ
                    "raw_output": raw_output,           # ä¿ç•™åŸå§‹è¾“å‡ºç”¨äºè°ƒè¯•
                    "confidence": 0.90,
                    "model_used": "gemini-2.5-pro",
                    "method_used": "openrouter_api" if self.use_openrouter else "google_api_fallback",
                    "prompt_type": "gemini_specialized" if "Geminiå¢å¼ºæŒ‡å¯¼" in prompt else "universal"
                }
            else:
                # ç›´æ¥æŠ¥é”™ï¼Œä¸ä½¿ç”¨å›é€€æ–¹æ³•
                error_msg = response.get("error", "Gemini APIè°ƒç”¨å¤±è´¥")
                logger.error(f"âŒ Gemini APIå¤±è´¥: {error_msg}")
                raise Exception(f"Gemini APIå¤±è´¥: {error_msg}")
                
        except Exception as e:
            logger.error(f"âŒ Geminiè§†é¢‘åˆ†æå¤±è´¥: {str(e)}")
            raise e
    


    def _call_gemini_openrouter_api(self, video_path: str, prompt: str) -> Dict[str, Any]:
        """ğŸ†• ä½¿ç”¨ OpenRouter API è°ƒç”¨ Gemini 2.5 Pro"""
        try:
            import requests
            import json
            import base64
            
            logger.info("ğŸš€ ä½¿ç”¨ OpenRouter API è°ƒç”¨ Gemini 2.5 Pro")
            
            # ğŸ¯ æ ¹æ®é…ç½®é€‰æ‹©å¸§æå–æ–¹å¼
            if self.use_smart_extractor and self.smart_extractor:
                logger.info("ğŸ§  ä½¿ç”¨æ™ºèƒ½å¸§æå–å™¨ï¼ˆå†…å®¹å˜åŒ–æ£€æµ‹ï¼‰")
                key_frames_data = self.smart_extractor.extract_key_frames(video_path)
                if key_frames_data:
                    frames = [frame_data["frame"] for frame_data in key_frames_data]
                    logger.info(f"ğŸ¯ æ™ºèƒ½æå–äº† {len(frames)} ä¸ªå…³é”®å¸§ï¼ˆåŸºäºå†…å®¹å˜åŒ–ï¼‰")
                else:
                    logger.warning("âš ï¸ æ™ºèƒ½å¸§æå–å¤±è´¥ï¼Œå›é€€åˆ°æ ‡å‡†æ–¹å¼")
                    frames = self._extract_frames_optimized(video_path)
            else:
                # ä½¿ç”¨ä¼˜åŒ–çš„æ ‡å‡†æå–æ–¹å¼
                frames = self._extract_frames_optimized(video_path)
                
            if not frames:
                return {"success": False, "error": "æ— æ³•ä»è§†é¢‘ä¸­æå–å¸§"}
            
            # å°†å¸§è½¬æ¢ä¸º base64 ç¼–ç 
            encoded_frames = []
            for i, frame in enumerate(frames):
                try:
                    import cv2  # type: ignore
                    # å°†å¸§ç¼–ç ä¸º JPEG
                    _, buffer = cv2.imencode('.jpg', frame)
                    frame_base64 = base64.b64encode(buffer).decode('utf-8')
                    encoded_frames.append(frame_base64)
                    logger.info(f"âœ… å¸§ {i+1} ç¼–ç å®Œæˆ")
                except Exception as e:
                    logger.warning(f"âš ï¸ å¸§ {i+1} ç¼–ç å¤±è´¥: {e}")
                    continue
            
            if not encoded_frames:
                return {"success": False, "error": "å¸§ç¼–ç å¤±è´¥"}
            
            # æ„å»º OpenRouter API è¯·æ±‚
            url = "https://openrouter.ai/api/v1/chat/completions"
            
            headers = {
                "Authorization": f"Bearer {self.openrouter_api_key}",
                "Content-Type": "application/json",
                "HTTP-Referer": "https://ai-video-master.com",  # Optional site URL
                "X-Title": "AI Video Master",  # Optional site title
            }
            
            # æ„å»ºæ¶ˆæ¯å†…å®¹ï¼šprompt + å›¾åƒ
            content = [
                {
                    "type": "text",
                    "text": prompt
                }
            ]
            
            # æ·»åŠ å›¾åƒå¸§
            for i, frame_base64 in enumerate(encoded_frames):
                image_content = {
                    "type": "image_url",
                    "image_url": {
                        "url": f"data:image/jpeg;base64,{frame_base64}"
                    }
                }
                content.append(image_content)  # type: ignore
                logger.info(f"ğŸ“¸ æ·»åŠ å¸§ {i+1} åˆ°è¯·æ±‚")
            
            payload = {
                "model": "google/gemini-2.5-pro",
                "messages": [
                    {
                        "role": "user",
                        "content": content
                    }
                ],
                "max_tokens": 2000,
                "temperature": 0.1
            }
            
            logger.info(f"ğŸ“¤ å‘é€ OpenRouter API è¯·æ±‚ï¼ŒåŒ…å« {len(encoded_frames)} ä¸ªå¸§")
            
            response = requests.post(url, headers=headers, data=json.dumps(payload), timeout=60)
            
            if response.status_code == 200:
                result = response.json()
                if result.get("choices") and result["choices"][0].get("message", {}).get("content"):
                    content = result["choices"][0]["message"]["content"]
                    logger.info(f"âœ… OpenRouter API è°ƒç”¨æˆåŠŸï¼Œå“åº”é•¿åº¦: {len(content)}å­—ç¬¦")
                    return {"success": True, "content": content}
                else:
                    return {"success": False, "error": "OpenRouter APIè¿”å›æ ¼å¼é”™è¯¯"}
            else:
                error_msg = f"OpenRouter APIè°ƒç”¨å¤±è´¥: {response.status_code}"
                if response.text:
                    error_msg += f", å“åº”: {response.text[:200]}"
                return {"success": False, "error": error_msg}
                
        except Exception as e:
            logger.error(f"âŒ OpenRouter APIè°ƒç”¨å¼‚å¸¸: {str(e)}")
            return {"success": False, "error": str(e)}

    def _call_gemini_google_api_fallback(self, video_path: str, prompt: str) -> Dict[str, Any]:
        """ğŸ†• çœŸæ­£çš„GoogleåŸç”ŸAPIè°ƒç”¨"""
        try:
            logger.info("ğŸ‡¬ ä½¿ç”¨GoogleåŸç”ŸAPIè°ƒç”¨Gemini 2.5 Pro")
            
            # æ–¹æ³•1ï¼šå°è¯•æ–°ç‰ˆGoogle Gen AI SDK
            try:
                import google.generativeai as genai  # type: ignore
                
                # é…ç½®APIå¯†é’¥
                genai.configure(api_key=self.api_key)
                
                # åˆ›å»ºæ¨¡å‹
                model = genai.GenerativeModel("gemini-2.5-pro")
                
                # æå–è§†é¢‘å¸§å¹¶è½¬æ¢ä¸ºPIL Image
                frames = self._extract_frames_optimized(video_path)
                if not frames:
                    return {"success": False, "error": "æ— æ³•æå–è§†é¢‘å¸§"}
                
                # è½¬æ¢å¸§ä¸ºPIL Imageæ ¼å¼
                pil_images = []
                for frame in frames:
                    import cv2  # type: ignore
                    from PIL import Image  # type: ignore
                    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                    pil_image = Image.fromarray(frame_rgb)
                    pil_images.append(pil_image)
                
                logger.info(f"ğŸ“¸ Google APIå‡†å¤‡åˆ†æ {len(pil_images)} ä¸ªå¸§")
                
                # æ„å»ºå†…å®¹ï¼šprompt + å›¾åƒ
                content = [prompt]
                content.extend(pil_images)
                
                # ç”Ÿæˆé…ç½®
                generation_config = {
                    "temperature": 0.1,
                    "top_p": 0.8,
                    "top_k": 40,
                    "max_output_tokens": 2000,
                }
                
                # å®‰å…¨è®¾ç½®
                safety_settings = [
                    {"category": "HARM_CATEGORY_HARASSMENT", "threshold": "BLOCK_NONE"},
                    {"category": "HARM_CATEGORY_HATE_SPEECH", "threshold": "BLOCK_NONE"},
                    {"category": "HARM_CATEGORY_SEXUALLY_EXPLICIT", "threshold": "BLOCK_NONE"},
                    {"category": "HARM_CATEGORY_DANGEROUS_CONTENT", "threshold": "BLOCK_NONE"},
                ]
                
                # è°ƒç”¨Google API
                response = model.generate_content(
                    content,
                    generation_config=generation_config,
                    safety_settings=safety_settings
                )
                
                if response.text:
                    logger.info(f"âœ… GoogleåŸç”ŸAPIè°ƒç”¨æˆåŠŸï¼Œå“åº”é•¿åº¦: {len(response.text)}å­—ç¬¦")
                    return {"success": True, "content": response.text}
                else:
                    return {"success": False, "error": "Google APIè¿”å›ç©ºå†…å®¹"}
                    
            except ImportError:
                logger.warning("âš ï¸ Google GenerativeAI SDKæœªå®‰è£…ï¼Œå›é€€åˆ°å¸§åˆ†æ")
                return self._call_gemini_frame_fallback(video_path, prompt)
            except Exception as e:
                logger.warning(f"âš ï¸ GoogleåŸç”ŸAPIè°ƒç”¨å¤±è´¥: {str(e)}")
                logger.info("ğŸ”„ å›é€€åˆ°å¸§åˆ†ææ–¹æ³•")
                return self._call_gemini_frame_fallback(video_path, prompt)
                
        except Exception as e:
            logger.error(f"âŒ Google APIæ•´ä½“è°ƒç”¨å¤±è´¥: {str(e)}")
            return {"success": False, "error": str(e)}

    def _extract_frames_optimized(self, video_path: str) -> List:
        """ä¼˜åŒ–çš„è§†é¢‘å¸§æå–ç­–ç•¥ - ä¸“ä¸ºGeminiä¼˜åŒ–ï¼Œæä¾›æ›´å…¨é¢çš„è§†é¢‘åˆ†æ"""
        try:
            import cv2  # type: ignore
            cap = cv2.VideoCapture(video_path)
            frames = []
            
            # è·å–è§†é¢‘åŸºæœ¬ä¿¡æ¯
            total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
            fps = cap.get(cv2.CAP_PROP_FPS)
            duration = total_frames / fps if fps > 0 else 0
            
            logger.info(f"ğŸ“¹ Geminiè§†é¢‘ä¿¡æ¯: {total_frames}å¸§, {fps:.1f}fps, {duration:.1f}ç§’")
            
            if total_frames == 0:
                cap.release()
                return frames
            
            # ğŸš€ ä¼˜åŒ–åçš„Geminiå¸§æå–ç­–ç•¥ï¼šæ›´å…¨é¢çš„è¦†ç›–
            if duration <= 2:  # æçŸ­è§†é¢‘ï¼šå¯†é›†é‡‡æ ·
                # æ¯0.3ç§’é‡‡æ ·ä¸€æ¬¡ï¼Œç¡®ä¿æ•æ‰æ‰€æœ‰å˜åŒ–
                frame_interval = max(1, int(fps * 0.3))
                for i in range(0, total_frames, frame_interval):
                    cap.set(cv2.CAP_PROP_POS_FRAMES, i)
                    ret, frame = cap.read()
                    if ret and frame is not None:
                        frames.append(frame)
                # ç¡®ä¿åŒ…å«æœ€åä¸€å¸§
                if total_frames > 1:
                    cap.set(cv2.CAP_PROP_POS_FRAMES, total_frames - 1)
                    ret, frame = cap.read()
                    if ret and frame is not None:
                        frames.append(frame)
                frames = frames[:6]  # æçŸ­è§†é¢‘æœ€å¤š6å¸§
                
            elif duration <= 8:  # çŸ­è§†é¢‘ï¼šå…³é”®æ—¶åˆ»é‡‡æ ·
                # æ›´å¯†é›†çš„å…³é”®æ—¶åˆ»ç‚¹
                key_positions = [0, 0.15, 0.35, 0.55, 0.75, 0.95]
                for pos in key_positions:
                    frame_idx = int(pos * (total_frames - 1))
                    cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)
                    ret, frame = cap.read()
                    if ret and frame is not None:
                        frames.append(frame)
                        
            elif duration <= 20:  # ä¸­ç­‰è§†é¢‘ï¼šæ··åˆç­–ç•¥
                # å…³é”®æ—¶åˆ» + å†…å®¹å˜åŒ–æ£€æµ‹
                key_positions = [0, 0.2, 0.4, 0.6, 0.8, 1.0]
                for pos in key_positions:
                    frame_idx = int(pos * (total_frames - 1))
                    cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)
                    ret, frame = cap.read()
                    if ret and frame is not None:
                        frames.append(frame)
                        
                # é¢å¤–é‡‡æ ·ä¸­é—´å˜åŒ–ç‚¹
                mid_points = [0.1, 0.3, 0.5, 0.7, 0.9]
                for pos in mid_points:
                    frame_idx = int(pos * (total_frames - 1))
                    cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)
                    ret, frame = cap.read()
                    if ret and frame is not None:
                        frames.append(frame)
                        
            else:  # é•¿è§†é¢‘ï¼šæ™ºèƒ½é‡‡æ ·
                # åˆ†æ®µç­–ç•¥ï¼šå°†è§†é¢‘åˆ†æˆ8æ®µï¼Œæ¯æ®µå–ä»£è¡¨å¸§
                segments = 8
                for i in range(segments):
                    # æ¯æ®µå–å¼€å§‹ã€ä¸­é—´ã€ç»“æŸä¸‰ä¸ªç‚¹
                    segment_start = i / segments
                    segment_mid = (i + 0.5) / segments
                    segment_end = (i + 1) / segments
                    
                    for pos in [segment_start, segment_mid, segment_end]:
                        if pos <= 1.0:
                            frame_idx = int(pos * (total_frames - 1))
                            cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)
                            ret, frame = cap.read()
                            if ret and frame is not None:
                                frames.append(frame)
            
            cap.release()
            
            # ğŸ”§ ä¼˜åŒ–è´¨é‡è¿‡æ»¤ï¼šæ›´æ™ºèƒ½çš„å¸§é€‰æ‹©
            frames = self._filter_frames_for_gemini_enhanced(frames)
            
            logger.info(f"âœ… Geminiä¼˜åŒ–åæå– {len(frames)} ä¸ªé«˜è´¨é‡å¸§")
            return frames
            
        except Exception as e:
            logger.error(f"Geminiå¸§æå–å¤±è´¥: {str(e)}")
            return []
    
    def _filter_frames_for_gemini_enhanced(self, frames: List) -> List:
        """å¢å¼ºç‰ˆGeminiå¸§è¿‡æ»¤ - æ™ºèƒ½é€‰æ‹©æœ€æœ‰ä»£è¡¨æ€§çš„å¸§"""
        if not frames:
            return frames
        
        # ğŸ¯ ç¬¬ä¸€æ­¥ï¼šè´¨é‡è¯„ä¼°
        frame_scores = []
        for i, frame in enumerate(frames):
            if frame is None or frame.size == 0:
                continue
            
            quality_score = self._calculate_gemini_frame_quality(frame)
            if quality_score > 0.3:  # é™ä½è´¨é‡é˜ˆå€¼ï¼Œä¿ç•™æ›´å¤šå¸§
                frame_scores.append((i, frame, quality_score))
        
        if not frame_scores:
            return [frames[0]] if frames else []
        
        # ğŸ¯ ç¬¬äºŒæ­¥ï¼šå¤šæ ·æ€§é€‰æ‹© - é¿å…é€‰æ‹©è¿‡äºç›¸ä¼¼çš„å¸§
        selected_frames = []
        frame_scores.sort(key=lambda x: x[2], reverse=True)  # æŒ‰è´¨é‡æ’åº
        
        for i, frame, score in frame_scores:
            if len(selected_frames) >= 8:  # ğŸš€ å¢åŠ åˆ°æœ€å¤š8å¸§
                break
                
            # æ£€æŸ¥ä¸å·²é€‰æ‹©å¸§çš„ç›¸ä¼¼åº¦
            is_similar = False
            for selected_frame in selected_frames:
                similarity = self._calculate_frame_similarity(frame, selected_frame)
                if similarity > 0.85:  # å¦‚æœå¤ªç›¸ä¼¼å°±è·³è¿‡
                    is_similar = True
                    break
            
            if not is_similar:
                selected_frames.append(frame)
        
        # ğŸ¯ ç¬¬ä¸‰æ­¥ï¼šç¡®ä¿æ—¶é—´åˆ†å¸ƒå‡åŒ€
        if len(selected_frames) < 3 and len(frame_scores) >= 3:
            # å¼ºåˆ¶é€‰æ‹©é¦–ã€ä¸­ã€å°¾ä¸‰å¸§ç¡®ä¿æ—¶é—´è¦†ç›–
            indices = [0, len(frame_scores)//2, len(frame_scores)-1]
            for idx in indices:
                frame = frame_scores[idx][1]
                if frame not in selected_frames:
                    selected_frames.append(frame)
        
        logger.info(f"ğŸ“Š Geminiå¸§è¿‡æ»¤ï¼š{len(frames)}å¸§ â†’ {len(selected_frames)}å¸§ï¼ˆå¤šæ ·æ€§é€‰æ‹©ï¼‰")
        return selected_frames[:8]  # æœ€ç»ˆé™åˆ¶ä¸º8å¸§
    
    def _calculate_frame_similarity(self, frame1, frame2) -> float:
        """è®¡ç®—ä¸¤å¸§ä¹‹é—´çš„ç›¸ä¼¼åº¦"""
        try:
            import cv2  # type: ignore
            
            # è½¬æ¢ä¸ºç°åº¦å›¾å¹¶è°ƒæ•´å¤§å°ä»¥åŠ é€Ÿè®¡ç®—
            gray1 = cv2.cvtColor(frame1, cv2.COLOR_BGR2GRAY)
            gray2 = cv2.cvtColor(frame2, cv2.COLOR_BGR2GRAY)
            
            # è°ƒæ•´åˆ°ç›¸åŒå°ºå¯¸
            h, w = 64, 64
            gray1 = cv2.resize(gray1, (w, h))
            gray2 = cv2.resize(gray2, (w, h))
            
            # è®¡ç®—ç›´æ–¹å›¾ç›¸å…³æ€§
            hist1 = cv2.calcHist([gray1], [0], None, [256], [0, 256])
            hist2 = cv2.calcHist([gray2], [0], None, [256], [0, 256])
            
            correlation = cv2.compareHist(hist1, hist2, cv2.HISTCMP_CORREL)
            return correlation
            
        except Exception as e:
            logger.warning(f"å¸§ç›¸ä¼¼åº¦è®¡ç®—å¤±è´¥: {e}")
            return 0.5
    
    def _calculate_gemini_frame_quality(self, frame) -> float:
        """è®¡ç®—é€‚åˆGeminiçš„å¸§è´¨é‡åˆ†æ•°"""
        try:
            import cv2  # type: ignore
            # è½¬æ¢ä¸ºç°åº¦å›¾
            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
            
            # æ¸…æ™°åº¦æ£€æµ‹ï¼ˆæ‹‰æ™®æ‹‰æ–¯ç®—å­ï¼‰
            laplacian_var = cv2.Laplacian(gray, cv2.CV_64F).var()
            sharpness = min(laplacian_var / 1500, 1.0)  # æ›´é«˜çš„æ¸…æ™°åº¦è¦æ±‚
            
            # ä¿¡æ¯å¯†åº¦æ£€æµ‹ï¼ˆè¾¹ç¼˜æ£€æµ‹ï¼‰
            edges = cv2.Canny(gray, 50, 150)
            edge_density = np.count_nonzero(edges) / edges.size
            
            # äº®åº¦å‡è¡¡
            brightness = gray.mean()
            brightness_score = 1.0 - abs(brightness - 128) / 128
            
            # Geminiåå¥½çš„ç»¼åˆè¯„åˆ†
            quality = (
                sharpness * 0.4 +           # æ¸…æ™°åº¦æƒé‡40%
                edge_density * 0.4 +        # ä¿¡æ¯å¯†åº¦æƒé‡40%
                brightness_score * 0.2      # äº®åº¦æƒé‡20%
            )
            
            return quality
            
        except Exception as e:
            logger.warning(f"Geminiå¸§è´¨é‡è®¡ç®—å¤±è´¥: {e}")
            return 0.5

    def _call_gemini_frame_fallback(self, video_path: str, prompt: str) -> Dict[str, Any]:
        """å›é€€æ–¹æ³•ï¼šä½¿ç”¨å¸§åˆ†æï¼ˆä¿æŒåŸæœ‰é€»è¾‘ä½œä¸ºå¤‡ç”¨ï¼‰"""
        try:
            logger.warning("ğŸ”„ Geminiè§†é¢‘APIå¤±è´¥ï¼Œå›é€€åˆ°å¸§åˆ†ææ–¹æ³•")
            
            # ä½¿ç”¨åŸæœ‰çš„å¸§æå–æ–¹æ³•
            frames = self._extract_frames_optimized(video_path)
            if not frames:
                return {"success": False, "error": "æ— æ³•æå–è§†é¢‘å¸§"}
            
            # è½¬æ¢å¸§ä¸ºPIL Imageæ ¼å¼
            pil_images = []
            for frame in frames:
                import cv2  # type: ignore
                from PIL import Image  # type: ignore
                frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                pil_image = Image.fromarray(frame_rgb)
                pil_images.append(pil_image)
            
            # ä½¿ç”¨æ—§ç‰ˆå›¾ç‰‡åˆ†æAPI
            return self._call_gemini_api_fallback(pil_images, prompt)
            
        except Exception as e:
            logger.error(f"Geminiå¸§åˆ†æå›é€€ä¹Ÿå¤±è´¥: {str(e)}")
            return {"success": False, "error": str(e)}

    def _call_gemini_api_fallback(self, pil_images: List, prompt: str) -> Dict[str, Any]:
        """å›é€€åˆ°æ—§ç‰ˆGemini APIï¼ˆå›¾ç‰‡åˆ†æï¼‰"""
        try:
            import google.generativeai as genai  # type: ignore
            
            # é…ç½®Gemini APIï¼ˆæ—§ç‰ˆæ–¹å¼ï¼‰
            genai.configure(api_key=self.api_key)
            model = genai.GenerativeModel("gemini-2.5-pro")
            
            # æ„å»ºæ¶ˆæ¯å†…å®¹
            content = [prompt]
            content.extend(pil_images)
            
            # ç”Ÿæˆé…ç½®
            generation_config = {
                "temperature": 0.1,
                "top_p": 0.8,
                "top_k": 40,
                "max_output_tokens": 2000,
            }
            
            # è°ƒç”¨æ—§ç‰ˆAPI
            response = model.generate_content(
                content, 
                generation_config=generation_config,
                safety_settings=[
                    {"category": "HARM_CATEGORY_HARASSMENT", "threshold": "BLOCK_NONE"},
                    {"category": "HARM_CATEGORY_HATE_SPEECH", "threshold": "BLOCK_NONE"},
                    {"category": "HARM_CATEGORY_SEXUALLY_EXPLICIT", "threshold": "BLOCK_NONE"},
                    {"category": "HARM_CATEGORY_DANGEROUS_CONTENT", "threshold": "BLOCK_NONE"},
                ]
            )
            
            if response.text:
                return {"success": True, "content": response.text}
            else:
                return {"success": False, "error": "Gemini APIè¿”å›ç©ºå†…å®¹"}
                
        except Exception as e:
            logger.error(f"Geminiå›é€€APIè°ƒç”¨å¼‚å¸¸: {str(e)}")
            return {"success": False, "error": str(e)}


    
    def _clean_deepseek_output(self, text: str) -> str:
        """æ¸…ç†DeepSeekè¾“å‡ºæ ¼å¼ï¼Œç§»é™¤æ–¹æ‹¬å·å’Œå¤šä½™ç¬¦å·"""
        if not text:
            return text
        
        lines = text.split('\n')
        cleaned_lines = []
        
        for line in lines:
            if ':' in line:
                key, value = line.split(':', 1)
                key = key.strip()
                value = value.strip()
                
                # ç§»é™¤æ–¹æ‹¬å·å’Œå¼•å·
                value = value.replace('[', '').replace(']', '')
                value = value.replace('"', '').replace("'", '')
                
                # æ¸…ç†é€—å·
                if value.startswith(','):
                    value = value[1:]
                if value.endswith(','):
                    value = value[:-1]
                
                # æ¸…ç†ç©ºæ ¼
                value = ' '.join(value.split())
                
                # ç‰¹æ®Šå¤„ç†
                if value.lower().strip() in ['æ— ', 'none', 'null', '']:
                    value = 'æ— '
                
                cleaned_lines.append(f"{key}: {value}")
            else:
                cleaned_lines.append(line)
        
        return '\n'.join(cleaned_lines)
    
    def _get_deepseek_api_key(self) -> str:
        """è·å–DeepSeek APIå¯†é’¥"""
        import os
        from pathlib import Path
        
        # 1. ä»ç¯å¢ƒå˜é‡è·å–
        api_key = os.getenv("DEEPSEEK_API_KEY")
        if api_key:
            return api_key
        
        # 2. ä»é…ç½®æ–‡ä»¶è·å–
        config_paths = [
            Path(__file__).parent.parent.parent / "feishu_pool" / ".env",
            Path(__file__).parent.parent / "config" / "env_config.txt"
        ]
        
        for config_path in config_paths:
            if config_path.exists():
                try:
                    with open(config_path, 'r', encoding='utf-8') as f:
                        for line in f:
                            if line.strip().startswith('DEEPSEEK_API_KEY='):
                                api_key = line.split('=', 1)[1].strip()
                                # ç§»é™¤å¼•å·
                                if api_key.startswith('"') and api_key.endswith('"'):
                                    api_key = api_key[1:-1]
                                elif api_key.startswith("'") and api_key.endswith("'"):
                                    api_key = api_key[1:-1]
                                return api_key
                except Exception:
                    continue
        
        return ""

    def _clean_gemini_raw_output(self, raw_output: str) -> str:
        """æ¸…ç†GeminiåŸå§‹è¾“å‡ºæ ¼å¼ï¼Œç»Ÿä¸€æ ¼å¼è§„èŒƒï¼Œä¸è¿›è¡Œç¿»è¯‘"""
        try:
            # åŸºç¡€æ¸…ç†ï¼šå»é™¤å¤šä½™ç©ºç™½å’Œæ¢è¡Œ
            cleaned = raw_output.strip()
            
            # å°è¯•æå–JSONéƒ¨åˆ†
            json_match = re.search(r'\{.*\}', cleaned, re.DOTALL)
            if json_match:
                cleaned = json_match.group(0)
            
            # ğŸ”§ ä½¿ç”¨ç»Ÿä¸€çš„å­—æ®µæ¸…ç†æ–¹æ³•
            def clean_field_format(value):
                if isinstance(value, str):
                    # ç§»é™¤å„ç§æ–¹æ‹¬å·å’Œå¼•å·
                    cleaned = value.replace('[', '').replace(']', '')
                    cleaned = cleaned.replace('"', '').replace("'", '')
                    if cleaned.startswith(','):
                        cleaned = cleaned[1:]
                    if cleaned.endswith(','):
                        cleaned = cleaned[:-1]
                    cleaned = ' '.join(cleaned.split())
                    if cleaned.lower().strip() in ['æ— ', 'none', 'null', '', 'æ²¡æœ‰', 'æœªçŸ¥']:
                        return 'æ— '
                    return cleaned.strip()
                return value
            
            # è§£æå’Œé‡æ–°æ ¼å¼åŒ–JSON
            try:
                import json
                data = json.loads(cleaned)
                
                # æ¸…ç†æ‰€æœ‰å­—æ®µæ ¼å¼
                for key, value in data.items():
                    if isinstance(value, str):
                        data[key] = clean_field_format(value)
                    elif isinstance(value, list):
                        data[key] = [clean_field_format(item) if isinstance(item, str) else item for item in value]
                
                # è¿”å›æ ¼å¼åŒ–çš„JSON
                return json.dumps(data, ensure_ascii=False, separators=(',', ': '))
                
            except json.JSONDecodeError:
                logger.warning("âš ï¸ JSONè§£æå¤±è´¥ï¼Œè¿”å›æ–‡æœ¬æ¸…ç†ç»“æœ")
                # å¦‚æœJSONè§£æå¤±è´¥ï¼Œè¿›è¡Œæ–‡æœ¬çº§åˆ«çš„æ ¼å¼æ¸…ç†
                cleaned = re.sub(r':\s*\["([^"]+)"\]', r': "\1"', cleaned)  # [content] -> content
                cleaned = re.sub(r':\s*\[([^\]]+)\]', r': "\1"', cleaned)   # å»é™¤æ–¹æ‹¬å·
                return cleaned
                
        except Exception as e:
            logger.warning(f"âš ï¸ æ ¼å¼æ¸…ç†å¤±è´¥: {e}ï¼Œè¿”å›åŸå§‹è¾“å‡º")
            return raw_output



class DeepSeekAnalyzer:
    """DeepSeekåˆ†æå™¨ - ç‹¬ç«‹å®ç°"""
    
    def __init__(self, api_key: str):
        self.api_key = api_key
        self.model_config = get_models_config()["deepseek"]
    
    def analyze_text(self, text: str, prompt: str) -> Dict[str, Any]:
        """åˆ†ææ–‡æœ¬å†…å®¹"""
        try:
            response = self._call_deepseek_api(text, prompt)
            
            if response.get("success"):
                return {
                    "success": True,
                    "analysis_result": response["content"],
                    "confidence": 0.9
                }
            else:
                return {"success": False, "error": response.get("error", "APIè°ƒç”¨å¤±è´¥")}
                
        except Exception as e:
            logger.error(f"DeepSeekæ–‡æœ¬åˆ†æå¤±è´¥: {str(e)}")
            return {"success": False, "error": str(e)}
    
    def _call_deepseek_api(self, text: str, prompt: str) -> Dict[str, Any]:
        """è°ƒç”¨DeepSeek API"""
        try:
            url = "https://api.deepseek.com/chat/completions"
            
            payload = {
                "model": self.model_config["model_name"],
                "messages": [
                    {"role": "system", "content": prompt},
                    {"role": "user", "content": text}
                ],
                "max_tokens": self.model_config["max_tokens"],
                "temperature": self.model_config["temperature"]
            }
            
            headers = {
                "Authorization": f"Bearer {self.api_key}",
                "Content-Type": "application/json"
            }
            
            response = requests.post(url, json=payload, headers=headers, timeout=30)
            
            if response.status_code == 200:
                result = response.json()
                if result.get("choices"):
                    content = result["choices"][0]["message"]["content"]
                    return {"success": True, "content": content}
                else:
                    return {"success": False, "error": "APIè¿”å›æ ¼å¼é”™è¯¯"}
            else:
                return {"success": False, "error": f"APIè°ƒç”¨å¤±è´¥: {response.status_code}"}
            
        except Exception as e:
            logger.error(f"DeepSeek APIè°ƒç”¨å¼‚å¸¸: {str(e)}")
            return {"success": False, "error": str(e)}
    
class DualStageAnalyzer:
    """
    åŒå±‚è¯†åˆ«æœºåˆ¶åˆ†æå™¨ - ç‹¬ç«‹å®ç°ç‰ˆæœ¬
    
    ç¬¬ä¸€å±‚ï¼ˆAI-Bï¼‰ï¼šé€šç”¨ç‰©ä½“/åœºæ™¯/æƒ…ç»ªè¯†åˆ« + ä¸»è°“å®¾åŠ¨ä½œè¯†åˆ«
    ç¬¬äºŒå±‚ï¼ˆAI-Aï¼‰ï¼šæ¡ä»¶è§¦å‘çš„å“ç‰Œä¸“ç”¨æ£€æµ‹
    """
    
    def __init__(self):
        """åˆå§‹åŒ–åŒå±‚åˆ†æå™¨"""
        self.api_keys = get_api_keys()
        self.models_config = get_models_config()
        self.core_brands = get_core_brands()
        self.trigger_keywords = get_brand_trigger_keywords()
        self.prompts = get_analysis_prompts()
        self.quality_control = get_quality_control()
        
        # åˆå§‹åŒ–åˆ†æå™¨
        self._initialize_analyzers()
        
    def _initialize_analyzers(self):
        """åˆå§‹åŒ–åˆ†æå™¨"""
        try:
            # æ£€æŸ¥APIå¯†é’¥
            if not self.api_keys.get("qwen"):
                logger.warning("Qwen APIå¯†é’¥æœªé…ç½®")
                self.qwen_analyzer = None
            else:
                self.qwen_analyzer = QwenVideoAnalyzer(self.api_keys["qwen"])
            
            # åˆå§‹åŒ–Geminiåˆ†æå™¨
            if not self.api_keys.get("gemini"):
                logger.warning("Gemini APIå¯†é’¥æœªé…ç½®")
                self.gemini_analyzer = None
            else:
                self.gemini_analyzer = GeminiVideoAnalyzer(self.api_keys["gemini"])
            
            if not self.api_keys.get("deepseek"):
                logger.warning("DeepSeek APIå¯†é’¥æœªé…ç½®")
                self.deepseek_analyzer = None
            else:
                self.deepseek_analyzer = DeepSeekAnalyzer(self.api_keys["deepseek"])
            
            logger.info("âœ… åˆ†æå™¨åˆå§‹åŒ–å®Œæˆ")
            
        except Exception as e:
            logger.error(f"åˆ†æå™¨åˆå§‹åŒ–å¤±è´¥: {e}")
            self.qwen_analyzer = None
            self.gemini_analyzer = None
            self.deepseek_analyzer = None
    
    def analyze_video_slice(self, video_path: str, analysis_type: str = "dual") -> Dict[str, Any]:
        """
        åˆ†æå•ä¸ªè§†é¢‘ç‰‡æ®µï¼ˆä¸¥æ ¼æŒ‰ç…§åŒå±‚è¯†åˆ«æœºåˆ¶ï¼‰
        
        Args:
            video_path: è§†é¢‘ç‰‡æ®µæ–‡ä»¶è·¯å¾„
            analysis_type: åˆ†æç±»å‹ ("dual", "enhanced")
            
        Returns:
            åˆ†æç»“æœå­—å…¸
        """
        try:
            logger.info(f"ğŸ¯ å¼€å§‹åŒå±‚è¯†åˆ«åˆ†æ: {video_path}")
            logger.info(f"ğŸ“Š åˆ†æç±»å‹: {analysis_type}")
            
            # è´¨é‡æ£€æŸ¥
            if not self._check_video_quality(video_path):
                return self._get_default_result("è§†é¢‘è´¨é‡æ£€æŸ¥å¤±è´¥")
            
            # æ£€æŸ¥åˆ†æå™¨å¯ç”¨æ€§
            if not self.qwen_analyzer:
                return self._get_default_result("Qwenåˆ†æå™¨æœªåˆå§‹åŒ–")
            
            # æ ¹æ®åˆ†æç±»å‹é€‰æ‹©å¤„ç†æ–¹å¼
            if analysis_type == "dual":
                # åŒå±‚è§†è§‰æœºåˆ¶ï¼ˆæ ¸å¿ƒæ¨èï¼‰
                return self._perform_dual_stage_visual_analysis(video_path)
            else:  # analysis_type == "enhanced"
                # åŒå±‚æœºåˆ¶ + éŸ³é¢‘å¢å¼º
                return self._perform_enhanced_dual_analysis(video_path)
                
        except Exception as e:
            logger.error(f"è§†é¢‘ç‰‡æ®µåˆ†æå¤±è´¥: {str(e)}")
            return self._get_default_result(f"åˆ†æå¼‚å¸¸: {str(e)}")
    
    def _perform_dual_stage_visual_analysis(self, video_path: str) -> Dict[str, Any]:
        """ğŸ†• æ‰§è¡ŒQwenå•çº§åˆ†æï¼ˆç¦ç”¨Geminiå›é€€ï¼‰"""
        try:
            logger.info("ğŸ¯ å¼€å§‹Qwenå•çº§åˆ†æï¼ˆå·²ç¦ç”¨Geminiå›é€€ï¼‰")
            logger.info("ğŸ“‹ åˆ†æé¡ºåº: 1ï¸âƒ£Qwen VL Max (å”¯ä¸€é€‰æ‹©)")
            
            # ğŸ”§ è·å–Qwenä¸“ç”¨prompt
            qwen_prompt = self.prompts.get("stage1_general_detection")
            
            if not qwen_prompt:
                logger.error("âŒ æœªèƒ½è·å–Qwenåˆ†ææç¤ºè¯æ¨¡æ¿")
                return self._get_default_result("æç¤ºè¯è·å–å¤±è´¥")
            
            logger.info("âœ… æˆåŠŸåŠ è½½Qwenä¸“ç”¨prompt")
            
            # ğŸ¥‡ å”¯ä¸€é€‰æ‹©ï¼šQwen VL Maxï¼ˆä½¿ç”¨é€šç”¨promptï¼‰
            if self.qwen_analyzer:
                logger.info("ğŸ¤– 1ï¸âƒ£ ä½¿ç”¨ Qwen VL Max åˆ†æï¼ˆé€šç”¨promptï¼‰...")
                logger.info(f"ğŸ“‹ Qwené€šç”¨prompté¢„è§ˆ: {qwen_prompt[:150]}...")
                
                qwen_result = self._try_analysis_with_language_detection(
                    self.qwen_analyzer, video_path, qwen_prompt, "qwen-vl-max"
                )
                
                if qwen_result.get("success"):
                    qwen_result["analysis_method"] = "qwen_only_mode"
                    logger.info("âœ… Qwen VL Maxåˆ†ææˆåŠŸï¼Œè¿”å›ç»“æœ")
                    return qwen_result
                else:
                    logger.error(f"âŒ Qwen VL Maxåˆ†æå¤±è´¥: {qwen_result.get('error', 'æœªçŸ¥é”™è¯¯')}")
                    logger.info("ğŸš« Geminiå›é€€å·²ç¦ç”¨ï¼Œç›´æ¥è¿”å›å¤±è´¥")
                    return self._get_default_result(f"Qwenåˆ†æå¤±è´¥: {qwen_result.get('error', 'æœªçŸ¥é”™è¯¯')}")
            else:
                logger.error("âŒ Qwenåˆ†æå™¨æœªåˆå§‹åŒ–")
                return self._get_default_result("Qwenåˆ†æå™¨æœªåˆå§‹åŒ–")
                
        except Exception as e:
            logger.error(f"âŒ Qwenå•çº§åˆ†æå¼‚å¸¸: {str(e)}")
            return self._get_default_result(f"åˆ†æå¼‚å¸¸: {str(e)}")

    def _try_analysis_with_language_detection(self, analyzer, video_path: str, prompt: str, model_name: str) -> Dict[str, Any]:
        """å¸¦è¯­è¨€æ£€æµ‹çš„åˆ†æå°è¯•ï¼ˆæ ¹æ®æ¨¡å‹è¾“å‡ºè¯­è¨€è¿›è¡Œç›¸åº”å¤„ç†ï¼‰"""
        try:
            # æ‰§è¡Œåˆ†æ
            result = analyzer.analyze_video_frames(video_path, prompt)
            
            if result.get("success"):
                # è§£æåˆ†æç»“æœ
                parsed_result = self._parse_result_with_language_detection(
                    result["analysis_result"], model_name
                )
                
                if parsed_result:
                    parsed_result["success"] = True
                    parsed_result["model_used"] = model_name
                    parsed_result["confidence"] = result.get("confidence", 0.85)
                    return parsed_result
                else:
                    return {"success": False, "error": "ç»“æœè§£æå¤±è´¥"}
            else:
                return {"success": False, "error": result.get("error", "åˆ†æå¤±è´¥")}
            
        except Exception as e:
            return {"success": False, "error": f"åˆ†æå¼‚å¸¸: {str(e)}"}
    
    def _parse_result_with_language_detection(self, analysis_text: str, model_name: str) -> Optional[Dict[str, Any]]:
        """æ ¹æ®æ¨¡å‹ç‰¹æ€§å’Œè¾“å‡ºè¯­è¨€è¿›è¡Œæ™ºèƒ½è§£æ"""
        try:
            # æ•°æ®é¢„å¤„ç†
            if isinstance(analysis_text, dict) and 'text' in analysis_text:
                analysis_text = analysis_text.get('text', '')  # type: ignore
            elif isinstance(analysis_text, list):
                analysis_text = analysis_text[0] if analysis_text else ""
            
            analysis_text = str(analysis_text)
            
            logger.info(f"ğŸ” è§£æ{model_name}åˆ†æç»“æœ: {analysis_text[:200]}...")
            
            # ğŸ¤– Geminiæ¨¡å‹ï¼šä¼˜å…ˆå¤„ç†JSONæ•°ç»„æ ¼å¼ï¼Œè¾“å‡ºé€šå¸¸æ˜¯è‹±æ–‡
            if "gemini" in model_name.lower():
                return self._parse_gemini_result(analysis_text)
            
            # ğŸ¤– Qwenæ¨¡å‹ï¼šä¼˜å…ˆå¤„ç†key:valueæ ¼å¼ï¼Œè¾“å‡ºé€šå¸¸æ˜¯ä¸­æ–‡
            elif "qwen" in model_name.lower():
                return self._parse_qwen_result(analysis_text)
            
            # ğŸ¤– å…¶ä»–æ¨¡å‹ï¼šé€šç”¨è§£æ
            else:
                return self._parse_generic_result(analysis_text)
                
        except Exception as e:
            logger.error(f"è§£æç»“æœå¤±è´¥: {str(e)}")
            return None
    
    def _perform_enhanced_dual_analysis(self, video_path: str) -> Dict[str, Any]:
        """æ‰§è¡Œå¢å¼ºåŒå±‚åˆ†æï¼šè§†è§‰åŒå±‚ + éŸ³é¢‘å¢å¼º"""
        # æš‚æ—¶åªè¿”å›è§†è§‰åˆ†æç»“æœ
        logger.info("ğŸ¯ å¢å¼ºåˆ†ææš‚æ—¶ä½¿ç”¨è§†è§‰åˆ†æ")
        return self._perform_dual_stage_visual_analysis(video_path)

    def _check_video_quality(self, video_path: str) -> bool:
        """æ£€æŸ¥è§†é¢‘è´¨é‡"""
        try:
            file_path = Path(video_path)
            
            # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            if not file_path.exists():
                logger.error(f"è§†é¢‘æ–‡ä»¶ä¸å­˜åœ¨: {video_path}")
                return False
            
            # æ£€æŸ¥æ–‡ä»¶å¤§å°
            file_size_mb = file_path.stat().st_size / (1024 * 1024)
            quality = self.quality_control
            
            if file_size_mb < quality["min_file_size_mb"]:
                logger.error(f"æ–‡ä»¶è¿‡å°: {file_size_mb:.2f}MB")
                return False
            
            if file_size_mb > quality["max_file_size_mb"]:
                logger.error(f"æ–‡ä»¶è¿‡å¤§: {file_size_mb:.2f}MB")
                return False
            
            # æ£€æŸ¥æ–‡ä»¶æ ¼å¼
            if file_path.suffix.lower() not in quality["supported_formats"]:
                logger.error(f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {file_path.suffix}")
                return False
            
            return True
                
        except Exception as e:
            logger.error(f"è§†é¢‘è´¨é‡æ£€æŸ¥å¤±è´¥: {str(e)}")
            return False

    def _get_default_result(self, reason: str) -> Dict[str, Any]:
        """è·å–é»˜è®¤ç»“æœ"""
        return {
            "success": False,
            "error": reason,
            "object": "analysis failed",
            "scene": "unknown scene",
            "emotion": "unknown emotion",
            "brand_elements": "none",
            "confidence": 0.0,
            "analysis_method": "failed"
        }

    def _parse_gemini_result(self, analysis_text: str) -> Optional[Dict[str, Any]]:
        """è§£æGeminiæ¨¡å‹ç»“æœ - ç®€åŒ–ç‰ˆï¼Œç›´æ¥è§£ækey:valueæ ¼å¼"""
        try:
            logger.info(f"ğŸ” è§£ægemini-2.5-proåˆ†æç»“æœ: {analysis_text[:200]}...")
            
            # ğŸ”§ ç®€åŒ–è§£æï¼šç›´æ¥ä½¿ç”¨key:valueæ ¼å¼è§£æï¼ˆä¸Qwenç›¸åŒï¼‰
            result = self._parse_key_value_format(analysis_text)
            
            if result and result.get('object'):
                # åŸºç¡€éªŒè¯ï¼šç¡®ä¿objectä¸æ˜¯é€šç”¨è¯æ±‡
                object_text = result.get('object', '').lower()
                if any(generic in object_text for generic in ['video', 'content', 'è§†é¢‘', 'å†…å®¹']):
                    logger.warning(f"âš ï¸ Geminiè¾“å‡ºåŒ…å«é€šç”¨è¯æ±‡: {result.get('object')}")
                    # ä¸è¿”å›Noneï¼Œè€Œæ˜¯ä¿ç•™ç»“æœä½†é™ä½ç½®ä¿¡åº¦
                    result['confidence'] = 0.6
                else:
                    result['confidence'] = 0.9
                
                logger.info("âœ… Geminiç®€åŒ–è§£ææˆåŠŸ")
                return result
            else:
                logger.warning("âš ï¸ Gemini key:valueè§£æå¤±è´¥ï¼Œå°è¯•æ™ºèƒ½æå–")
                # å¤‡ç”¨ï¼šæ™ºèƒ½æ–‡æœ¬æå–
                result = self._extract_meaningful_content_from_text(analysis_text)
                if result and result.get("success"):
                    logger.info("âœ… Geminiæ™ºèƒ½æ–‡æœ¬æå–å®Œæˆ")
                    return result
                else:
                    logger.error("âŒ Geminiæ‰€æœ‰è§£ææ–¹æ³•éƒ½å¤±è´¥")
                    return None
            
        except Exception as e:
            logger.error(f"Geminiç»“æœè§£æå¼‚å¸¸: {e}")
            return None
    
    def _parse_qwen_result(self, analysis_text: str) -> Optional[Dict[str, Any]]:
        """è§£æQwenæ¨¡å‹ç»“æœï¼ˆé€šå¸¸æ˜¯ä¸­æ–‡ï¼Œkey:valueæ ¼å¼ï¼‰"""
        try:
            result = {}
            
            # ğŸ”§ ä¼˜å…ˆå°è¯•key:valueæ ¼å¼ï¼ˆQwenç‰¹è‰²ï¼‰
            parsed_kv = self._parse_key_value_format(analysis_text)
            if parsed_kv:
                result.update(parsed_kv)
                logger.info(f"âœ… Qwen key:valueæ ¼å¼è§£ææˆåŠŸ")
                return self._finalize_result(result)
            
            # ğŸ”§ å¤‡ç”¨æ–¹æ¡ˆï¼šæ™ºèƒ½æ–‡æœ¬æå–ï¼ˆä¸­æ–‡å‹å¥½ï¼‰
            result = self._extract_meaningful_content_from_text(analysis_text)
            logger.info(f"âœ… Qwenæ™ºèƒ½æ–‡æœ¬æå–å®Œæˆ")
            return self._finalize_result(result)
                
        except Exception as e:
            logger.error(f"Qwenç»“æœè§£æå¤±è´¥: {str(e)}")
            return None

    def _parse_generic_result(self, analysis_text: str) -> Optional[Dict[str, Any]]:
        """é€šç”¨æ¨¡å‹ç»“æœè§£æ"""
        try:
            result = {}
            
            # ğŸ”§ å°è¯•å¤šç§è§£ææ–¹æ¡ˆ
            # 1. key:valueæ ¼å¼
            parsed_kv = self._parse_key_value_format(analysis_text)
            if parsed_kv:
                result.update(parsed_kv)
                return self._finalize_result(result)
            
            # 2. æ™ºèƒ½æ–‡æœ¬æå–
            result = self._extract_meaningful_content_from_text(analysis_text)
            return self._finalize_result(result)
                
        except Exception as e:
            logger.error(f"é€šç”¨ç»“æœè§£æå¤±è´¥: {str(e)}")
            return None

    def _extract_from_gemini_json_array(self, json_data: list) -> Dict[str, Any]:
        """ä»Geminiçš„JSONæ•°ç»„ä¸­æå–ä¿¡æ¯ï¼Œä¿æŒåŸå§‹è¯­è¨€"""
        try:
            result = {}
            
            # åˆå¹¶æ‰€æœ‰labelå†…å®¹
            all_labels = []
            for item in json_data:
                if isinstance(item, dict) and 'label' in item:
                    label_text = item['label'].strip()
                    if label_text and len(label_text) > 5:  # è¿‡æ»¤æ‰å¤ªçŸ­çš„æ ‡ç­¾
                        all_labels.append(label_text)
            
            if not all_labels:
                return {}
            
            # ğŸ”§ ä½¿ç”¨ç¬¬ä¸€ä¸ªæœ‰æ„ä¹‰çš„æ ‡ç­¾ä½œä¸ºä¸»è¦æè¿°
            main_description = all_labels[0]
            result['object'] = main_description
            
            # ğŸ”§ ä»æè¿°ä¸­æ™ºèƒ½æ¨æ–­åœºæ™¯å’Œæƒ…ç»ª
            result['scene'] = self._infer_scene_from_text(main_description)
            result['emotion'] = self._infer_emotion_from_text(main_description)
            
            logger.info(f"ğŸ¯ Gemini JSONæå–: object='{result['object'][:50]}...', scene='{result['scene']}', emotion='{result['emotion']}'")
            
            return result
            
        except Exception as e:
            logger.error(f"Gemini JSONæ•°ç»„æå–å¤±è´¥: {e}")
            return {}

    def _parse_key_value_format(self, text: str) -> Dict[str, Any]:
        """è§£ækey:valueæ ¼å¼çš„æ–‡æœ¬"""
        try:
            result = {}
            lines = text.strip().split('\n')
            
            for line in lines:
                line = line.strip()
                if ':' in line:
                    key, value = line.split(':', 1)
                    key = key.strip().lower()
                    value = value.strip()
                    
                    # ğŸ”§ ç»Ÿä¸€æ ¼å¼æ¸…ç†
                    value = self._clean_field_format(value)
                    
                    if key == 'interaction' and 'object' not in result:
                        result['object'] = value
                    elif key == 'scene' and 'scene' not in result:
                        result['scene'] = value
                    elif key == 'emotion' and 'emotion' not in result:
                        result['emotion'] = value
                    elif key == 'brand_elements' and 'brand_elements' not in result:
                        result['brand_elements'] = value
            
            # æ£€æŸ¥æ˜¯å¦æ‰¾åˆ°äº†è¶³å¤Ÿçš„å­—æ®µ
            if len(result) >= 3:
                return result
            else:
                return {}
                
        except Exception as e:
            logger.error(f"key:valueæ ¼å¼è§£æå¤±è´¥: {e}")
            return {}

    def _finalize_result(self, result: Dict[str, Any]) -> Dict[str, Any]:
        """å®Œå–„åˆ†æç»“æœï¼Œæ·»åŠ å¿…éœ€å­—æ®µå’Œè´¨é‡æ§åˆ¶"""
        try:
            # æ·»åŠ é»˜è®¤brand_elements
            if 'brand_elements' not in result:
                result['brand_elements'] = 'æ— '
            
            # ğŸ”§ å¯¹æ‰€æœ‰å­—æ®µè¿›è¡Œæ ¼å¼æ¸…ç†
            for key in ['object', 'scene', 'emotion', 'brand_elements']:
                if key in result:
                    result[key] = self._clean_field_format(result[key])
            
            # ğŸš¨ è´¨é‡æ§åˆ¶æ£€æµ‹
            if result.get('object'):
                is_invalid, invalid_reason = self._detect_invalid_slice(result['object'])
                result['quality_status'] = 'invalid' if is_invalid else 'valid'
                if is_invalid:
                    result['invalid_reason'] = invalid_reason
                    logger.warning(f"ğŸš¨ æ£€æµ‹åˆ°æ— æ•ˆåˆ‡ç‰‡: {invalid_reason}")
            else:
                result['quality_status'] = 'valid'
            
            return result
            
        except Exception as e:
            logger.error(f"ç»“æœå®Œå–„å¤±è´¥: {e}")
            return result

    def _infer_scene_from_text(self, text: str) -> str:
        """ä»æ–‡æœ¬æè¿°ä¸­æ¨æ–­åœºæ™¯ï¼ˆæ”¯æŒä¸­è‹±æ–‡ï¼‰"""
        try:
            text_lower = text.lower()
            
            # ä¸­æ–‡åœºæ™¯å…³é”®è¯
            chinese_scene_keywords = {
                'å¨æˆ¿': 'å®¶ä¸­å¨æˆ¿',
                'å®¢å…': 'å®¶ä¸­å®¢å…', 
                'é¤å…': 'å®¶ä¸­é¤å…',
                'å§å®¤': 'å®¶ä¸­å§å®¤',
                'åŒ»é™¢': 'åŒ»é™¢ç¯å¢ƒ',
                'è¯Šæ‰€': 'åŒ»ç–—åœºæ‰€',
                'æˆ·å¤–': 'æˆ·å¤–åœºæ™¯',
                'æ•™å®¤': 'å®¤å†…æ•™å®¤',
                'åŠå…¬': 'åŠå…¬ç¯å¢ƒ'
            }
            
            # è‹±æ–‡åœºæ™¯å…³é”®è¯
            english_scene_keywords = {
                'kitchen': 'å®¶ä¸­å¨æˆ¿',
                'living room': 'å®¶ä¸­å®¢å…',
                'bedroom': 'å®¶ä¸­å§å®¤', 
                'hospital': 'åŒ»é™¢ç¯å¢ƒ',
                'clinic': 'åŒ»ç–—åœºæ‰€',
                'outdoor': 'æˆ·å¤–åœºæ™¯',
                'classroom': 'å®¤å†…æ•™å®¤',
                'office': 'åŠå…¬ç¯å¢ƒ'
            }
            
            # ä¸­æ–‡å…³é”®è¯åŒ¹é…
            for keyword, scene in chinese_scene_keywords.items():
                if keyword in text:
                    return scene
            
            # è‹±æ–‡å…³é”®è¯åŒ¹é…
            for keyword, scene in english_scene_keywords.items():
                if keyword in text_lower:
                    return scene
            
            # æ ¹æ®å†…å®¹æ¨æ–­
            if any(word in text_lower for word in ['cooking', 'preparing', 'formula', 'å†²æ³¡', 'å‡†å¤‡']):
                return 'å®¶ä¸­å¨æˆ¿'
            elif any(word in text_lower for word in ['playing', 'dancing', 'ç©è€', 'æ¸¸æˆ']):
                return 'å®¶ä¸­å®¢å…'
            
            # é»˜è®¤å®¤å†…åœºæ™¯
            return 'å®¤å†…åœºæ™¯'
            
        except Exception as e:
            logger.warning(f"åœºæ™¯æ¨æ–­å¤±è´¥: {e}")
            return 'å®¤å†…åœºæ™¯'

    def _infer_emotion_from_text(self, text: str) -> str:
        """ä»æ–‡æœ¬æè¿°ä¸­æ¨æ–­æƒ…ç»ªï¼ˆæ”¯æŒä¸­è‹±æ–‡ï¼‰"""
        try:
            text_lower = text.lower()
            
            # ä¸­æ–‡æƒ…ç»ªå…³é”®è¯
            chinese_emotion_keywords = {
                'å“­': 'ä¸å®‰',
                'å“­é—¹': 'ä¸å®‰',
                'å¼€å¿ƒ': 'å¼€å¿ƒ',
                'é«˜å…´': 'å¼€å¿ƒ',
                'ç©': 'å¼€å¿ƒ',
                'ç¬‘': 'å¼€å¿ƒ',
                'ä¸“æ³¨': 'ä¸“æ³¨',
                'è®¤çœŸ': 'ä¸“æ³¨',
                'ç„¦è™‘': 'ç„¦è™‘',
                'æ‹…å¿ƒ': 'ç„¦è™‘'
            }
            
            # è‹±æ–‡æƒ…ç»ªå…³é”®è¯  
            english_emotion_keywords = {
                'crying': 'ä¸å®‰',
                'happy': 'å¼€å¿ƒ',
                'smiling': 'å¼€å¿ƒ',
                'playing': 'å¼€å¿ƒ',
                'focused': 'ä¸“æ³¨',
                'worried': 'ç„¦è™‘',
                'calm': 'å¹³é™'
            }
            
            # ä¸­æ–‡å…³é”®è¯åŒ¹é…
            for keyword, emotion in chinese_emotion_keywords.items():
                if keyword in text:
                    return emotion
            
            # è‹±æ–‡å…³é”®è¯åŒ¹é…
            for keyword, emotion in english_emotion_keywords.items():
                if keyword in text_lower:
                    return emotion
            
            # é»˜è®¤å¹³é™æƒ…ç»ª
            return 'å¹³é™'
            
        except Exception as e:
            logger.warning(f"æƒ…ç»ªæ¨æ–­å¤±è´¥: {e}")
            return 'å¹³é™'

    def _extract_concise_object_from_multi_frame_analysis(self, text: str) -> tuple[str, bool]:
        """ä»å¤šå›¾ç‰‡åˆ†ææ–‡æœ¬ä¸­æå–è¯¦ç»†ä¸”ç»“æ„åŒ–çš„å¤šåœºæ™¯æè¿°
        
        Returns:
            tuple: (è¯¦ç»†æè¿°æ–‡æœ¬, æ˜¯å¦ä¸ºå¤šåœºæ™¯)
        """
        try:
            # æ£€æµ‹æ˜¯å¦ä¸ºå¤šå›¾ç‰‡åˆ†ææ ¼å¼
            if "### ç¬¬ä¸€å¼ å›¾ç‰‡åˆ†æ" in text or "### å›¾ç‰‡ä¸€" in text or "### ç¬¬ä¸€ç»„" in text:
                logger.info("ğŸ” æ£€æµ‹åˆ°å¤šå›¾ç‰‡åˆ†ææ ¼å¼ï¼Œè¿›è¡Œå¤šåœºæ™¯æ ‡è®°å’Œç»“æ„åŒ–åˆ†æ")
                
                # æå–æ‰€æœ‰interactionå†…å®¹
                interactions = []
                scenes = []
                emotions = []
                import re
                
                # åŒ¹é…æ‰€æœ‰å­—æ®µ
                interaction_pattern = r'\*\*interaction\*\*:\s*([^*\n]+)'
                scene_pattern = r'\*\*scene\*\*:\s*([^*\n]+)'
                emotion_pattern = r'\*\*emotion\*\*:\s*([^*\n]+)'
                
                interaction_matches = re.findall(interaction_pattern, text)
                scene_matches = re.findall(scene_pattern, text)
                emotion_matches = re.findall(emotion_pattern, text)
                
                # æ¸…ç†å’Œæ”¶é›†æ•°æ®
                for match in interaction_matches:
                    interaction = match.strip()
                    if interaction and len(interaction) > 2:
                        interactions.append(interaction)
                
                for match in scene_matches:
                    scene = match.strip()
                    if scene and len(scene) > 2:
                        scenes.append(scene)
                
                for match in emotion_matches:
                    emotion = match.strip()
                    if emotion and len(emotion) > 1:
                        emotions.append(emotion)
                
                if interactions:
                    scene_count = len(interactions)
                    logger.info(f"ğŸ“Š å‘ç° {scene_count} ä¸ªåœºæ™¯ï¼š{len(set(interactions))} ä¸ªä¸åŒåŠ¨ä½œ")
                    
                    # ğŸ¯ ç­–ç•¥1: å•åœºæ™¯æƒ…å†µï¼ˆä¸æ ‡è®°ä¸ºå¤šåœºæ™¯ï¼‰
                    if scene_count == 1:
                        return interactions[0], False
                    
                    # ğŸ¯ ç­–ç•¥2: å¤šåœºæ™¯æƒ…å†µ - ç”Ÿæˆè¯¦ç»†ç»“æ„åŒ–æè¿°å¹¶æ ‡è®°
                    is_multi_scene = True
                    structured_description = self._create_structured_multi_scene_description(interactions, scenes, emotions)
                    
                    # ğŸ†• æ·»åŠ åœºæ™¯æ€»ç»“ä¿¡æ¯ï¼Œä¾¿äºåç»­åˆ†ç±»
                    scene_summary = self._generate_scene_summary_for_classification(interactions, scenes, emotions)
                    
                    # ğŸ¯ ç»„åˆè¯¦ç»†æè¿°å’Œåˆ†ç±»æ€»ç»“
                    full_description = f"{structured_description} | æ€»ç»“: {scene_summary}"
                    
                    logger.info(f"ğŸ“ å¤šåœºæ™¯ç»“æ„åŒ–æè¿°: {full_description}")
                    return full_description, is_multi_scene
                
                # å¦‚æœæ²¡æœ‰æ‰¾åˆ°interactionï¼Œå°è¯•æå–å…¶ä»–æœ‰ç”¨ä¿¡æ¯
                logger.warning("âš ï¸ å¤šå›¾ç‰‡åˆ†æä¸­æœªæ‰¾åˆ°interactionå­—æ®µ")
                
            # ğŸ¯ ç­–ç•¥3: éå¤šå›¾ç‰‡æ ¼å¼çš„å¤„ç†ï¼ˆä¸æ˜¯å¤šåœºæ™¯ï¼‰
            simple_desc = self._extract_simple_action_description(text)
            return simple_desc, False
            
        except Exception as e:
            logger.error(f"âŒ å¤šåœºæ™¯åˆ†æå¤±è´¥: {e}")
            # è¿”å›å‰50ä¸ªå­—ç¬¦ä½œä¸ºå¤‡é€‰ï¼Œä¸æ ‡è®°ä¸ºå¤šåœºæ™¯
            fallback = text[:50] + "..." if len(text) > 50 else text
            return fallback, False

    def _create_structured_multi_scene_description(self, interactions: list, scenes: list, emotions: list) -> str:
        """åˆ›å»ºç»“æ„åŒ–çš„å¤šåœºæ™¯æè¿°ï¼Œè¯¦ç»†è¦†ç›–æ‰€æœ‰åœºæ™¯ï¼Œä¾¿äºåç»­åˆ†ç±»"""
        try:
            scene_count = len(interactions)
            
            # ğŸ¯ æ–°é€»è¾‘ï¼šç”Ÿæˆè¯¦ç»†çš„ç»“æ„åŒ–æè¿°
            if scene_count <= 1:
                return interactions[0] if interactions else "å•åœºæ™¯å†…å®¹"
            
            # ğŸ“‹ åˆ†æåœºæ™¯æ¨¡å¼å’Œä¸»é¢˜
            scene_analysis = self._analyze_scene_patterns(interactions, scenes)
            
            # ğŸ” è¯†åˆ«ä¸»è¦å‚ä¸è€…å’Œè¡Œä¸ºç±»å‹
            main_subjects = scene_analysis.get('unique_subjects', [])
            main_actions = scene_analysis.get('unique_actions', [])
            main_subject = main_subjects[0] if main_subjects else 'ä¸»ä½“'
            
            # ğŸ“Š ç”Ÿæˆè¯¦ç»†çš„åœºæ™¯æè¿°
            detailed_scenes = []
            for i, interaction in enumerate(interactions[:5]):  # æœ€å¤šæ˜¾ç¤º5ä¸ªåœºæ™¯
                # æå–æ¯ä¸ªåœºæ™¯çš„å…³é”®ä¿¡æ¯
                scene_info = self._extract_scene_key_info(interaction, scenes[i] if i < len(scenes) else "")
                detailed_scenes.append(f"åœºæ™¯{i+1}: {scene_info}")
            
            # ğŸ¯ åˆ›å»ºç»“æ„åŒ–æè¿°
            if scene_count == 2:
                return f"åŒåœºæ™¯åºåˆ— - {detailed_scenes[0]}; {detailed_scenes[1]}"
            elif scene_count == 3:
                return f"ä¸‰åœºæ™¯åºåˆ— - {detailed_scenes[0]}; {detailed_scenes[1]}; {detailed_scenes[2]}"
            elif scene_count <= 5:
                scenes_desc = "; ".join(detailed_scenes)
                return f"å¤šåœºæ™¯åºåˆ—({scene_count}ä¸ª) - {scenes_desc}"
            else:
                # è¶…è¿‡5ä¸ªåœºæ™¯ï¼Œæ˜¾ç¤ºå‰3ä¸ªå’Œå2ä¸ª
                first_scenes = "; ".join(detailed_scenes[:3])
                last_scenes = "; ".join(detailed_scenes[-2:])
                return f"å¤æ‚å¤šåœºæ™¯({scene_count}ä¸ª) - {first_scenes}; ...; {last_scenes}"
                
        except Exception as e:
            logger.error(f"âŒ ç»“æ„åŒ–æè¿°åˆ›å»ºå¤±è´¥: {e}")
            # å›é€€åˆ°ç®€å•çš„åœºæ™¯åˆ—è¡¨
            return self._create_simple_scene_list(interactions)

    def _extract_scene_key_info(self, interaction: str, scene: str) -> str:
        """æå–å•ä¸ªåœºæ™¯çš„å…³é”®ä¿¡æ¯"""
        try:
            # ğŸ” æå–ä¸»è¦å…ƒç´ 
            subject = "ä¸»ä½“"
            action = "è¡Œä¸º"
            object_item = "å¯¹è±¡"
            
            # è¯†åˆ«ä¸»ä½“
            if 'å¦ˆå¦ˆ' in interaction or 'å¥³äºº' in interaction or 'æ¯äº²' in interaction:
                subject = "å¦ˆå¦ˆ"
            elif 'å®å®' in interaction or 'å©´å„¿' in interaction or 'å­©å­' in interaction:
                subject = "å®å®"
            elif 'åŒ»ç”Ÿ' in interaction or 'ä¸“å®¶' in interaction or 'æŠ¤å£«' in interaction:
                subject = "åŒ»ç”Ÿ"
            elif 'å·¥ä½œäººå‘˜' in interaction or 'æ•™ç»ƒ' in interaction:
                subject = "å·¥ä½œäººå‘˜"
            elif 'äº§å“' in interaction or 'å¥¶ç²‰' in interaction or 'å¥¶ç“¶' in interaction:
                subject = "äº§å“"
            
            # è¯†åˆ«åŠ¨ä½œ
            action_keywords = {
                'å–‚å…»': ['å–‚', 'å–å¥¶', 'é¥®ç”¨', 'åƒ'],
                'æŠ¤ç†': ['æŠ¤ç†', 'æŠšæ‘¸', 'è½»æ‹', 'ç…§é¡¾', 'æ¢å°¿å¸ƒ'],
                'äº’åŠ¨': ['é€—', 'äº’åŠ¨', 'äº¤è°ˆ', 'ç©è€', 'æ‹ç…§'],
                'å±•ç¤º': ['å±•ç¤º', 'æ˜¾ç¤º', 'æ‹¿ç€', 'æ‰‹æŒ', 'å‘ˆç°'],
                'å‡†å¤‡': ['å†²æ³¡', 'å‡†å¤‡', 'è°ƒåˆ¶', 'æ…æ‹Œ'],
                'æƒ…ç»ª': ['å“­é—¹', 'å“­æ³£', 'ç¬‘', 'å¼€å¿ƒ', 'ä¸å®‰'],
                'æ•™è‚²': ['æ•™å¯¼', 'æŒ‡å¯¼', 'è§£é‡Š', 'æ¼”ç¤º'],
                'æ£€æŸ¥': ['æ£€æŸ¥', 'è§‚å¯Ÿ', 'æŸ¥çœ‹', 'æµ‹è¯•']
            }
            
            for action_type, keywords in action_keywords.items():
                if any(keyword in interaction for keyword in keywords):
                    action = action_type
                    break
            
            # è¯†åˆ«å¯¹è±¡
            if 'å¥¶ç²‰' in interaction or 'é…æ–¹å¥¶' in interaction:
                object_item = "å¥¶ç²‰"
            elif 'å¥¶ç“¶' in interaction:
                object_item = "å¥¶ç“¶"
            elif 'äº§å“' in interaction or 'åŒ…è£…' in interaction:
                object_item = "äº§å“"
            elif 'è¥å…»' in interaction or 'æ ‡ç­¾' in interaction:
                object_item = "è¥å…»ä¿¡æ¯"
            
            # æ·»åŠ åœºæ™¯ä¿¡æ¯
            scene_info = ""
            if scene and scene.strip():
                scene_clean = scene.strip()
                if 'å¨æˆ¿' in scene_clean:
                    scene_info = "åœ¨å¨æˆ¿"
                elif 'å®¢å…' in scene_clean:
                    scene_info = "åœ¨å®¢å…"
                elif 'åŒ»é™¢' in scene_clean:
                    scene_info = "åœ¨åŒ»é™¢"
                elif 'æˆ·å¤–' in scene_clean:
                    scene_info = "åœ¨æˆ·å¤–"
                elif 'å§å®¤' in scene_clean:
                    scene_info = "åœ¨å§å®¤"
            
            # ç»„åˆæˆç®€æ´æè¿°
            return f"{subject}{action}{object_item}{scene_info}"
            
        except Exception as e:
            logger.error(f"âŒ åœºæ™¯å…³é”®ä¿¡æ¯æå–å¤±è´¥: {e}")
            return interaction[:15] + "..." if len(interaction) > 15 else interaction

    def _create_simple_scene_list(self, interactions: list) -> str:
        """åˆ›å»ºç®€å•çš„åœºæ™¯åˆ—è¡¨ï¼ˆå¤‡ç”¨æ–¹æ¡ˆï¼‰"""
        try:
            if not interactions:
                return "æ— åœºæ™¯ä¿¡æ¯"
            
            # é™åˆ¶æ¯ä¸ªåœºæ™¯æè¿°é•¿åº¦
            simplified_scenes = []
            for i, interaction in enumerate(interactions[:5]):
                short_desc = interaction[:12] + "..." if len(interaction) > 12 else interaction
                simplified_scenes.append(f"ç¬¬{i+1}æ®µ:{short_desc}")
            
            if len(interactions) > 5:
                return f"å¤šåœºæ™¯å†…å®¹({len(interactions)}ä¸ª) - {'; '.join(simplified_scenes)}ç­‰"
            else:
                return f"å¤šåœºæ™¯å†…å®¹({len(interactions)}ä¸ª) - {'; '.join(simplified_scenes)}"
                
        except Exception as e:
            logger.error(f"âŒ ç®€å•åœºæ™¯åˆ—è¡¨åˆ›å»ºå¤±è´¥: {e}")
            return f"å¤šåœºæ™¯å†…å®¹({len(interactions)}ä¸ªåœºæ™¯)"

    def _extract_action_keywords(self, interaction: str) -> str:
        """ä»interactionä¸­æå–å…³é”®åŠ¨ä½œè¯"""
        try:
            # åŠ¨ä½œå…³é”®è¯æ˜ å°„
            action_mapping = {
                'å±•ç¤º': ['å±•ç¤º', 'æ˜¾ç¤º', 'æ‹¿ç€', 'æ‰‹æŒ'],
                'é€—': ['é€—', 'ç©', 'äº’åŠ¨'],
                'å–‚å…»': ['å–å¥¶', 'å–‚', 'å–‚å…»'],
                'æŠ¤ç†': ['æŠšæ‘¸', 'è½»æ‹', 'æŠ¤ç†', 'ç…§é¡¾'],
                'è‡ªæ‹': ['è‡ªæ‹', 'æ‹ç…§'],
                'äº¤è°ˆ': ['äº¤è°ˆ', 'äº¤æµ', 'å¯¹è¯'],
                'æ¸¸æ³³': ['æ¸¸æ³³', 'æ°´ä¸­', 'æ”¯æ’‘'],
                'å‡†å¤‡': ['å‡†å¤‡', 'å†²æ³¡', 'è°ƒåˆ¶']
            }
            
            for action, keywords in action_mapping.items():
                if any(keyword in interaction for keyword in keywords):
                    return action
            
            # å¦‚æœæ²¡æ‰¾åˆ°æ˜ å°„ï¼Œæå–ä¸»è¦åŠ¨è¯
            import re
            # æå–ä¸­æ–‡åŠ¨è¯æ¨¡å¼
            verb_pattern = r'([\u4e00-\u9fa5]{1,2}(?:ç€|äº†|è¿‡|åœ¨|ç»™)*[\u4e00-\u9fa5]{0,2})'
            matches = re.findall(verb_pattern, interaction)
            if matches:
                return matches[0][:4]  # å–å‰4ä¸ªå­—ç¬¦
            
            # æœ€åå›é€€
            return interaction[:6] + "..." if len(interaction) > 6 else interaction
            
        except Exception as e:
            logger.error(f"âŒ åŠ¨ä½œå…³é”®è¯æå–å¤±è´¥: {e}")
            return interaction[:8]

    def _actions_are_similar(self, action1: str, action2: str) -> bool:
        """åˆ¤æ–­ä¸¤ä¸ªåŠ¨ä½œæ˜¯å¦ç›¸ä¼¼"""
        try:
            # æå–å…³é”®è¯è¿›è¡Œæ¯”è¾ƒ
            key1 = self._extract_action_keywords(action1)
            key2 = self._extract_action_keywords(action2)
            
            # å¦‚æœå…³é”®è¯ç›¸åŒï¼Œè®¤ä¸ºç›¸ä¼¼
            if key1 == key2:
                return True
            
            # æ£€æŸ¥æ˜¯å¦åŒ…å«ç›¸åŒçš„ä¸»ä½“å’ŒåŠ¨è¯
            similar_pairs = [
                ['å–å¥¶', 'å–‚å…»'], ['å±•ç¤º', 'æ˜¾ç¤º'], ['é€—', 'äº’åŠ¨'], 
                ['æŠšæ‘¸', 'è½»æ‹'], ['äº¤è°ˆ', 'äº¤æµ'], ['è‡ªæ‹', 'æ‹ç…§']
            ]
            
            for pair in similar_pairs:
                if (pair[0] in action1 and pair[1] in action2) or (pair[1] in action1 and pair[0] in action2):
                    return True
            
            return False
        
        except Exception as e:
            logger.error(f"âŒ åŠ¨ä½œç›¸ä¼¼æ€§åˆ¤æ–­å¤±è´¥: {e}")
            return False

    def _analyze_scene_patterns(self, interactions: list, scenes: list) -> dict:
        """åˆ†æåœºæ™¯æ¨¡å¼"""
        try:
            # ğŸ” ä¸»ä½“åˆ†æ
            subjects = []
            for interaction in interactions:
                if 'å¦ˆå¦ˆ' in interaction or 'å¥³äºº' in interaction or 'å¥³æ€§' in interaction:
                    subjects.append('å¦ˆå¦ˆ')
                elif 'å®å®' in interaction or 'å©´å„¿' in interaction:
                    subjects.append('å®å®')
                elif 'å·¥ä½œäººå‘˜' in interaction or 'æ•™ç»ƒ' in interaction:
                    subjects.append('å·¥ä½œäººå‘˜')
                elif 'å¥¶ç²‰' in interaction or 'äº§å“' in interaction or 'åŒ…è£…' in interaction:
                    subjects.append('äº§å“')
                else:
                    subjects.append('å…¶ä»–')
            
            # ğŸ” åŠ¨ä½œç±»å‹åˆ†æ
            action_types = []
            action_mapping = {
                'æŠ¤ç†': ['æŠ¤ç†', 'æŠšæ‘¸', 'è½»æ‹', 'æ¢å°¿å¸ƒ', 'ç…§é¡¾'],
                'äº’åŠ¨': ['é€—', 'äº’åŠ¨', 'äº¤è°ˆ', 'æ‹ç…§', 'è‡ªæ‹', 'ç©'],
                'å–‚å…»': ['å–å¥¶', 'å–‚å…»', 'å‡†å¤‡å¥¶', 'å†²æ³¡', 'å–'],
                'å±•ç¤º': ['å±•ç¤º', 'æ˜¾ç¤º', 'æ‹¿ç€', 'æ‰‹æŒ'],
                'è¿åŠ¨': ['æ¸¸æ³³', 'æ°´ä¸­', 'è¿åŠ¨', 'é”»ç‚¼']
            }
            
            for interaction in interactions:
                categorized = False
                for action_type, keywords in action_mapping.items():
                    if any(keyword in interaction for keyword in keywords):
                        action_types.append(action_type)
                        categorized = True
                        break
                if not categorized:
                    action_types.append('å…¶ä»–')
            
            # ğŸ” åœºæ™¯ç¯å¢ƒåˆ†æ
            environments = []
            if scenes:
                for scene in scenes:
                    if 'æ¸¸æ³³' in scene or 'æ°´ä¸­' in scene:
                        environments.append('æ¸¸æ³³é¦†')
                    elif 'å¨æˆ¿' in scene:
                        environments.append('å¨æˆ¿')
                    elif 'å©´å„¿æˆ¿' in scene or 'å§å®¤' in scene:
                        environments.append('å©´å„¿æˆ¿')
                    elif 'å®¢å…' in scene:
                        environments.append('å®¢å…')
                    else:
                        environments.append('å®¤å†…')
            
            # ğŸ¯ æ¨¡å¼åˆ¤æ–­
            unique_subjects = list(set(subjects))
            unique_actions = list(set(action_types))
            unique_environments = list(set(environments))
            
            is_progressive = len(unique_actions) > 1 and len(unique_subjects) >= 1
            is_parallel = len(unique_subjects) > 1 and len(set(action_types[:2])) == 1  # å‰ä¸¤ä¸ªåŠ¨ä½œç›¸åŒ
            is_repetitive = len(set(interactions)) < len(interactions) * 0.6  # 60%ç›¸ä¼¼åº¦é˜ˆå€¼
            
            return {
                'subjects': subjects,
                'unique_subjects': unique_subjects,
                'action_types': action_types,
                'unique_actions': unique_actions,
                'environments': environments,
                'unique_environments': unique_environments,
                'is_progressive': is_progressive,
                'is_parallel': is_parallel,
                'is_repetitive': is_repetitive,
                'total_scenes': len(interactions)
            }
            
        except Exception as e:
            logger.error(f"âŒ åœºæ™¯æ¨¡å¼åˆ†æå¤±è´¥: {e}")
            return {
                'subjects': [], 'unique_subjects': [], 'action_types': [], 'unique_actions': [],
                'environments': [], 'unique_environments': [],
                'is_progressive': False, 'is_parallel': False, 'is_repetitive': False,
                'total_scenes': len(interactions)
            }

    def _extract_simple_action_description(self, text: str) -> str:
        """æå–ç®€å•åŠ¨ä½œæè¿°ï¼ˆéå¤šå›¾ç‰‡æ ¼å¼ï¼‰"""
        try:
            # å¦‚æœåŒ…å«æ˜æ˜¾çš„è¡Œä¸ºæè¿°ï¼Œç›´æ¥æå–
            simple_patterns = [
                r'(å¦ˆå¦ˆ|å¥³äºº|å¥³æ€§|å®å®|å·¥ä½œäººå‘˜)[^ã€‚ï¼Œ]+?(?:å±•ç¤º|é€—|æŠšæ‘¸|è½»æ‹|æ¢|å–‚|å–|å‡†å¤‡|äº¤è°ˆ|è‡ªæ‹|æ‹ç…§)[^ã€‚ï¼Œ]*',
                r'(å±•ç¤º|æ˜¾ç¤º)[^ã€‚ï¼Œ]*?(äº§å“|å¥¶ç“¶|å¥¶ç²‰|è¥å…»æ ‡ç­¾)[^ã€‚ï¼Œ]*',
                r'(æ‰‹æŒ|æ‹¿ç€)[^ã€‚ï¼Œ]*?(å±•ç¤º|æ˜¾ç¤º)[^ã€‚ï¼Œ]*'
            ]
            
            for pattern in simple_patterns:
                match = re.search(pattern, text)
                if match:
                    result = match.group(0).strip()
                    if len(result) <= 25:  # ç¨å¾®æ”¾å®½é•¿åº¦é™åˆ¶
                        return result
            
            # å¦‚æœéƒ½æ²¡æ‰¾åˆ°ï¼Œè¿”å›å‰25ä¸ªå­—ç¬¦
            cleaned_text = re.sub(r'[#*]+', '', text).strip()
            if len(cleaned_text) > 25:
                return cleaned_text[:25] + "..."
            
            return cleaned_text
            
        except Exception as e:
            logger.error(f"âŒ ç®€å•åŠ¨ä½œæè¿°æå–å¤±è´¥: {e}")
            return text[:20] + "..." if len(text) > 20 else text

    def _extract_meaningful_content_from_text(self, text: str) -> Dict[str, Any]:
        """ä»æ–‡æœ¬ä¸­æå–æœ‰æ„ä¹‰çš„å†…å®¹ï¼Œç¡®ä¿å…·ä½“æ€§"""
        try:
            # ğŸ”§ åŸºç¡€æ¸…ç†å’Œé¢„å¤„ç†
            text = re.sub(r'[{}[\]"]', '', text).strip()
            
            # ğŸ†• ä¸“é—¨å¤„ç†å¤šå›¾ç‰‡åˆ†ææ ¼å¼ï¼Œæå–ç®€æ´çš„objectæè¿°
            concise_object, is_multi_scene = self._extract_concise_object_from_multi_frame_analysis(text)
            
            # ğŸ¯ å¦‚æœæˆåŠŸæå–åˆ°æè¿°ï¼Œä½¿ç”¨å®ƒä½œä¸ºä¸»è¦ç»“æœ
            if concise_object and len(concise_object) <= 100:  # ğŸ†• é€‚åº¦æ”¾å®½é•¿åº¦é™åˆ¶ä»¥å®¹çº³è¯¦ç»†å¤šåœºæ™¯æè¿°
                extraction_type = "å¤šåœºæ™¯ç»“æ„åŒ–" if is_multi_scene else "ç®€æ´"
                logger.info(f"âœ… æˆåŠŸæå–{extraction_type}æè¿°: {concise_object}")
                
                # ğŸ“Š æ„å»ºç»“æœ
                result = {
                    "object": self._clean_field_format(concise_object),
                    "scene": self._infer_scene_from_text(text),  # ä»å®Œæ•´æ–‡æœ¬æ¨æ–­åœºæ™¯
                    "emotion": self._infer_emotion_from_text(text),  # ä»å®Œæ•´æ–‡æœ¬æ¨æ–­æƒ…æ„Ÿ
                    "brand_elements": "æ— ",
                    "confidence": 0.92 if is_multi_scene else 0.88,  # ğŸ†• å¤šåœºæ™¯åˆ†ææœ‰æ›´é«˜ç½®ä¿¡åº¦
                    "success": True,
                    "is_multi_scene": is_multi_scene,  # ğŸ†• æ·»åŠ å¤šåœºæ™¯æ ‡è®°
                    "scene_count": text.count("### ") if is_multi_scene else 1  # ğŸ†• åœºæ™¯æ•°é‡
                }
                
                logger.info(f"ğŸ¯ {extraction_type}æå–ç»“æœ: object='{result['object']}', scene='{result['scene']}', emotion='{result['emotion']}', multi_scene={is_multi_scene}")
                return result
            
            # ğŸ”„ å¦‚æœç®€æ´æå–å¤±è´¥ï¼Œå›é€€åˆ°åŸæœ‰çš„å…³é”®å¥å­æå–é€»è¾‘
            logger.info("ğŸ”„ ç®€æ´æå–æœªæˆåŠŸï¼Œå›é€€åˆ°å…³é”®å¥å­æå–")
            sentences = self._extract_key_sentences_from_text(text)
            
            if not sentences:
                return self._get_default_result("æ— æ³•æå–å…³é”®å¥å­")
            
            # ğŸ¯ ä½¿ç”¨ç¬¬ä¸€ä¸ªæœ€å…·ä½“çš„å¥å­ä½œä¸ºä¸»æè¿°
            main_sentence = sentences[0]
            
            # ğŸ”§ ä¿®å¤ï¼šå¤§å¹…é™ä½å…·ä½“æ€§é˜ˆå€¼ï¼Œé¿å…è¯¯åˆ¤
            specificity_score = self._calculate_specificity_score(main_sentence)
            if specificity_score < 0.2:  # ğŸš€ é™ä½é˜ˆå€¼ä»0.6åˆ°0.2ï¼Œåªè¿‡æ»¤çœŸæ­£æ— æ„ä¹‰çš„å†…å®¹
                logger.warning(f"âš ï¸ å†…å®¹å¯èƒ½è¿‡äºé€šç”¨ (å…·ä½“æ€§å¾—åˆ†: {specificity_score:.2f}): {main_sentence}")
                # å°è¯•ä½¿ç”¨ä¸‹ä¸€ä¸ªæ›´å…·ä½“çš„å¥å­
                for sentence in sentences[1:]:
                    alt_score = self._calculate_specificity_score(sentence)
                    if alt_score >= 0.2:  # ğŸš€ é™ä½å¤‡é€‰é˜ˆå€¼
                        main_sentence = sentence
                        specificity_score = alt_score
                        logger.info(f"âœ… ä½¿ç”¨æ›´å…·ä½“çš„æè¿° (å¾—åˆ†: {alt_score:.2f}): {sentence}")
                        break
                else:
                    # ğŸ”§ å…³é”®ä¿®å¤ï¼šå³ä½¿æ‰€æœ‰å¥å­å¾—åˆ†éƒ½ä½ï¼Œä¹Ÿä¸è¿”å›å¤±è´¥ï¼Œè€Œæ˜¯ä½¿ç”¨åŸå¥å­
                    logger.warning(f"âš ï¸ æ‰€æœ‰æè¿°å¾—åˆ†åä½ï¼Œä½†ä»ä¿ç•™åˆ†æç»“æœ: {main_sentence}")
                    specificity_score = 0.5  # ç»™äºˆé»˜è®¤ç½®ä¿¡åº¦
            
            # ğŸ“Š æ„å»ºç»“æœ
            result = {
                "object": self._clean_field_format(main_sentence),
                "scene": self._infer_scene_from_text(main_sentence),
                "emotion": self._infer_emotion_from_text(main_sentence),
                "brand_elements": "æ— ",
                "confidence": max(0.7, specificity_score),  # åŸºäºå…·ä½“æ€§è°ƒæ•´ç½®ä¿¡åº¦
                "success": True
            }
            
            logger.info(f"ğŸ¯ æ™ºèƒ½æå– (å…·ä½“æ€§: {specificity_score:.2f}): object='{result['object'][:50]}...', scene='{result['scene']}', emotion='{result['emotion']}'")
            
            return result
            
        except Exception as e:
            logger.error(f"æ™ºèƒ½å†…å®¹æå–å¤±è´¥: {e}")
            return self._get_default_result("å†…å®¹æå–å¼‚å¸¸")

    def _calculate_specificity_score(self, text: str) -> float:
        """è®¡ç®—æ–‡æœ¬çš„å…·ä½“æ€§å¾—åˆ† - ä¼˜åŒ–ç‰ˆï¼Œæ›´å®½æ¾åˆç†"""
        try:
            text_lower = text.lower()
            score = 0.3  # ğŸš€ åŸºç¡€åˆ†æ•°ä»0æ”¹ä¸º0.3ï¼Œæ›´å®½æ¾
            
            # ğŸ¯ å…·ä½“ç‰©ä½“è¯æ±‡ (+0.2ï¼Œé™ä½æƒé‡)
            specific_objects = [
                'baby', 'child', 'girl', 'boy', 'mother', 'father', 'toddler',
                'å®å®', 'å­©å­', 'å¥³å­©', 'ç”·å­©', 'å¦ˆå¦ˆ', 'çˆ¸çˆ¸', 'å¹¼å„¿', 'å¥³äºº', 'äºº',
                'bottle', 'milk', 'formula', 'toy', 'book', 'food', 'package', 'product',
                'å¥¶ç“¶', 'ç‰›å¥¶', 'å¥¶ç²‰', 'ç©å…·', 'ä¹¦', 'é£Ÿç‰©', 'é¤å…·', 'åŒ…è£…', 'äº§å“', 'ç½'
            ]
            for obj in specific_objects:
                if obj in text_lower:
                    score += 0.2
                    break
            
            # ğŸ¯ å…·ä½“åŠ¨ä½œè¯æ±‡ (+0.3ï¼Œä¿æŒæƒé‡)
            specific_actions = [
                'drinking', 'eating', 'playing', 'crying', 'smiling', 'walking', 'sitting',
                'holding', 'showing', 'displaying', 'preparing',
                'å–', 'åƒ', 'ç©', 'å“­', 'ç¬‘', 'èµ°', 'å', 'æ‹¿', 'æ”¾', 'çœ‹', 'å¬', 'å±•ç¤º', 
                'æ˜¾ç¤º', 'æ‘†æ”¾', 'çªå‡º', 'å†²æ³¡', 'å‡†å¤‡', 'é€’ç»™', 'æŠ±ç€'
            ]
            for action in specific_actions:
                if action in text_lower:
                    score += 0.3
                    break
            
            # ğŸ¯ å…·ä½“åœºæ™¯è¯æ±‡ (+0.1ï¼Œé™ä½æƒé‡)
            specific_places = [
                'table', 'chair', 'bed', 'sofa', 'kitchen', 'living room', 'hospital',
                'æ¡Œå­', 'æ¤…å­', 'åºŠ', 'æ²™å‘', 'å¨æˆ¿', 'å®¢å…', 'åŒ»é™¢', 'é¤æ¡Œ', 'å®¤å†…', 'æ¡Œé¢'
            ]
            for place in specific_places:
                if place in text_lower:
                    score += 0.1
                    break
            
            # ğŸ”§ ä¿®å¤ï¼šå¤§å¹…å‡å°‘é€šç”¨è¯æ±‡æƒ©ç½š (-0.2ï¼Œä»-0.5å‡å°‘)
            very_generic_terms = [
                'video content analysis', 'general content', 'unknown content',
                'è§†é¢‘å†…å®¹åˆ†æ', 'é€šç”¨å†…å®¹', 'æœªçŸ¥å†…å®¹'  # åªæƒ©ç½šçœŸæ­£é€šç”¨çš„è¯æ±‡
            ]
            for generic in very_generic_terms:
                if generic in text_lower:
                    score -= 0.2  # ğŸš€ å‡å°‘æƒ©ç½šåŠ›åº¦
                    break
            
            # ğŸ”§ ç§»é™¤è¿‡çŸ­æè¿°æƒ©ç½šï¼Œå› ä¸ºç®€æ´ä¹Ÿå¯èƒ½å¾ˆæœ‰æ•ˆ
            
            # ğŸ¯ æ–°å¢ï¼šåŒ…å«å“ç‰Œæˆ–äº§å“ä¿¡æ¯çš„åŠ åˆ†
            product_terms = [
                'è¥å…»æ ‡ç­¾', 'å“ç‰Œæ ‡è¯†', 'å¥¶ç²‰ç½', 'åŒ…è£…', 'æ ‡ç­¾', 'æˆåˆ†', 'logo', 'brand'
            ]
            for term in product_terms:
                if term in text_lower:
                    score += 0.2
                    break
            
            return max(0.1, min(1.0, score))  # ğŸš€ æœ€ä½åˆ†ä»0æ”¹ä¸º0.1ï¼Œé¿å…å®Œå…¨ä¸º0
            
        except Exception as e:
            logger.warning(f"å…·ä½“æ€§è¯„åˆ†å¤±è´¥: {e}")
            return 0.5  # é»˜è®¤ä¸­ç­‰å¾—åˆ†

    def _extract_key_sentences_from_text(self, text: str, scenario: str = "æ¯å©´äº§å“") -> list:
        """ä»æ–‡æœ¬ä¸­æå–å…³é”®å¥å­ - ä½¿ç”¨é…ç½®åŒ–çš„å…³é”®è¯ç®¡ç†"""
        try:
            import re
            import sys
            from pathlib import Path
            sys.path.append(str(Path(__file__).parent.parent))
            from config.keyword_extraction_config import get_keyword_config
            
            # è·å–å…³é”®è¯é…ç½®
            keyword_config = get_keyword_config()
            extraction_settings = keyword_config.get_extraction_settings()
            keywords = keyword_config.get_keywords_for_extraction(scenario)
            regex_patterns = keyword_config.get_regex_patterns()
            
            # åˆ†å‰²å¥å­
            sentences = re.split(r'[.!?ã€‚ï¼ï¼Ÿ]+', text)
            key_sentences = []
            
            logger.info(f"ğŸ” ä½¿ç”¨ {len(keywords)} ä¸ªå…³é”®è¯å’Œ {len(regex_patterns)} ä¸ªæ­£åˆ™æ¨¡å¼è¿›è¡Œæå–")
            
            for sentence in sentences:
                sentence = sentence.strip()
                
                # åŸºç¡€è¿‡æ»¤
                min_length = extraction_settings.get("min_sentence_length", 8)
                if len(sentence) < min_length:
                    continue
                
                sentence_score = 0
                match_details = []
                
                # ğŸ¯ æ–¹æ³•1: å…³é”®è¯åŒ¹é…ï¼ˆåŸºç¡€æ–¹æ³•ï¼‰
                sentence_lower = sentence.lower() if not extraction_settings.get("case_sensitive", False) else sentence
                for keyword in keywords:
                    keyword_check = keyword.lower() if not extraction_settings.get("case_sensitive", False) else keyword
                    if keyword_check in sentence_lower:
                        sentence_score += 1
                        match_details.append(f"å…³é”®è¯:{keyword}")
                
                # ğŸ¯ æ–¹æ³•2: æ­£åˆ™è¡¨è¾¾å¼æ¨¡å¼åŒ¹é…ï¼ˆé«˜çº§æ–¹æ³•ï¼‰
                for pattern_config in regex_patterns:
                    pattern = pattern_config["pattern"]
                    weight = pattern_config["weight"]
                    
                    if pattern.search(sentence):
                        sentence_score += weight
                        match_details.append(f"æ¨¡å¼:{pattern_config['name']}")
                
                # ğŸ¯ æ–¹æ³•3: è¯­ä¹‰ç»“æ„åˆ†æï¼ˆæ™ºèƒ½æ–¹æ³•ï¼‰
                if self._has_subject_verb_object_structure(sentence):
                    sentence_score += 1.5
                    match_details.append("ä¸»è°“å®¾ç»“æ„")
                
                # å†³å®šæ˜¯å¦ä¿ç•™å¥å­
                if sentence_score >= 1.0:  # è‡³å°‘åŒ¹é…ä¸€ä¸ªæ¡ä»¶
                    key_sentences.append({
                        "sentence": sentence,
                        "score": sentence_score,
                        "matches": match_details
                    })
                    logger.debug(f"âœ… ä¿ç•™å¥å­ (å¾—åˆ†:{sentence_score:.1f}): {sentence[:50]}...")
            
            # æŒ‰å¾—åˆ†æ’åºå¹¶è¿”å›æœ€ä½³å¥å­
            key_sentences.sort(key=lambda x: x["score"], reverse=True)
            max_sentences = extraction_settings.get("max_sentences", 3)
            
            best_sentences = [item["sentence"] for item in key_sentences[:max_sentences]]
            
            if best_sentences:
                logger.info(f"ğŸ¯ æå–åˆ° {len(best_sentences)} ä¸ªå…³é”®å¥å­ï¼Œæœ€é«˜å¾—åˆ†: {key_sentences[0]['score']:.1f}")
            else:
                logger.warning("âš ï¸ æœªæå–åˆ°ä»»ä½•å…³é”®å¥å­ï¼Œå¯èƒ½éœ€è¦è°ƒæ•´é…ç½®")
            
            return best_sentences
            
        except Exception as e:
            logger.error(f"âŒ é…ç½®åŒ–å…³é”®å¥å­æå–å¤±è´¥: {e}")
            # å›é€€åˆ°ç®€å•æ–¹æ³•
            return self._extract_key_sentences_fallback(text)
    
    def _has_subject_verb_object_structure(self, sentence: str) -> bool:
        """æ£€æµ‹å¥å­æ˜¯å¦å…·æœ‰ä¸»è°“å®¾ç»“æ„"""
        try:
            # ç®€å•çš„ä¸­æ–‡ä¸»è°“å®¾ç»“æ„æ£€æµ‹
            chinese_verbs = ["å±•ç¤º", "æ‹¿ç€", "å–", "åƒ", "ç©", "å", "çœ‹", "æŠ±", "å–‚", "å“­", "ç¬‘", "åˆ¶ä½œ", "å‡†å¤‡"]
            chinese_subjects = ["å®å®", "å¦ˆå¦ˆ", "çˆ¸çˆ¸", "å­©å­", "å©´å„¿", "äº§å“", "å¥¶ç²‰ç½", "åŒ…è£…"]
            chinese_objects = ["å¥¶ç“¶", "å¥¶ç²‰", "ç©å…·", "é£Ÿå“", "æ ‡ç­¾", "è¥å…»æˆåˆ†"]
            
            has_subject = any(subj in sentence for subj in chinese_subjects)
            has_verb = any(verb in sentence for verb in chinese_verbs)
            has_object = any(obj in sentence for obj in chinese_objects)
            
            return has_subject and has_verb
            
        except Exception:
            return False
    
    def _extract_key_sentences_fallback(self, text: str) -> list:
        """å›é€€çš„ç®€å•å…³é”®å¥å­æå–æ–¹æ³• - ä¹Ÿä½¿ç”¨é…ç½®åŒ–å…³é”®è¯"""
        try:
            import re
            sentences = re.split(r'[.!?ã€‚ï¼ï¼Ÿ]+', text)
            
            # ğŸ”§ ç§»é™¤ç¡¬ç¼–ç ï¼šä½¿ç”¨é…ç½®åŒ–çš„åŸºç¡€å…³é”®è¯
            try:
                import sys
                from pathlib import Path
                sys.path.append(str(Path(__file__).parent.parent))
                from config.keyword_extraction_config import get_keyword_config
                
                # ä½¿ç”¨é«˜æƒé‡ç±»åˆ«çš„å…³é”®è¯ä½œä¸ºåŸºç¡€å›é€€
                keyword_config = get_keyword_config()
                all_keywords = []
                for category, config in keyword_config.keywords_config["keyword_categories"].items():
                    if config.get("weight", 1.0) >= 1.0:  # åªä½¿ç”¨æƒé‡>=1.0çš„ç±»åˆ«
                        for keywords in config["keywords"].values():
                            all_keywords.extend(keywords[:5])  # æ¯ç±»æœ€å¤š5ä¸ªè¯
                
                basic_keywords = list(set(all_keywords))  # å»é‡
                logger.info(f"ğŸ”„ å›é€€æ–¹æ³•ä½¿ç”¨ {len(basic_keywords)} ä¸ªé…ç½®åŒ–å…³é”®è¯")
                
            except Exception as config_error:
                logger.warning(f"âš ï¸ é…ç½®åŠ è½½å¤±è´¥ï¼Œä½¿ç”¨æœ€å°ç¡¬ç¼–ç é›†: {config_error}")
                # ğŸš¨ æœ€åçš„ç¡¬ç¼–ç ä¿éšœï¼šåªä¿ç•™æœ€æ ¸å¿ƒçš„è¯æ±‡
                basic_keywords = ["å®å®", "å¦ˆå¦ˆ", "baby", "mother"]
            
            key_sentences = []
            for sentence in sentences:
                sentence = sentence.strip()
                if (len(sentence) >= 8 and 
                    any(word in sentence.lower() for word in basic_keywords)):
                    key_sentences.append(sentence)
            
            return key_sentences[:3]
            
        except Exception as e:
            logger.warning(f"å›é€€æå–æ–¹æ³•å¤±è´¥: {e}")
            return []

    def _clean_field_format(self, text: str) -> str:
        """ç»Ÿä¸€æ¸…ç†å­—æ®µæ ¼å¼ï¼Œç§»é™¤æ–¹æ‹¬å·å’Œå¤šä½™ç¬¦å·"""
        if not text:
            return text
        
        try:
            # ç§»é™¤å„ç§æ–¹æ‹¬å·å’Œå¼•å·
            cleaned = text
            
            # ç§»é™¤æ–¹æ‹¬å· [ ]
            cleaned = cleaned.replace('[', '').replace(']', '')
            
            # ç§»é™¤å¤šä½™çš„å¼•å·
            cleaned = cleaned.replace('"', '').replace("'", '')
            
            # ç§»é™¤å¤šä½™çš„é€—å·åˆ†éš”ï¼ˆä½†ä¿ç•™å†…å®¹ä¸­çš„é€—å·ï¼‰
            if cleaned.startswith(','):
                cleaned = cleaned[1:]
            if cleaned.endswith(','):
                cleaned = cleaned[:-1]
            
            # æ¸…ç†å¤šä½™çš„ç©ºæ ¼
            cleaned = ' '.join(cleaned.split())
            
            # ğŸ”§ ç»Ÿä¸€å¤„ç†ç©ºå€¼
            if cleaned.lower().strip() in ['æ— ', 'none', 'null', '', 'æ²¡æœ‰', 'æœªçŸ¥']:
                return 'æ— '
            
            return cleaned.strip()
            
        except Exception as e:
            logger.warning(f"å­—æ®µæ ¼å¼æ¸…ç†å¤±è´¥: {e}")
            return text

    def _detect_invalid_slice(self, interaction_text: str) -> tuple[bool, str]:
        """æ£€æµ‹æ— æ•ˆåˆ‡ç‰‡ - ä»…æ£€æµ‹å½»åº•æ— å†…å®¹çš„æƒ…å†µï¼Œä¸å†åŸºäºäººç‰©å­˜åœ¨åˆ¤æ–­"""
        try:
            if not interaction_text:
                return True, "empty content"
                
            text_lower = interaction_text.lower()
            
            # ğŸ¯ æ–°ç­–ç•¥ï¼šä»»ä½•åˆ‡ç‰‡éƒ½æœ‰åˆ†æä»·å€¼ï¼Œä¸å†å› ä¸ºæ— äººç‰©è€Œæ ‡è®°ä¸ºæ— æ•ˆ
            # æ— äººç‰©çš„åˆ‡ç‰‡ä¹Ÿå¯èƒ½åŒ…å«äº§å“ä¿¡æ¯ã€å“ç‰Œå…ƒç´ ã€ç¯å¢ƒä¿¡æ¯ç­‰æœ‰ä»·å€¼å†…å®¹
            
            # 1. æ£€æµ‹åˆ†æå½»åº•å¤±è´¥çš„æƒ…å†µ - ğŸ”§ æ›´ä¸¥æ ¼çš„æ£€æµ‹ï¼Œé¿å…è¯¯åˆ¤
            true_failure_phrases = [
                "analysis completely failed", "å¤„ç†å½»åº•å¤±è´¥", "æ— æ³•è§£æè§†é¢‘", "è§†é¢‘æŸå",
                "fatal error", "ä¸¥é‡é”™è¯¯", "complete failure", "total failure"
            ]
            # ğŸš€ ç§»é™¤"analysis failed"çš„æ£€æµ‹ï¼Œå› ä¸ºè¿™å¯èƒ½æ˜¯æ­£å¸¸çš„åˆ†æç»“æœæè¿°
            for phrase in true_failure_phrases:
                if phrase in text_lower:
                    return True, "analysis failed completely"
            
            # 2. æ£€æµ‹å®Œå…¨ç©ºç™½æˆ–æ— æ„ä¹‰çš„å†…å®¹
            very_short = len(interaction_text.strip()) < 3
            completely_empty = interaction_text.strip() in ["", "æ— ", "none", "null", "N/A", "æœªçŸ¥", "unknown"]
            
            if very_short or completely_empty:
                return True, "empty or meaningless content"
            
            # 3. æ£€æµ‹è§†é¢‘æ–‡ä»¶æŸåæˆ–æ— æ³•è§£æçš„æƒ…å†µ
            corrupted_indicators = [
                "corrupted", "æŸå", "æ— æ³•æ’­æ”¾", "æ–‡ä»¶é”™è¯¯", "format error",
                "cannot decode", "è§£ç å¤±è´¥", "è§†é¢‘æŸå"
            ]
            if any(phrase in text_lower for phrase in corrupted_indicators):
                return True, "corrupted or unreadable video file"
            
            # ğŸ”§ é‡è¦æ”¹å˜ï¼šç§»é™¤æ‰€æœ‰åŸºäºäººç‰©å­˜åœ¨çš„æ— æ•ˆåˆ¤æ–­
            # åŒ…å«ä»¥ä¸‹å†…å®¹çš„åˆ‡ç‰‡ç°åœ¨éƒ½è¢«è®¤ä¸ºæ˜¯æœ‰æ•ˆçš„ï¼š
            # - çº¯äº§å“å±•ç¤ºï¼ˆæ— äººç‰©ä½†æœ‰äº§å“ä¿¡æ¯ï¼‰
            # - å“ç‰Œæ ‡è¯†ç‰¹å†™ï¼ˆæ— äººç‰©ä½†æœ‰å“ç‰Œä»·å€¼ï¼‰
            # - ç¯å¢ƒåœºæ™¯ï¼ˆæ— äººç‰©ä½†æœ‰åœºæ™¯ä¿¡æ¯ï¼‰
            # - æ–‡å­—è¯´æ˜ï¼ˆæ— äººç‰©ä½†æœ‰æ–‡å­—å†…å®¹ï¼‰
            
            # ğŸ”§ æ›´å®½æ¾çš„æœ‰æ•ˆæ€§åˆ¤æ–­ï¼šåªè¦æœ‰ä»»ä½•å¯è¯†åˆ«å†…å®¹å°±è®¤ä¸ºæœ‰æ•ˆ
            return False, ""
            
        except Exception as e:
            logger.warning(f"âš ï¸ æ— æ•ˆåˆ‡ç‰‡æ£€æµ‹å¼‚å¸¸: {e}")
            return False, ""

    def _generate_scene_summary_for_classification(self, interactions: list, scenes: list, emotions: list) -> str:
        """ç”Ÿæˆåœºæ™¯æ€»ç»“ä¿¡æ¯ï¼Œä¸“é—¨ç”¨äºåç»­çš„1çº§2çº§æ ‡ç­¾åˆ†ç±»"""
        try:
            # ğŸ” åˆ†æä¸»è¦å…ƒç´ 
            summary_parts = []
            
            # 1. ä¸»ä½“åˆ†æ
            subjects = set()
            for interaction in interactions:
                if 'å¦ˆå¦ˆ' in interaction or 'å¥³äºº' in interaction or 'æ¯äº²' in interaction:
                    subjects.add('å¦ˆå¦ˆ')
                if 'å®å®' in interaction or 'å©´å„¿' in interaction or 'å­©å­' in interaction:
                    subjects.add('å®å®')
                if 'åŒ»ç”Ÿ' in interaction or 'ä¸“å®¶' in interaction or 'æŠ¤å£«' in interaction:
                    subjects.add('åŒ»ç”Ÿ')
                if 'äº§å“' in interaction or 'å¥¶ç²‰' in interaction or 'å¥¶ç“¶' in interaction:
                    subjects.add('äº§å“')
            
            if subjects:
                summary_parts.append(f"ä¸»ä½“: {', '.join(sorted(subjects))}")
            
            # 2. è¡Œä¸ºç±»å‹åˆ†æ
            behavior_types = set()
            behavior_mapping = {
                'å–‚å…»è¡Œä¸º': ['å–‚', 'å–å¥¶', 'é¥®ç”¨', 'åƒ', 'å“ºä¹³'],
                'æŠ¤ç†è¡Œä¸º': ['æŠ¤ç†', 'æŠšæ‘¸', 'è½»æ‹', 'ç…§é¡¾', 'æ¢å°¿å¸ƒ', 'æ¸…æ´'],
                'äº’åŠ¨è¡Œä¸º': ['é€—', 'äº’åŠ¨', 'äº¤è°ˆ', 'ç©è€', 'æ‹ç…§', 'è‡ªæ‹'],
                'å±•ç¤ºè¡Œä¸º': ['å±•ç¤º', 'æ˜¾ç¤º', 'æ‹¿ç€', 'æ‰‹æŒ', 'å‘ˆç°', 'æ¨è'],
                'å‡†å¤‡è¡Œä¸º': ['å†²æ³¡', 'å‡†å¤‡', 'è°ƒåˆ¶', 'æ…æ‹Œ'],
                'æƒ…ç»ªè¡¨è¾¾': ['å“­é—¹', 'å“­æ³£', 'ç¬‘', 'å¼€å¿ƒ', 'ä¸å®‰', 'æ‹’ç»'],
                'æ•™è‚²è¡Œä¸º': ['æ•™å¯¼', 'æŒ‡å¯¼', 'è§£é‡Š', 'æ¼”ç¤º', 'è¯´æ˜'],
                'æ£€æŸ¥è¡Œä¸º': ['æ£€æŸ¥', 'è§‚å¯Ÿ', 'æŸ¥çœ‹', 'æµ‹è¯•', 'ç¡®è®¤']
            }
            
            for behavior_type, keywords in behavior_mapping.items():
                if any(keyword in interaction for interaction in interactions for keyword in keywords):
                    behavior_types.add(behavior_type)
            
            if behavior_types:
                summary_parts.append(f"è¡Œä¸º: {', '.join(sorted(behavior_types))}")
            
            # 3. äº§å“ç›¸å…³æ€§åˆ†æ
            product_relevance = []
            if any('å¥¶ç²‰' in interaction for interaction in interactions):
                product_relevance.append('å¥¶ç²‰ç›¸å…³')
            if any('å¥¶ç“¶' in interaction for interaction in interactions):
                product_relevance.append('å¥¶ç“¶ç›¸å…³')
            if any('è¥å…»' in interaction or 'æ ‡ç­¾' in interaction for interaction in interactions):
                product_relevance.append('è¥å…»ä¿¡æ¯')
            if any('å“ç‰Œ' in interaction or 'äº§å“' in interaction for interaction in interactions):
                product_relevance.append('äº§å“å±•ç¤º')
            
            if product_relevance:
                summary_parts.append(f"äº§å“: {', '.join(product_relevance)}")
            
            # 4. åœºæ™¯ç¯å¢ƒåˆ†æ
            scene_types = set()
            for scene in scenes:
                if 'å¨æˆ¿' in scene:
                    scene_types.add('å¨æˆ¿ç¯å¢ƒ')
                elif 'å®¢å…' in scene:
                    scene_types.add('å®¢å…ç¯å¢ƒ')
                elif 'åŒ»é™¢' in scene or 'è¯Šæ‰€' in scene:
                    scene_types.add('åŒ»ç–—ç¯å¢ƒ')
                elif 'æˆ·å¤–' in scene:
                    scene_types.add('æˆ·å¤–ç¯å¢ƒ')
                elif 'å§å®¤' in scene:
                    scene_types.add('å§å®¤ç¯å¢ƒ')
            
            if scene_types:
                summary_parts.append(f"åœºæ™¯: {', '.join(sorted(scene_types))}")
            
            # 5. æƒ…ç»ªå€¾å‘åˆ†æ
            emotion_analysis = self._analyze_emotion_trend(emotions)
            if emotion_analysis:
                summary_parts.append(f"æƒ…ç»ª: {emotion_analysis}")
            
            # 6. è§†é¢‘ç±»å‹å»ºè®®ï¼ˆç”¨äºåˆ†ç±»å‚è€ƒï¼‰
            video_type_hints = self._suggest_video_type_for_classification(interactions, subjects, behavior_types)
            if video_type_hints:
                summary_parts.append(f"ç±»å‹å»ºè®®: {video_type_hints}")
            
            # ç»„åˆæ€»ç»“
            return "; ".join(summary_parts) if summary_parts else "å¤šåœºæ™¯å†…å®¹"
            
        except Exception as e:
            logger.error(f"âŒ åœºæ™¯æ€»ç»“ç”Ÿæˆå¤±è´¥: {e}")
            return f"å¤šåœºæ™¯å†…å®¹({len(interactions)}ä¸ªåœºæ™¯)"

    def _analyze_emotion_trend(self, emotions: list) -> str:
        """åˆ†ææƒ…ç»ªè¶‹åŠ¿"""
        try:
            if not emotions:
                return ""
            
            positive_emotions = ['å¼€å¿ƒ', 'æ¸©é¦¨', 'æ„‰æ‚¦', 'æ»¡æ„', 'å¼€å¿ƒ', 'å¿«ä¹']
            negative_emotions = ['å“­é—¹', 'ä¸å®‰', 'ç„¦è™‘', 'ç—›è‹¦', 'æ‹…å¿ƒ', 'æ‹’ç»']
            neutral_emotions = ['å¹³é™', 'ä¸“æ³¨', 'ä¸­æ€§', 'è§‚å¯Ÿ', 'æ€è€ƒ']
            
            pos_count = sum(1 for emotion in emotions if any(pos in emotion for pos in positive_emotions))
            neg_count = sum(1 for emotion in emotions if any(neg in emotion for neg in negative_emotions))
            neu_count = sum(1 for emotion in emotions if any(neu in emotion for neu in neutral_emotions))
            
            if pos_count > neg_count and pos_count > neu_count:
                return "ç§¯æå€¾å‘"
            elif neg_count > pos_count and neg_count > neu_count:
                return "æ¶ˆæå€¾å‘"
            elif neu_count > pos_count and neu_count > neg_count:
                return "ä¸­æ€§å€¾å‘"
            else:
                return "æƒ…ç»ªæ··åˆ"
                
        except Exception as e:
            logger.error(f"âŒ æƒ…ç»ªè¶‹åŠ¿åˆ†æå¤±è´¥: {e}")
            return "æƒ…ç»ªæœªçŸ¥"

    def _suggest_video_type_for_classification(self, interactions: list, subjects: set, behavior_types: set) -> str:
        """åŸºäºåœºæ™¯å†…å®¹å»ºè®®è§†é¢‘ç±»å‹ï¼Œç”¨äºåç»­åˆ†ç±»å‚è€ƒ"""
        try:
            suggestions = []
            
            # äº§å“ä»‹ç»ç±»å‹åˆ¤æ–­
            if 'äº§å“' in subjects and 'å±•ç¤ºè¡Œä¸º' in behavior_types:
                suggestions.append('äº§å“ä»‹ç»')
            
            # ä½¿ç”¨æ•ˆæœç±»å‹åˆ¤æ–­
            if 'å®å®' in subjects and ('å–‚å…»è¡Œä¸º' in behavior_types or 'æƒ…ç»ªè¡¨è¾¾' in behavior_types):
                suggestions.append('ä½¿ç”¨æ•ˆæœ')
            
            # é’©å­ç±»å‹åˆ¤æ–­
            if 'æƒ…ç»ªè¡¨è¾¾' in behavior_types and any('å“­' in interaction or 'ä¸å®‰' in interaction for interaction in interactions):
                suggestions.append('é’©å­')
            
            # ä¿ƒé”€æœºåˆ¶ç±»å‹åˆ¤æ–­
            if 'åŒ»ç”Ÿ' in subjects and ('å±•ç¤ºè¡Œä¸º' in behavior_types or 'æ•™è‚²è¡Œä¸º' in behavior_types):
                suggestions.append('ä¿ƒé”€æœºåˆ¶')
            
            return ', '.join(suggestions) if suggestions else "å¾…åˆ†ç±»"
            
        except Exception as e:
            logger.error(f"âŒ è§†é¢‘ç±»å‹å»ºè®®å¤±è´¥: {e}")
            return "å¾…åˆ†ç±»"


class BatchSliceAnalyzer:
    """
    æ‰¹é‡åˆ‡ç‰‡åˆ†æå™¨
    """
    
    def __init__(self):
        """åˆå§‹åŒ–æ‰¹é‡åˆ†æå™¨"""
        self.dual_analyzer = DualStageAnalyzer()
        self.quality_control = get_quality_control()
        
    def analyze_batch(
        self, 
        video_files: List[str], 
        progress_callback: Optional[callable] = None  # type: ignore
    ) -> Dict[str, Any]:
        """
        æ‰¹é‡åˆ†æè§†é¢‘æ–‡ä»¶
        """
        logger.info(f"ğŸ¯ å¼€å§‹æ‰¹é‡åŒå±‚è¯†åˆ«åˆ†æï¼Œå…± {len(video_files)} ä¸ªæ–‡ä»¶")
        
        batch_result = {
            "total_files": len(video_files),
            "success_count": 0,
            "failed_count": 0,
            "results": [],
            "statistics": {},
            "start_time": time.time()
        }
        
        for i, video_file in enumerate(video_files):
            try:
                if progress_callback:
                    progress_callback(f"åˆ†æ {i+1}/{len(video_files)}: {Path(video_file).name}")
                
                # åˆ†æå•ä¸ªæ–‡ä»¶
                result = self.dual_analyzer.analyze_video_slice(video_file)
                
                if result["success"]:
                    batch_result["success_count"] += 1
                else:
                    batch_result["failed_count"] += 1
                
                batch_result["results"].append(result)
                
                # APIé™æµ
                time.sleep(0.5)
                
            except Exception as e:
                logger.error(f"æ‰¹é‡åˆ†ææ–‡ä»¶ {video_file} å¤±è´¥: {e}")
                batch_result["failed_count"] += 1
                batch_result["results"].append({
                    "file_path": video_file,
                    "success": False,
                    "error": str(e)
                })
        
        # ç”Ÿæˆç»Ÿè®¡ä¿¡æ¯
        batch_result["statistics"] = self._generate_batch_statistics(batch_result["results"])
        batch_result["end_time"] = time.time()
        batch_result["duration"] = batch_result["end_time"] - batch_result["start_time"]
        
        logger.info(f"âœ… æ‰¹é‡åˆ†æå®Œæˆ: æˆåŠŸ {batch_result['success_count']}, å¤±è´¥ {batch_result['failed_count']}")
        
        return batch_result
    
    def _generate_batch_statistics(self, results: List[Dict[str, Any]]) -> Dict[str, Any]:
        """ç”Ÿæˆæ‰¹é‡åˆ†æç»Ÿè®¡ä¿¡æ¯"""
        statistics = {
            "tag_frequency": {},
            "brand_frequency": {},
            "interaction_frequency": {},
            "scene_frequency": {},
            "emotion_frequency": {},
            "average_confidence": 0.0,
            "stage2_trigger_rate": 0.0
        }
        
        successful_results = [r for r in results if r.get("success", False)]
        if not successful_results:
            return statistics
        
        total_confidence = 0
        stage2_triggered = 0
        
        for result in successful_results:
            final_tags = result.get("final_tags", {})
            
            # ç»Ÿè®¡æ ‡ç­¾é¢‘æ¬¡
            all_tags = final_tags.get("all_tags", [])
            for tag in all_tags:
                statistics["tag_frequency"][tag] = statistics["tag_frequency"].get(tag, 0) + 1
            
            # ç»Ÿè®¡å„ç±»åˆ«é¢‘æ¬¡
            interaction = final_tags.get("interaction", "")
            if interaction:
                statistics["interaction_frequency"][interaction] = statistics["interaction_frequency"].get(interaction, 0) + 1
            
            scene = final_tags.get("scene", "")
            if scene:
                statistics["scene_frequency"][scene] = statistics["scene_frequency"].get(scene, 0) + 1
            
            emotion = final_tags.get("emotion", "")
            if emotion:
                statistics["emotion_frequency"][emotion] = statistics["emotion_frequency"].get(emotion, 0) + 1
            
            brand_elements = final_tags.get("brand_elements", "")
            if brand_elements:
                brands = [b.strip() for b in brand_elements.split(',') if b.strip()]
                for brand in brands:
                    statistics["brand_frequency"][brand] = statistics["brand_frequency"].get(brand, 0) + 1
            
            # ç´¯è®¡ç½®ä¿¡åº¦
            total_confidence += final_tags.get("confidence", 0.0)
            
            # ç»Ÿè®¡ç¬¬äºŒé˜¶æ®µè§¦å‘ç‡
            stage2_result = result.get("stage2_result", {})
            if stage2_result.get("triggered", False):
                stage2_triggered += 1
        
        # è®¡ç®—å¹³å‡å€¼
        statistics["average_confidence"] = total_confidence / len(successful_results)
        statistics["stage2_trigger_rate"] = stage2_triggered / len(successful_results) * 100
        
        return statistics 

def translate_json_file_with_deepseek(json_file_path: str) -> bool:
    """ç›´æ¥å¯¹JSONæ–‡ä»¶è¿›è¡ŒDeepSeekç¿»è¯‘ï¼Œç¿»è¯‘è‹±æ–‡å­—æ®µä¸ºä¸­æ–‡"""
    try:
        import json
        import requests
        import os
        from pathlib import Path
        
        # è¯»å–JSONæ–‡ä»¶
        with open(json_file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        # éœ€è¦ç¿»è¯‘çš„å­—æ®µ
        translate_fields = ['object', 'scene', 'emotion']
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦ç¿»è¯‘æˆ–æ™ºèƒ½æ¨æ–­
        needs_translation = False
        
        for field in translate_fields:
            if field in data and isinstance(data[field], str):
                # æ£€æŸ¥æ˜¯å¦åŒ…å«è‹±æ–‡éœ€è¦ç¿»è¯‘
                if any(c.isalpha() and ord(c) < 256 for c in data[field]):
                    needs_translation = True
                    break
        
        # ç‰¹åˆ«æ£€æŸ¥emotionå­—æ®µæ˜¯å¦éœ€è¦æ™ºèƒ½æ¨æ–­
        if ('emotion' in data and 
            data['emotion'].strip() in ['', 'æ— ', '[æ— ]', '[enthusiastically]', '[æ— æƒ…ç»ª]']):
            needs_translation = True
            print("ğŸ§  æ£€æµ‹åˆ°emotionå­—æ®µéœ€è¦æ™ºèƒ½æ¨æ–­")
        
        if not needs_translation:
            print("âœ… JSONæ–‡ä»¶å·²ç»æ˜¯ä¸­æ–‡ä¸”emotionå®Œæ•´ï¼Œæ— éœ€å¤„ç†")
            return True
        
        print(f"ğŸ”„ å¼€å§‹ç¿»è¯‘JSONæ–‡ä»¶: {json_file_path}")
        
        # è·å–DeepSeek APIå¯†é’¥
        api_key = os.getenv("DEEPSEEK_API_KEY")
        if not api_key:
            # ä»é…ç½®æ–‡ä»¶è·å–
            config_paths = [
                Path(__file__).parent.parent.parent / "feishu_pool" / ".env",
                Path(__file__).parent.parent / "config" / "env_config.txt"
            ]
            
            for config_path in config_paths:
                if config_path.exists():
                    try:
                        with open(config_path, 'r', encoding='utf-8') as f:
                            for line in f:
                                if line.strip().startswith('DEEPSEEK_API_KEY='):
                                    api_key = line.split('=', 1)[1].strip()
                                    if api_key.startswith('"') and api_key.endswith('"'):
                                        api_key = api_key[1:-1]
                                    elif api_key.startswith("'") and api_key.endswith("'"):
                                        api_key = api_key[1:-1]
                                    break
                        if api_key:
                            break
                    except Exception:
                        continue
        
        if not api_key:
            print("âŒ æœªæ‰¾åˆ°DeepSeek APIå¯†é’¥")
            return False
        
        # æ„é€ ç¿»è¯‘è¯·æ±‚
        translation_content = []
        for field in translate_fields:
            if field in data:
                translation_content.append(f"{field}: {data[field]}")
        
        content_to_translate = "\n".join(translation_content)
        
        # DeepSeekç¿»è¯‘æç¤ºè¯
        translate_prompt = f"""è¯·å°†ä»¥ä¸‹è§†é¢‘åˆ†æç»“æœç¿»è¯‘ä¸ºä¸­æ–‡æ ‡å‡†æ ¼å¼ï¼š

{content_to_translate}

è¦æ±‚ï¼š
1. objectå­—æ®µï¼šç¿»è¯‘ä¸º"ä¸»è¯­+åŠ¨è¯+å®¾è¯­"çš„ä¸­æ–‡æ ¼å¼ï¼Œå»é™¤æ–¹æ‹¬å·
2. sceneå­—æ®µï¼šç¿»è¯‘ä¸ºç®€æ´çš„ä¸­æ–‡åœºæ™¯æè¿°ï¼Œå»é™¤æ–¹æ‹¬å·
3. emotionå­—æ®µï¼šç¿»è¯‘ä¸ºå•ä¸ªä¸­æ–‡æƒ…ç»ªè¯ï¼Œå»é™¤æ–¹æ‹¬å·
4. ä¿æŒåŸæœ‰çš„åˆ†æå«ä¹‰ä¸å˜

ğŸ§  **æ™ºèƒ½emotionæ¨æ–­**ï¼š
å¦‚æœemotionå­—æ®µä¸ºç©ºã€"æ— "æˆ–"[æ— ]"ï¼Œè¯·æ ¹æ®objectå’Œsceneçš„å†…å®¹æ™ºèƒ½æ¨æ–­åˆé€‚çš„æƒ…ç»ªï¼š
- æ‹æ‰‹ã€ç¬‘å®¹ã€ç©è€ â†’ å¼€å¿ƒã€å…´å¥‹ã€å¿«ä¹
- å“­æ³£ã€æ‹’ç»ã€ä¸å®‰ â†’ ä¼¤å¿ƒã€ä¸å®‰ã€ç„¦è™‘  
- å–å¥¶ã€åƒé¥­ã€ç¡è§‰ â†’ æ»¡è¶³ã€å®‰é™ã€èˆ’é€‚
- æ•™å®¤ã€å­¦ä¹ ç¯å¢ƒ â†’ ä¸“æ³¨ã€å¥½å¥‡ã€ç§¯æ
- å®¶åº­ç¯å¢ƒã€äº²å­äº’åŠ¨ â†’ æ¸©é¦¨ã€å®‰å…¨ã€æ„‰æ‚¦

è¾“å‡ºæ ¼å¼ï¼ˆä¸¥æ ¼éµå¾ªï¼Œä¸è¦ä½¿ç”¨æ–¹æ‹¬å·ï¼‰ï¼š
object: ä¸­æ–‡ä¸»è¯­+åŠ¨è¯+å®¾è¯­
scene: ä¸­æ–‡åœºæ™¯æè¿°
emotion: ä¸­æ–‡æƒ…ç»ªè¯

ç›´æ¥è¾“å‡ºç¿»è¯‘ç»“æœï¼Œä¸è¦é¢å¤–è§£é‡Šï¼Œä¸è¦ä½¿ç”¨æ–¹æ‹¬å·ï¼š"""

        # è°ƒç”¨DeepSeek API
        payload = {
            "model": "deepseek-chat",
            "messages": [{"role": "user", "content": translate_prompt}],
            "max_tokens": 200,
            "temperature": 0.1
        }
        
        headers = {
            "Authorization": f"Bearer {api_key}",
            "Content-Type": "application/json"
        }
        
        print("ğŸš€ è°ƒç”¨DeepSeek APIè¿›è¡Œç¿»è¯‘...")
        response = requests.post(
            "https://api.deepseek.com/chat/completions",
            json=payload,
            headers=headers,
            timeout=15
        )
        
        if response.status_code == 200:
            result = response.json()
            if result.get("choices"):
                translated_text = result["choices"][0]["message"]["content"].strip()
                print(f"âœ… DeepSeekç¿»è¯‘ç»“æœ: {translated_text}")
                
                # è§£æç¿»è¯‘ç»“æœå¹¶åº”ç”¨æ ¼å¼æ¸…ç†
                for line in translated_text.split('\n'):
                    line = line.strip()
                    if ':' in line:
                        field, value = line.split(':', 1)
                        field = field.strip()
                        value = value.strip()
                        
                        # ğŸ”§ æ ¼å¼æ¸…ç†ï¼šç§»é™¤æ–¹æ‹¬å·å’Œå¤šä½™ç¬¦å·
                        value = value.replace('[', '').replace(']', '')
                        value = value.replace('"', '').replace("'", '')
                        
                        # æ¸…ç†é€—å·
                        if value.startswith(','):
                            value = value[1:]
                        if value.endswith(','):
                            value = value[:-1]
                        
                        # æ¸…ç†ç©ºæ ¼
                        value = ' '.join(value.split())
                        
                        # ç‰¹æ®Šå¤„ç†
                        if value.lower().strip() in ['æ— ', 'none', 'null', '']:
                            value = 'æ— '
                        
                        if field in translate_fields and field in data:
                            data[field] = value
                            print(f"âœ… æ›´æ–°å­—æ®µ {field}: {value}")
                
                # ä¿å­˜ç¿»è¯‘åçš„JSON
                with open(json_file_path, 'w', encoding='utf-8') as f:
                    json.dump(data, f, ensure_ascii=False, indent=2)
                
                print(f"âœ… JSONæ–‡ä»¶ç¿»è¯‘å®Œæˆ: {json_file_path}")
                return True
            else:
                print("âŒ DeepSeek APIå“åº”ä¸­æ²¡æœ‰choiceså­—æ®µ")
                return False
        else:
            print(f"âŒ DeepSeek APIè°ƒç”¨å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code}")
            print(f"é”™è¯¯ä¿¡æ¯: {response.text}")
            return False
            
    except Exception as e:
        print(f"âŒ JSONç¿»è¯‘å¤±è´¥: {e}")
        return False


"""
# ğŸ”„ åŸå§‹ Google API å®ç°ä»£ç ï¼ˆå·²æ³¨é‡Šï¼Œä¿ç•™ä½œä¸ºå‚è€ƒï¼‰
# åŸæ¥çš„ _call_gemini_video_api_new æ–¹æ³•å®ç°

def _call_gemini_video_api_new(self, client, video_path: str, prompt: str) -> Dict[str, Any]:
    # ä½¿ç”¨æ–°çš„Google Gen AI SDKè°ƒç”¨Geminiè§†é¢‘APIï¼ˆåŸå®ç°ï¼‰
    try:
        from google.genai import types
        
        logger.info(f"ğŸ“¤ å¼€å§‹ä¸Šä¼ è§†é¢‘æ–‡ä»¶: {video_path}")
        
        # ğŸ”§ **ä¿®å¤æ–‡ä»¶ä¸Šä¼ é—®é¢˜**ï¼šæ ¹æ®å®˜æ–¹æ–‡æ¡£ä½¿ç”¨æ­£ç¡®çš„APIè°ƒç”¨æ–¹å¼
        try:
            # æ–¹æ³•1: ä½¿ç”¨å®˜æ–¹æ–‡æ¡£æ¨èçš„æ–¹å¼ - ç›´æ¥ä¼ é€’fileå‚æ•°
            uploaded_file = client.files.upload(file=video_path)
            logger.info(f"âœ… æ–‡ä»¶ä¸Šä¼ æˆåŠŸï¼ŒURI: {uploaded_file.uri}")
            
        except Exception as e1:
            logger.warning(f"âš ï¸ æ–¹æ³•1å¤±è´¥: {e1}")
            try:
                # æ–¹æ³•2: ä½¿ç”¨æ–‡ä»¶å¯¹è±¡ä¸Šä¼ ï¼Œæ˜ç¡®æŒ‡å®šMIMEç±»å‹
                with open(video_path, 'rb') as video_file:
                    uploaded_file = client.files.upload(
                        file=video_file,
                        mime_type="video/mp4"
                    )
                logger.info(f"âœ… æ–¹æ³•2æˆåŠŸï¼šæ–‡ä»¶ä¸Šä¼ å®Œæˆï¼ŒURI: {uploaded_file.uri}")
                
            except Exception as e2:
                logger.warning(f"âš ï¸ æ–¹æ³•2å¤±è´¥: {e2}")
                try:
                    # æ–¹æ³•3: ä½¿ç”¨display_nameå’ŒMIMEç±»å‹çš„å®Œæ•´é…ç½®
                    with open(video_path, 'rb') as video_file:
                        uploaded_file = client.files.upload(
                            file=video_file,
                            mime_type="video/mp4",
                            display_name=f"video_analysis_{os.path.basename(video_path)}"
                        )
                    logger.info(f"âœ… æ–¹æ³•3æˆåŠŸï¼šæ–‡ä»¶ä¸Šä¼ å®Œæˆï¼ŒURI: {uploaded_file.uri}")
                    
                except Exception as e3:
                    logger.warning(f"âš ï¸ æ–¹æ³•3å¤±è´¥: {e3}")
                    raise Exception(f"æ‰€æœ‰ä¸Šä¼ æ–¹æ³•å‡å¤±è´¥: æ–¹æ³•1={e1}, æ–¹æ³•2={e2}, æ–¹æ³•3={e3}")
        
        # ğŸ”§ **æ–°å¢ï¼šç­‰å¾…æ–‡ä»¶å¤„ç†å®Œæˆ**
        import time
        max_wait_time = 60  # æœ€å¤§ç­‰å¾…60ç§’
        wait_interval = 2   # æ¯2ç§’æ£€æŸ¥ä¸€æ¬¡
        elapsed_time = 0
        
        logger.info("â³ ç­‰å¾…æ–‡ä»¶å¤„ç†å®Œæˆ...")
        
        while elapsed_time < max_wait_time:
            try:
                # æ£€æŸ¥æ–‡ä»¶çŠ¶æ€
                file_info = client.files.get(name=uploaded_file.name)
                file_state = getattr(file_info, 'state', 'UNKNOWN')
                
                logger.info(f"ğŸ“Š æ–‡ä»¶çŠ¶æ€æ£€æŸ¥: {file_state} (ç­‰å¾…æ—¶é—´: {elapsed_time}s)")
                
                if file_state == 'ACTIVE':
                    logger.info("âœ… æ–‡ä»¶å¤„ç†å®Œæˆï¼ŒçŠ¶æ€ä¸ºACTIVE")
                    break
                elif file_state == 'FAILED':
                    raise Exception("æ–‡ä»¶å¤„ç†å¤±è´¥")
                else:
                    # ç»§ç»­ç­‰å¾…
                    time.sleep(wait_interval)
                    elapsed_time += wait_interval
                    
            except Exception as status_error:
                logger.warning(f"âš ï¸ çŠ¶æ€æ£€æŸ¥å¤±è´¥: {status_error}")
                time.sleep(wait_interval)
                elapsed_time += wait_interval
        
        if elapsed_time >= max_wait_time:
            logger.warning(f"âš ï¸ æ–‡ä»¶å¤„ç†è¶…æ—¶ï¼ˆ{max_wait_time}sï¼‰ï¼Œå°è¯•ç»§ç»­åˆ†æ")
        
        logger.info(f"ğŸ¬ å¼€å§‹è§†é¢‘åˆ†æï¼Œæ¨¡å‹: {self.gemini_model}")
        
        # è°ƒç”¨è§†é¢‘åˆ†æAPI
        response = client.models.generate_content(
            model=self.gemini_model,
            contents=[prompt, uploaded_file]
        )
        
        if not response or not response.text:
            raise Exception("Gemini APIè¿”å›ç©ºå“åº”")
        
        logger.info(f"ğŸ“Š Geminiåˆ†æå®Œæˆï¼Œå“åº”é•¿åº¦: {len(response.text)}å­—ç¬¦")
        logger.info(f"ğŸ“ GeminiåŸå§‹å“åº”å¼€å¤´: {response.text[:200]}...")
        
        # æ¸…ç†ä¸Šä¼ çš„æ–‡ä»¶
        try:
            client.files.delete(name=uploaded_file.name)
            logger.info("ğŸ—‘ï¸ ä¸´æ—¶æ–‡ä»¶å·²æ¸…ç†")
        except Exception as cleanup_error:
            logger.warning(f"âš ï¸ æ¸…ç†ä¸´æ—¶æ–‡ä»¶å¤±è´¥: {cleanup_error}")
        
        # ğŸ”§ **æ–°æ–¹æ¡ˆ**ï¼šä¿æŒåŸå§‹è¾“å‡ºå®Œæ•´æ€§ï¼Œä¸å¼ºåˆ¶è§£æ
        logger.info("ğŸ“‹ ä¿æŒGeminiåŸå§‹è¾“å‡ºæ ¼å¼ï¼Œäº¤ç”±åç»­ç»Ÿä¸€å¤„ç†")
        
        return {
            'success': True,
            'content': response.text,  # ç›´æ¥è¿”å›åŸå§‹æ–‡æœ¬
            'model': 'gemini'
        }
            
    except Exception as e:
        logger.error(f"âŒ Gemini APIè°ƒç”¨å¼‚å¸¸: {e}")
        return {
            'success': False,
            'error': str(e),
            'model': 'gemini'
        }
"""

