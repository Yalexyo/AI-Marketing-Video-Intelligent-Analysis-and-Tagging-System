#!/usr/bin/env python3
"""
DeepSeek AIåˆ†æå™¨ - SRTè½¬äº§å“ä»‹ç»è§†é¢‘
ä½¿ç”¨DeepSeek APIåˆ†æSRTå†…å®¹ï¼Œè¯†åˆ«äº§å“ä»‹ç»ç›¸å…³ç‰‡æ®µ
"""

import json
import logging
import time
from typing import List, Dict, Optional, Tuple
from openai import OpenAI
from tenacity import retry, stop_after_attempt, wait_exponential

try:
    from .srt_parser import SRTSegment
    from .env_loader import (
        get_deepseek_api_key, get_deepseek_base_url, get_deepseek_model,
        get_max_tokens, get_temperature, get_product_keywords, get_brand_keywords,
        get_min_segment_duration, get_max_segment_duration
    )
except ImportError:
    from srt_parser import SRTSegment
    from env_loader import (
        get_deepseek_api_key, get_deepseek_base_url, get_deepseek_model,
        get_max_tokens, get_temperature, get_product_keywords, get_brand_keywords,
        get_min_segment_duration, get_max_segment_duration
    )

logger = logging.getLogger(__name__)

class ProductSegment:
    """ä»£è¡¨ä¸€ä¸ªäº§å“ä»‹ç»ç‰‡æ®µ"""
    def __init__(self, topic: str, sequence_ids: List[int], summary: str,
                 keywords: List[str], logic_pattern: str, confidence: float,
                 start_time: float = 0.0, end_time: float = 0.0,
                 scene_type: str = "æœªåˆ†ç±»"):
        self.topic = topic
        self.sequence_ids = sequence_ids
        self.summary = summary
        self.keywords = keywords
        self.logic_pattern = logic_pattern
        self.confidence = confidence
        self.start_time = start_time
        self.end_time = end_time
        self.duration = end_time - start_time if end_time > start_time else 0
        self.scene_type = scene_type # å¯é€‰çš„æ—§å­—æ®µ

class DeepSeekAnalyzer:
    """DeepSeek AIåˆ†æå™¨"""
    
    def __init__(self, api_key: Optional[str] = None):
        """
        åˆå§‹åŒ–DeepSeekåˆ†æå™¨
        
        Args:
            api_key: DeepSeek APIå¯†é’¥ï¼Œå¦‚æœä¸ºNoneåˆ™ä»ç¯å¢ƒå˜é‡è·å–
        """
        self.api_key = api_key or get_deepseek_api_key()
        if not self.api_key:
            raise ValueError("DeepSeek APIå¯†é’¥æœªè®¾ç½®")
        
        # åˆå§‹åŒ–OpenAIå®¢æˆ·ç«¯ (DeepSeekå…¼å®¹OpenAIæ¥å£)
        self.client = OpenAI(
            api_key=self.api_key,
            base_url=get_deepseek_base_url(),
            timeout=30.0  # æ·»åŠ 30ç§’è¶…æ—¶
        )
        
        self.model = get_deepseek_model()
        self.max_tokens = get_max_tokens()
        self.temperature = get_temperature()
        self.product_keywords = get_product_keywords()
        self.brand_keywords = get_brand_keywords()
        self.min_duration = get_min_segment_duration()
        self.max_duration = get_max_segment_duration()
        
        self.logger = logging.getLogger(__name__)
        
        self.logger.info(f"DeepSeekåˆ†æå™¨åˆå§‹åŒ–å®Œæˆ")
        self.logger.info(f"æ¨¡å‹: {self.model}, æœ€å¤§tokens: {self.max_tokens}")
        self.logger.info(f"äº§å“å…³é”®è¯: {len(self.product_keywords)}ä¸ª")
    
    @retry(stop=stop_after_attempt(2), wait=wait_exponential(multiplier=1, min=1, max=3))
    def analyze_srt_content(self, segments: List[SRTSegment], 
                           filename: str = "unknown") -> List[ProductSegment]:
        """
        åˆ†æSRTå†…å®¹ï¼Œè¯†åˆ«äº§å“ä»‹ç»ç‰‡æ®µ
        
        Args:
            segments: SRTç‰‡æ®µåˆ—è¡¨
            filename: æ–‡ä»¶åï¼ˆç”¨äºä¸Šä¸‹æ–‡ï¼‰
            
        Returns:
            äº§å“ä»‹ç»ç‰‡æ®µåˆ—è¡¨
        """
        if not segments:
            self.logger.warning("SRTç‰‡æ®µä¸ºç©ºï¼Œæ— æ³•åˆ†æ")
            return []
        
        try:
            # å‡†å¤‡åˆ†æå†…å®¹
            content_with_timestamps = self._prepare_content(segments)
            
            # ä¼˜å…ˆä½¿ç”¨ç²¾ç®€ç‰ˆpromptæé«˜é€Ÿåº¦
            prompt = self._build_analysis_prompt_optimized(content_with_timestamps, filename)
            
            # è°ƒç”¨DeepSeek API
            response = self._call_deepseek_api(prompt)
            
            # è§£æAIå“åº”
            product_segments = self._parse_ai_response(response, segments)
            
            self.logger.info(f"AIåˆ†æå®Œæˆï¼Œè¯†åˆ«åˆ°{len(product_segments)}ä¸ªäº§å“ä»‹ç»ç‰‡æ®µ")
            return product_segments
            
        except Exception as e:
            self.logger.error(f"DeepSeekåˆ†æå¤±è´¥: {e}")
            # ğŸš« ç¦ç”¨å¤‡ç”¨åˆ†æï¼Œé¿å…äº§ç”Ÿä½è´¨é‡çš„é¢å¤–ä¸»é¢˜
            self.logger.warning("ä¸ºç¡®ä¿è´¨é‡ï¼Œä¸ä½¿ç”¨å¤‡ç”¨åˆ†ææ–¹æ¡ˆ")
            return []
    
    def _prepare_content(self, segments: List[SRTSegment]) -> str:
        """å‡†å¤‡åˆ†æå†…å®¹ - ä½¿ç”¨åºåˆ—å·æ ¼å¼ä¸promptä¿æŒä¸€è‡´"""
        content_parts = []
        
        for segment in segments:
            # ä½¿ç”¨åºåˆ—å·æ ¼å¼ [1], [2] è€Œä¸æ˜¯æ—¶é—´æˆ³æ ¼å¼ [00:34]
            content_parts.append(f"[{segment.index}] {segment.text}")
        
        return '\n'.join(content_parts)
    
    def _format_timestamp(self, seconds: float) -> str:
        """æ ¼å¼åŒ–æ—¶é—´æˆ³"""
        minutes = int(seconds // 60)
        secs = int(seconds % 60)
        return f"{minutes:02d}:{secs:02d}"
    
    def _build_analysis_prompt_optimized(self, content: str, filename: str) -> str:
        """æ„å»ºAIåˆ†ææç¤ºï¼ˆTokenä¼˜åŒ–ç‰ˆï¼‰- ä¸“æ³¨ä¸‰å¤§äº§å“å“ç±»"""
        
        prompt = f"""ä½ æ˜¯äº§å“ä»‹ç»åˆ†æå¸ˆï¼Œä»å­—å¹•ä¸­è¯†åˆ«**å¯èµ‹å¥¶ç²‰äº§å“ä»‹ç»**å†…å®¹ã€‚

## ğŸ¯ æ ¸å¿ƒä»»åŠ¡
åªè¯†åˆ«ä»¥ä¸‹ä¸‰å¤§äº§å“å“ç±»çš„äº§å“ä»‹ç»å†…å®¹ï¼š
1. **å¯èµ‹è•´æ·³** - æ ¸å¿ƒå¥¶ç²‰äº§å“
2. **å¯èµ‹æ°´å¥¶** - ä¾¿æºè£…äº§å“  
3. **å¯èµ‹è“é’»** - é«˜ç«¯ç³»åˆ—äº§å“

## ğŸ“‹ äº§å“ä»‹ç»è¯†åˆ«æ ‡å‡†

### å¿…é¡»åŒ…å«çš„äº§å“ä»‹ç»è¦ç´ 
- **æˆåˆ†é…æ–¹**ï¼šæ¯ä¹³ä½èšç³–HMOã€A2å¥¶æºã€æ´»æ€§è›‹ç™½ã€OPNã€DHAç­‰
- **æ ¸å¿ƒå–ç‚¹**ï¼šè¥å…»æˆåˆ†ã€é…æ–¹ä¼˜åŠ¿ã€å“ç‰ŒèƒŒæ™¯ã€ç§‘ç ”å®åŠ›
- **äº§å“æ–¹æ¡ˆ**ï¼šå…·ä½“äº§å“åç§°ã€ä½¿ç”¨åœºæ™¯ã€äº§å“ç‰¹æ€§

### æ’é™¤çš„éäº§å“ä»‹ç»å†…å®¹
âŒ è‚²å„¿ç—›ç‚¹/å›°æƒ‘ï¼ˆè½¬å¥¶é—®é¢˜ã€è‚²å„¿ç„¦è™‘ç­‰ï¼‰
âŒ è‚²å„¿æˆæœ/ä½“éªŒï¼ˆå®å®è¡¨ç°ã€ä½“è´¨åŸºç¡€ç­‰ï¼‰  
âŒ ç”Ÿæ´»æ—¥å¸¸ï¼ˆæ—©é¤ã€å®¶é•¿ä¼šã€ç©è€ç­‰ï¼‰
âŒ çº¯ç§‘æ™®å†…å®¹ï¼ˆä¸æ¶‰åŠå…·ä½“äº§å“çš„è¥å…»çŸ¥è¯†ï¼‰

### äº§å“å…³é”®è¯è¯†åˆ«
- **å¯èµ‹è•´æ·³**ï¼šå¯èµ‹è•´æ·‡/è•´é†‡ã€ç‰¹è‰²é…æ–¹ã€HMOã€æ ¸å¿ƒæˆåˆ†ã€æƒ æ°èƒŒæ™¯
- **å¯èµ‹æ°´å¥¶**ï¼šæ°´å¥¶ã€ä¾¿æºã€A2å¥¶æºã€å°ç“¶è£…ã€æºå¸¦æ–¹ä¾¿
- **å¯èµ‹è“é’»**ï¼šè“é’»ã€é«˜ç«¯ã€å‡çº§é…æ–¹ï¼ˆå¦‚å‡ºç°ï¼‰

### âš ï¸ æ®µè½å†…äº§å“åˆ‡æ¢æ£€æµ‹
å¦‚æœä¸€ä¸ªæ®µè½å†…åŒ…å«å¤šä¸ªäº§å“çš„ä»‹ç»ï¼Œéœ€è¦ï¼š
1. **è¯†åˆ«åˆ‡æ¢ç‚¹**ï¼šæ‰¾åˆ°ä»ä¸€ä¸ªäº§å“è½¬å‘å¦ä¸€ä¸ªäº§å“çš„å…·ä½“ä½ç½®
2. **ç²¾ç¡®æ—¶é—´æ ‡æ³¨**ï¼šä½¿ç”¨ `time_offset_seconds` æ ‡æ³¨åˆ‡æ¢æ—¶é—´ç‚¹
3. **åˆ†åˆ«å½’ç±»**ï¼šå°†åŒä¸€æ®µè½çš„ä¸åŒéƒ¨åˆ†å½’ç±»åˆ°ä¸åŒäº§å“

### æ—¶é—´åç§»ç¤ºä¾‹
```json
{{
  "topic": "å¯èµ‹æ°´å¥¶ - ä¾¿æºäº§å“ä»‹ç»",
  "sequence_ids": [14],
  "time_offset_seconds": 5.0,
  "summary": "ä»å¥å­ä¸­é—´å¼€å§‹çš„æ°´å¥¶ä¾¿æºæ€§ä»‹ç»",
  "keywords": ["æ°´å¥¶", "ä¾¿æº", "å°ç“¶è£…"]
}}
```

## ç¤ºä¾‹è¾“å‡º
```json
{{
  "product_mentions": [
    {{
      "topic": "å¯èµ‹è•´æ·³ - æ ¸å¿ƒé…æ–¹ä»‹ç»",
      "sequence_ids": [8, 9, 10, 11, 12],
      "summary": "ç‰¹è‰²é…æ–¹ç§‘æ™®â†’HMOæ ¸å¿ƒæˆåˆ†â†’å“ç‰Œé€‰æ‹©â†’æƒ æ°ç§‘ç ”å®åŠ›ï¼Œå®Œæ•´äº§å“ä»‹ç»",
      "keywords": ["ç‰¹è‰²é…æ–¹", "HMO", "å¯èµ‹è•´æ·³", "æƒ æ°", "ç§‘ç ”å®åŠ›"],
      "logic_pattern": "äº§å“ä»‹ç»å‹",
      "confidence": 0.95
    }},
    {{
      "topic": "å¯èµ‹æ°´å¥¶ - ä¾¿æºäº§å“ä»‹ç»", 
      "sequence_ids": [14],
      "time_offset_seconds": 5.0,
      "summary": "æ°´å¥¶ä¾¿æºæ€§â†’ä½¿ç”¨åœºæ™¯ï¼Œäº§å“ç‰¹æ€§è¯´æ˜ï¼ˆä»æ®µè½ä¸­é—´å¼€å§‹ï¼‰",
      "keywords": ["æ°´å¥¶", "ä¾¿æº", "å°ç“¶è£…"],
      "logic_pattern": "äº§å“ä»‹ç»å‹",
      "confidence": 0.90
    }}
  ]
}}
```

## åˆ†æå†…å®¹
**æ–‡ä»¶**: {filename}
**å­—å¹•**: 
{content}

**è¾“å‡ºJSON**:
```json
```"""
        return prompt
    
    def _build_analysis_prompt(self, content: str, filename: str) -> str:
        """æ„å»ºAIåˆ†æprompt - ä¸“æ³¨ä¸‰å¤§äº§å“å“ç±»"""
        prompt = f"""ä½ æ˜¯äº§å“ä»‹ç»åˆ†æå¸ˆï¼Œä»å­—å¹•ä¸­è¯†åˆ«**å¯èµ‹å¥¶ç²‰äº§å“ä»‹ç»**å†…å®¹ã€‚

## ğŸ¯ æ ¸å¿ƒä»»åŠ¡
åªè¯†åˆ«ä»¥ä¸‹ä¸‰å¤§äº§å“å“ç±»çš„äº§å“ä»‹ç»å†…å®¹ï¼š
1. **å¯èµ‹è•´æ·³** - æ ¸å¿ƒå¥¶ç²‰äº§å“  
2. **å¯èµ‹æ°´å¥¶** - ä¾¿æºè£…äº§å“
3. **å¯èµ‹è“é’»** - é«˜ç«¯ç³»åˆ—äº§å“

## ğŸ“‹ äº§å“ä»‹ç»è¯†åˆ«æ ‡å‡†

### å¿…é¡»åŒ…å«çš„äº§å“ä»‹ç»è¦ç´ 
- **æˆåˆ†é…æ–¹**ï¼šæ¯ä¹³ä½èšç³–HMOã€A2å¥¶æºã€æ´»æ€§è›‹ç™½ã€OPNã€DHAç­‰æ ¸å¿ƒæˆåˆ†ä»‹ç»
- **æ ¸å¿ƒå–ç‚¹**ï¼šè¥å…»ä¼˜åŠ¿ã€é…æ–¹ç‰¹è‰²ã€å“ç‰ŒèƒŒæ™¯ã€ç§‘ç ”å®åŠ›ã€å®‰å…¨ä¿éšœ
- **äº§å“æ–¹æ¡ˆ**ï¼šå…·ä½“äº§å“åç§°ã€ä½¿ç”¨åœºæ™¯ã€äº§å“ç‰¹æ€§ã€åŠŸèƒ½è¯´æ˜

### ä¸¥æ ¼æ’é™¤çš„éäº§å“ä»‹ç»å†…å®¹
âŒ **è‚²å„¿ç—›ç‚¹/å›°æƒ‘**ï¼šè½¬å¥¶é—®é¢˜ã€è‚²å„¿ç„¦è™‘ã€å–‚å…»å›°æ‰°ç­‰
âŒ **è‚²å„¿æˆæœ/ä½“éªŒ**ï¼šå®å®è¡¨ç°ã€ä½“è´¨åŸºç¡€ã€æˆé•¿æ•ˆæœç­‰
âŒ **ç”Ÿæ´»æ—¥å¸¸åœºæ™¯**ï¼šæ—©é¤åˆ¶ä½œã€å®¶é•¿ä¼šã€æˆ·å¤–ç©è€ç­‰
âŒ **çº¯ç§‘æ™®æ•™è‚²**ï¼šä¸æ¶‰åŠå…·ä½“äº§å“çš„è¥å…»çŸ¥è¯†æ™®åŠ

### äº§å“å…³é”®è¯ç²¾å‡†è¯†åˆ«
- **å¯èµ‹è•´æ·³ç›¸å…³**ï¼šå¯èµ‹è•´æ·‡/è•´é†‡ã€ç‰¹è‰²é…æ–¹ã€HMOã€æ ¸å¿ƒæˆåˆ†ã€æƒ æ°èƒŒæ™¯ã€ç§‘ç ”å®åŠ›
- **å¯èµ‹æ°´å¥¶ç›¸å…³**ï¼šæ°´å¥¶ã€ä¾¿æºè£…ã€A2å¥¶æºã€å°ç“¶è£…ã€æºå¸¦æ–¹ä¾¿ã€åŒå“ç‰Œ
- **å¯èµ‹è“é’»ç›¸å…³**ï¼šè“é’»ã€é«˜ç«¯ç³»åˆ—ã€å‡çº§é…æ–¹ï¼ˆå¦‚å‡ºç°ï¼‰

### åºåˆ—å·é€‰æ‹©åŸåˆ™
- **æ—¶é•¿æ§åˆ¶**ï¼š10-40ç§’ç†æƒ³èŒƒå›´ï¼Œç¡®ä¿äº§å“ä»‹ç»å®Œæ•´
- **è¾¹ç•Œè¯†åˆ«**ï¼šä»äº§å“æåŠå¼€å§‹ï¼Œåˆ°äº§å“ä»‹ç»é€»è¾‘ç»“æŸ
- **é€»è¾‘å®Œæ•´**ï¼šåŒ…å«å®Œæ•´çš„äº§å“ä»‹ç»è¦ç´ é“¾æ¡

## ğŸ“˜ ç²¾å‡†è¯†åˆ«ç¤ºä¾‹

**æ–‡ä»¶å**: video_1_full.srt
**äº§å“ä»‹ç»ç‰‡æ®µè¯†åˆ«**:
```
[8] å…¶å®å…³é”®å°±æ˜¯ç‰¹è‰²é…æ–¹...é€‰å¥¶å°±çœ‹æ ¸å¿ƒæˆåˆ†å°±è¡Œã€‚   â† äº§å“ä»‹ç»å¼€å§‹
[9] è€Œæ¯ä¹³ä½èšç³–hm...èƒ½å¸®å®å®å»¶ç»­å¤©ç„¶è¥å…»...     â† HMOæ ¸å¿ƒæˆåˆ†
[10] é€‰hmå¥¶ç²‰ï¼Œæˆ‘ä¹Ÿè®¤çœŸå¯¹æ¯”ä¸å°‘å“ç‰Œï¼Œå‘ç°å¯èµ‹è•´é†‡æ›´é€‚åˆå¦®å¦®ã€‚â† äº§å“é€‰æ‹©
[11] å¥¶ç²‰æ˜¯å®å®å…¥å£çš„ä¸œè¥¿...æˆ‘ä¼šæ¯”è¾ƒå…³å¿ƒå“ç‰ŒèƒŒæ™¯ã€‚  â† å“ç‰Œé‡è¦æ€§
[12] å¯èµ‹èƒŒé æƒ æ°åˆ¶è¯å…¬å¸...æ›´æœ‰ç§‘ç ”å®åŠ›ã€‚        â† å“ç‰ŒèƒŒæ™¯ä»‹ç»
[14] æˆ‘å’Œçˆ¸çˆ¸éƒ½è‚ èƒƒä¸å¤ªå¥½...æ°´å¥¶ï¼Œå°å°ä¸€ç“¶ï¼Œæºå¸¦å¾ˆæ–¹ä¾¿...â† æ°´å¥¶äº§å“ä»‹ç»
```

**åº”è¯¥è¯†åˆ«**:
- **å¯èµ‹è•´æ·³äº§å“ä»‹ç»**: [8,9,10,11,12] - å®Œæ•´äº§å“ä»‹ç»é€»è¾‘é“¾
- **å¯èµ‹æ°´å¥¶äº§å“ä»‹ç»**: [14] - æ°´å¥¶äº§å“ç‰¹æ€§ä»‹ç»

**åº”è¯¥æ’é™¤**:
- **è‚²å„¿ä½“éªŒ**: [3] "å¦®å¦®è¢«å…»çš„å¾ˆå¥½" - éäº§å“ä»‹ç»
- **è‚²å„¿å›°æƒ‘**: [4,5] "è½¬å¥¶ç”Ÿç—…+ä¸“å®¶å»ºè®®" - éäº§å“ä»‹ç»  
- **ç”Ÿæ´»åœºæ™¯**: [16,17,18] "å¹¼å„¿å›­+å–å¥¶åœºæ™¯" - éäº§å“ä»‹ç»

**AIåº”è¯¥è¾“å‡º**:
```json
{{
  "product_mentions": [
    {{
      "topic": "å¯èµ‹è•´æ·³ - æ ¸å¿ƒé…æ–¹ä»‹ç»",
      "sequence_ids": [8, 9, 10, 11, 12],
      "summary": "ç‰¹è‰²é…æ–¹ç§‘æ™®â†’HMOæ ¸å¿ƒæˆåˆ†â†’å“ç‰Œå¯¹æ¯”é€‰æ‹©â†’å®‰å…¨é‡è¦æ€§â†’æƒ æ°ç§‘ç ”èƒŒæ™¯ï¼Œå®Œæ•´äº§å“ä»‹ç»é€»è¾‘",
      "keywords": ["ç‰¹è‰²é…æ–¹", "HMO", "å¯èµ‹è•´æ·³", "æƒ æ°", "ç§‘ç ”å®åŠ›"],
      "logic_pattern": "äº§å“ä»‹ç»å‹",
      "confidence": 0.95
    }},
    {{
      "topic": "å¯èµ‹æ°´å¥¶ - ä¾¿æºäº§å“ä»‹ç»",
      "sequence_ids": [14],
      "summary": "A2å¥¶æºç‰¹æ€§â†’æ°´å¥¶ä¾¿æºæ€§â†’ä½¿ç”¨åœºæ™¯ï¼Œäº§å“ç‰¹è‰²è¯´æ˜",
      "keywords": ["A2å¥¶æº", "æ°´å¥¶", "ä¾¿æº", "å°ç“¶è£…"],
      "logic_pattern": "äº§å“ä»‹ç»å‹",
      "confidence": 0.90
    }}
  ]
}}
```

## ğŸ¯ ä½ çš„ä»»åŠ¡
åˆ†æä»¥ä¸‹å­—å¹•å†…å®¹ï¼Œ**ä¸¥æ ¼æŒ‰ç…§äº§å“ä»‹ç»æ ‡å‡†**ï¼Œåªè¯†åˆ«ä¸‰å¤§äº§å“å“ç±»çš„çº¯äº§å“ä»‹ç»å†…å®¹ã€‚

**æ–‡ä»¶å**: {filename}
**å­—å¹•å†…å®¹**:
{content}

**è¾“å‡ºJSON**:
```json
```"""
        return prompt
    
    def _call_deepseek_api(self, prompt: str) -> str:
        """è°ƒç”¨DeepSeek API"""
        try:
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {
                        "role": "user", 
                        "content": prompt
                    }
                ],
                max_tokens=self.max_tokens,
                temperature=self.temperature
            )
            
            return response.choices[0].message.content
            
        except Exception as e:
            self.logger.error(f"DeepSeek APIè°ƒç”¨å¤±è´¥: {e}")
            raise
    
    def _parse_ai_response(self, response: str, segments: List[SRTSegment]) -> List[ProductSegment]:
        """è§£æAIå“åº”"""
        try:
            # æ¸…ç†å“åº”å†…å®¹ï¼Œåªä¿ç•™JSONéƒ¨åˆ†
            response = response.strip()
            if '```json' in response:
                response = response.split('```json')[1].split('```')[0]
            elif '```' in response:
                response = response.split('```')[1].split('```')[0]
            
            data = json.loads(response)
            product_mentions = []
            
            for item in data.get('product_mentions', []):
                topic = item.get('topic', 'æœªå®šä¹‰ä¸»é¢˜')
                sequence_ids = item.get('sequence_ids', [])
                summary = item.get('summary', '')
                keywords = item.get('keywords', [])
                logic_pattern = item.get('logic_pattern', 'å…¶ä»–')
                confidence = float(item.get('confidence', 0.0))
                time_offset_seconds = float(item.get('time_offset_seconds', 0.0))
                
                if not sequence_ids:
                    self.logger.warning(f"è·³è¿‡æ— æ•ˆæ•°æ®ï¼ˆç¼ºå°‘sequence_idsï¼‰: {item}")
                    continue
                
                # ä»åºå·è®¡ç®—ç²¾ç¡®æ—¶é—´ï¼Œæ”¯æŒæ—¶é—´åç§»
                start_time, end_time = self._get_time_from_sequence(sequence_ids, segments, time_offset_seconds)
                
                product_mention = ProductSegment(
                    topic=topic,
                    sequence_ids=sequence_ids,
                    summary=summary,
                    keywords=keywords,
                    logic_pattern=logic_pattern,
                    confidence=confidence,
                    start_time=start_time,
                    end_time=end_time
                )
                product_mentions.append(product_mention)
                
                if time_offset_seconds > 0:
                    self.logger.debug(f"æˆåŠŸè§£æä¸»é¢˜: '{topic}'ï¼Œåºå·: {sequence_ids}ï¼Œæ—¶é—´åç§»: {time_offset_seconds}s")
                else:
                    self.logger.debug(f"æˆåŠŸè§£æä¸»é¢˜: '{topic}'ï¼Œåºå·: {sequence_ids}")

            # æŒ‰ç½®ä¿¡åº¦æ’åº
            product_mentions.sort(key=lambda x: x.confidence, reverse=True)
            
            return product_mentions
            
        except json.JSONDecodeError as e:
            self.logger.error(f"AIå“åº”JSONè§£æå¤±è´¥: {e}")
            self.logger.debug(f"åŸå§‹å“åº”: {response}")
            return []
        except Exception as e:
            self.logger.error(f"AIå“åº”è§£æå¤±è´¥: {e}")
            return []
    
    def _get_time_from_sequence(self, ids: List[int], segments: List[SRTSegment], time_offset_seconds: float = 0.0) -> tuple:
        """æ ¹æ®å­—å¹•åºå·åˆ—è¡¨è·å–èµ·æ­¢æ—¶é—´ï¼Œæ”¯æŒæ—¶é—´åç§»"""
        if not ids:
            return 0.0, 0.0
        
        # å°†åºå·è½¬æ¢ä¸ºä»0å¼€å§‹çš„ç´¢å¼•
        indices = [i - 1 for i in ids]
        
        # éªŒè¯ç´¢å¼•èŒƒå›´
        valid_indices = [i for i in indices if 0 <= i < len(segments)]
        if not valid_indices:
            return 0.0, 0.0
            
        start_time = segments[min(valid_indices)].start_time
        end_time = segments[max(valid_indices)].end_time
        
        # åº”ç”¨æ—¶é—´åç§»
        if time_offset_seconds > 0:
            # å¦‚æœæœ‰æ—¶é—´åç§»ï¼Œè°ƒæ•´å¼€å§‹æ—¶é—´
            adjusted_start = start_time + time_offset_seconds
            # ç¡®ä¿è°ƒæ•´åçš„å¼€å§‹æ—¶é—´ä¸è¶…è¿‡ç»“æŸæ—¶é—´
            if adjusted_start < end_time:
                start_time = adjusted_start
                self.logger.debug(f"åº”ç”¨æ—¶é—´åç§» {time_offset_seconds}s: {start_time:.1f}s -> {end_time:.1f}s")
            else:
                self.logger.warning(f"æ—¶é—´åç§» {time_offset_seconds}s è¿‡å¤§ï¼Œè·³è¿‡è°ƒæ•´")
        
        return start_time, end_time
    
    def _map_to_precise_timestamps(self, ai_start: float, ai_end: float, 
                                 segments: List[SRTSegment]) -> tuple:
        """å°†AIè¿”å›çš„æ—¶é—´æˆ³æ˜ å°„åˆ°ç²¾ç¡®çš„SRTæ—¶é—´æˆ³"""
        # æ‰¾åˆ°æœ€æ¥è¿‘AIæ—¶é—´æˆ³çš„SRTç‰‡æ®µ
        start_segment = None
        end_segment = None
        
        # æ‰¾åˆ°å¼€å§‹æ—¶é—´å¯¹åº”çš„ç‰‡æ®µ
        for segment in segments:
            if segment.start_time <= ai_start <= segment.end_time:
                start_segment = segment
                break
        
        # å¦‚æœæ²¡æ‰¾åˆ°ç²¾ç¡®åŒ¹é…ï¼Œæ‰¾æœ€æ¥è¿‘çš„
        if start_segment is None:
            min_diff = float('inf')
            for segment in segments:
                diff = abs(segment.start_time - ai_start)
                if diff < min_diff:
                    min_diff = diff
                    start_segment = segment
        
        # æ‰¾åˆ°ç»“æŸæ—¶é—´å¯¹åº”çš„ç‰‡æ®µ
        for segment in segments:
            if segment.start_time <= ai_end <= segment.end_time:
                end_segment = segment
                break
        
        # å¦‚æœæ²¡æ‰¾åˆ°ç²¾ç¡®åŒ¹é…ï¼Œæ‰¾æœ€æ¥è¿‘çš„
        if end_segment is None:
            min_diff = float('inf')
            for segment in segments:
                diff = abs(segment.end_time - ai_end)
                if diff < min_diff:
                    min_diff = diff
                    end_segment = segment
        
        # è¿”å›ç²¾ç¡®çš„SRTæ—¶é—´æˆ³
        precise_start = start_segment.start_time if start_segment else ai_start
        precise_end = end_segment.end_time if end_segment else ai_end
        
        return precise_start, precise_end
    
    def _validate_time_range(self, start_time: float, end_time: float, 
                           segments: List[SRTSegment]) -> bool:
        """éªŒè¯æ—¶é—´èŒƒå›´æ˜¯å¦æœ‰æ•ˆ"""
        if start_time >= end_time:
            return False
        
        duration = end_time - start_time
        if duration < self.min_duration or duration > self.max_duration:
            return False
        
        # æ£€æŸ¥æ—¶é—´èŒƒå›´æ˜¯å¦åœ¨SRTç‰‡æ®µèŒƒå›´å†…
        total_duration = max(seg.end_time for seg in segments)
        if start_time < 0 or end_time > total_duration:
            return False
        
        return True
    
    def _fallback_keyword_analysis(self, segments: List[SRTSegment]) -> List[ProductSegment]:
        """å…³é”®è¯å¤‡ç”¨åˆ†ææ–¹æ¡ˆ"""
        self.logger.info("ä½¿ç”¨å…³é”®è¯å¤‡ç”¨åˆ†ææ–¹æ¡ˆ")
        
        product_segments = []
        current_segment_start = None
        current_keywords = []
        
        for i, segment in enumerate(segments):
            # æ£€æŸ¥æ˜¯å¦åŒ…å«äº§å“å…³é”®è¯
            matched_keywords = []
            for keyword in self.product_keywords + self.brand_keywords:
                if keyword in segment.text:
                    matched_keywords.append(keyword)
            
            if matched_keywords:
                if current_segment_start is None:
                    current_segment_start = segment.start_time
                current_keywords.extend(matched_keywords)
                
                # æ£€æŸ¥æ˜¯å¦åº”è¯¥ç»“æŸå½“å‰ç‰‡æ®µ
                if (i == len(segments) - 1 or  # æœ€åä¸€ä¸ªç‰‡æ®µ
                    segment.end_time - current_segment_start >= self.max_duration):
                    
                    duration = segment.end_time - current_segment_start
                    if duration >= self.min_duration:
                        confidence = min(len(set(current_keywords)) * 0.2, 1.0)
                        
                        product_segment = ProductSegment(
                            topic="å…³é”®è¯åŒ¹é…",
                            sequence_ids=[],
                            summary="å…³é”®è¯åŒ¹é…",
                            keywords=list(set(current_keywords)),
                            logic_pattern="å…³é”®è¯åŒ¹é…",
                            confidence=confidence,
                            start_time=current_segment_start,
                            end_time=segment.end_time
                        )
                        product_segments.append(product_segment)
                    
                    current_segment_start = None
                    current_keywords = []
            else:
                # å¦‚æœå½“å‰ç‰‡æ®µæ²¡æœ‰å…³é”®è¯ï¼Œç»“æŸå½“å‰äº§å“ç‰‡æ®µ
                if current_segment_start is not None:
                    duration = segments[i-1].end_time - current_segment_start
                    if duration >= self.min_duration:
                        confidence = min(len(set(current_keywords)) * 0.2, 1.0)
                        
                        product_segment = ProductSegment(
                            topic="å…³é”®è¯åŒ¹é…",
                            sequence_ids=[],
                            summary="å…³é”®è¯åŒ¹é…",
                            keywords=list(set(current_keywords)),
                            logic_pattern="å…³é”®è¯åŒ¹é…",
                            confidence=confidence,
                            start_time=current_segment_start,
                            end_time=segments[i-1].end_time
                        )
                        product_segments.append(product_segment)
                    
                    current_segment_start = None
                    current_keywords = []
        
        # æŒ‰ç½®ä¿¡åº¦æ’åº
        product_segments.sort(key=lambda x: x.confidence, reverse=True)
        
        self.logger.info(f"å…³é”®è¯åˆ†æå®Œæˆï¼Œæ‰¾åˆ°{len(product_segments)}ä¸ªäº§å“ç‰‡æ®µ")
        return product_segments
    
    def get_best_segment(self, product_segments: List[ProductSegment]) -> Optional[ProductSegment]:
        """
        ä»åˆ†æå‡ºçš„äº§å“ç‰‡æ®µåˆ—è¡¨ä¸­é€‰æ‹©æœ€ä½³çš„ä¸€ä¸ªã€‚
        åœ¨æ–°æ¨¡å¼ä¸‹ï¼Œè¿™é€šå¸¸æ˜¯ç½®ä¿¡åº¦æœ€é«˜çš„ç‰‡æ®µã€‚
        """
        if not product_segments:
            return None
        
        # åˆ—è¡¨åœ¨è§£ææ—¶å·²æŒ‰ç½®ä¿¡åº¦æ’åºï¼Œæ‰€ä»¥ç¬¬ä¸€ä¸ªå°±æ˜¯æœ€ä½³çš„
        return product_segments[0]
    
    def get_analysis_summary(self, product_segments: List[ProductSegment]) -> Dict:
        """è·å–AIåˆ†æçš„æ‘˜è¦ç»Ÿè®¡"""
        if not product_segments:
            return {'total_segments': 0}
        
        # ç»Ÿè®¡é€»è¾‘æ¨¡å¼åˆ†å¸ƒ
        logic_patterns = {}
        for seg in product_segments:
            pattern = seg.logic_pattern
            logic_patterns[pattern] = logic_patterns.get(pattern, 0) + 1
        
        # ç»Ÿè®¡å®Œæ•´æ€§åˆ†å¸ƒ
        high_completeness = [seg for seg in product_segments if seg.confidence >= 0.8]
        medium_completeness = [seg for seg in product_segments if 0.5 <= seg.confidence < 0.8]
        low_completeness = [seg for seg in product_segments if seg.confidence < 0.5]
        
        return {
            'total_segments': len(product_segments),
            'best_confidence': product_segments[0].confidence,
            'best_completeness': product_segments[0].confidence, # åœ¨æ–°æ¨¡å¼ä¸‹ï¼Œcompletenessç”±confidenceä½“ç°
            'avg_confidence': sum(seg.confidence for seg in product_segments) / len(product_segments),
            'avg_completeness': sum(seg.confidence for seg in product_segments) / len(product_segments),
            'total_duration': sum(seg.duration for seg in product_segments),
            'avg_duration': sum(seg.duration for seg in product_segments) / len(product_segments),
            'unique_keywords': len(set(kw for seg in product_segments for kw in seg.keywords)),
            'logic_patterns': logic_patterns,
            'completeness_distribution': {
                'high_completeness': len(high_completeness),
                'medium_completeness': len(medium_completeness),
                'low_completeness': len(low_completeness)
            },
            'best_segment_info': {
                'topic': product_segments[0].topic,
                'logic_pattern': product_segments[0].logic_pattern,
                'scene_type': product_segments[0].scene_type,
                'duration': product_segments[0].duration,
                'keywords_count': len(product_segments[0].keywords),
                'sequence_ids': product_segments[0].sequence_ids
            }
        }
    
    def _build_keyword_screening_prompt(self, content: str, filename: str) -> str:
        """ä¸ºå…³é”®è¯ç­›é€‰æ„å»ºprompt"""
        
        prompt = f"""
åˆ†æå­—å¹•ï¼Œå¿«é€Ÿè¯†åˆ«äº§å“ä»‹ç»ç›¸å…³ç‰‡æ®µåºå·ã€‚

äº§å“å…³é”®è¯: {', '.join(self.product_keywords[:10])}
å“ç‰Œå…³é”®è¯: {', '.join(self.brand_keywords[:5])}

å­—å¹•å†…å®¹:
{content}

è¾“å‡ºåŒ…å«äº§å“ä»‹ç»çš„ç‰‡æ®µåºå·(å¦‚: 8,9,10,11,12):
"""
        return prompt
    
    def analyze_srt_content_layered(self, segments: List[SRTSegment], 
                                   filename: str = "unknown") -> List[ProductSegment]:
        """
        åˆ†å±‚åˆ†æSRTå†…å®¹ï¼ˆTokenä¼˜åŒ–ç‰ˆï¼‰
        ç¬¬ä¸€å±‚ï¼šå…³é”®è¯å¿«é€Ÿç­›é€‰ (~200 tokens)
        ç¬¬äºŒå±‚ï¼šè¯¦ç»†AIåˆ†æç­›é€‰ç»“æœ (~800 tokens)
        æ€»è®¡å¯èŠ‚çœ40-60% tokenä½¿ç”¨
        """
        if not segments:
            self.logger.warning("SRTç‰‡æ®µä¸ºç©ºï¼Œæ— æ³•åˆ†æ")
            return []
        
        try:
            # ç¬¬ä¸€å±‚ï¼šå…³é”®è¯é¢„ç­›é€‰
            self.logger.info("ç¬¬ä¸€å±‚åˆ†æï¼šå…³é”®è¯é¢„ç­›é€‰...")
            
            # åˆ›å»ºç®€åŒ–çš„å†…å®¹ç”¨äºé¢„ç­›é€‰
            screening_content = self._prepare_screening_content(segments)
            screening_prompt = self._build_keyword_screening_prompt(screening_content, filename)
            
            # è°ƒç”¨APIè¿›è¡Œé¢„ç­›é€‰
            screening_response = self._call_deepseek_api(screening_prompt)
            
            # è§£æé¢„ç­›é€‰ç»“æœ
            candidate_indices = self._parse_screening_response(screening_response)
            
            if not candidate_indices:
                self.logger.warning("é¢„ç­›é€‰æœªæ‰¾åˆ°å€™é€‰ç‰‡æ®µï¼Œä½¿ç”¨å…¨é‡åˆ†æ")
                return self.analyze_srt_content(segments, filename)
            
            # ç¬¬äºŒå±‚ï¼šè¯¦ç»†åˆ†æå€™é€‰ç‰‡æ®µ
            self.logger.info(f"ç¬¬äºŒå±‚åˆ†æï¼šè¯¦ç»†åˆ†æ{len(candidate_indices)}ä¸ªå€™é€‰ç‰‡æ®µ...")
            
            candidate_segments = [segments[i-1] for i in candidate_indices if 1 <= i <= len(segments)]
            
            if not candidate_segments:
                return []
            
            # å¯¹å€™é€‰ç‰‡æ®µè¿›è¡Œè¯¦ç»†åˆ†æ
            candidate_content = self._prepare_content(candidate_segments)
            detailed_prompt = self._build_analysis_prompt_optimized(candidate_content, filename)
            
            # è°ƒç”¨APIè¿›è¡Œè¯¦ç»†åˆ†æ
            detailed_response = self._call_deepseek_api(detailed_prompt)
            
            # è§£æè¯¦ç»†åˆ†æç»“æœ
            product_segments = self._parse_ai_response(detailed_response, segments)
            
            self.logger.info(f"åˆ†å±‚åˆ†æå®Œæˆï¼Œè¯†åˆ«åˆ°{len(product_segments)}ä¸ªäº§å“ä»‹ç»ç‰‡æ®µ")
            return product_segments
            
        except Exception as e:
            self.logger.error(f"åˆ†å±‚åˆ†æå¤±è´¥: {e}")
            # å¦‚æœåˆ†å±‚åˆ†æå¤±è´¥ï¼Œå›é€€åˆ°æ ‡å‡†åˆ†æ
            return self.analyze_srt_content(segments, filename)
    
    def _prepare_screening_content(self, segments: List[SRTSegment]) -> str:
        """å‡†å¤‡é¢„ç­›é€‰å†…å®¹ï¼ˆæç®€ç‰ˆï¼‰"""
        content_parts = []
        
        for i, segment in enumerate(segments, 1):
            # åªä¿ç•™åºå·å’Œæ–‡æœ¬ï¼Œå»é™¤è¯¦ç»†æ—¶é—´æˆ³
            content_parts.append(f"{i}. {segment.text}")
        
        return '\n'.join(content_parts)
    
    def _parse_screening_response(self, response: str) -> List[int]:
        """è§£æé¢„ç­›é€‰å“åº”ï¼Œæå–ç‰‡æ®µåºå·"""
        try:
            # æå–æ•°å­—åºå·
            import re
            numbers = re.findall(r'\d+', response)
            return [int(n) for n in numbers if int(n) > 0]
        except Exception as e:
            self.logger.warning(f"é¢„ç­›é€‰å“åº”è§£æå¤±è´¥: {e}")
            return [] 