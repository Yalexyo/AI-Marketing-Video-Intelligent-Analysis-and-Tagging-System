#!/usr/bin/env python3
"""
è¯çº§è¯­ä¹‰åˆ†å‰²å™¨ - æ™ºèƒ½SRTè¯­ä¹‰åˆ†æä¸ç²¾ç¡®åˆ‡åˆ†

åŠŸèƒ½ç‰¹æ€§:
1. æå–è¯çº§æ—¶é—´æˆ³ (ç²¾ç¡®åˆ°æ¯ä¸ªè¯æ±‡)
2. DeepSeek/Claudeè¯­ä¹‰åˆ†æ (é’©å­ã€äº§å“ä»‹ç»ã€ä½¿ç”¨æ•ˆæœã€ä¿ƒé”€æœºåˆ¶)
3. æ¨¡å‹æ•ˆæœå¯¹æ¯”åˆ†æ
4. è¾“å‡ºç²¾ç¡®çš„æ¨¡å—åŒ–SRT

ä½¿ç”¨åœºæ™¯:
- è§£å†³é•¿ç‰‡æ®µæ¨¡å—åˆ’åˆ†ä¸æ¸…æ™°é—®é¢˜
- æä¾›è¯çº§ç²¾åº¦çš„æ—¶é—´æˆ³
- è¯­ä¹‰é©±åŠ¨çš„æ™ºèƒ½åˆ‡åˆ†
"""

import json
import logging
import tempfile
import requests
from pathlib import Path
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass
import os
import sys

# æ·»åŠ å½“å‰è·¯å¾„åˆ°ç³»ç»Ÿè·¯å¾„
current_dir = os.path.dirname(os.path.abspath(__file__))
if current_dir not in sys.path:
    sys.path.insert(0, current_dir)

from env_loader import load_env_config

logger = logging.getLogger(__name__)

@dataclass
class WordTimestamp:
    """è¯çº§æ—¶é—´æˆ³æ•°æ®ç»“æ„"""
    text: str
    start_time: float
    end_time: float
    confidence: float

@dataclass
class SemanticSegment:
    """è¯­ä¹‰ç‰‡æ®µæ•°æ®ç»“æ„"""
    id: int
    start_time: float
    end_time: float
    text: str
    semantic_label: str  # ğŸªé’©å­, ğŸ¼äº§å“ä»‹ç», ğŸŒŸä½¿ç”¨æ•ˆæœ, ğŸä¿ƒé”€æœºåˆ¶
    confidence: float
    words: List[WordTimestamp]

class WordLevelSemanticSplitter:
    """è¯çº§è¯­ä¹‰åˆ†å‰²å™¨"""
    
    def __init__(self, deepseek_api_key: str = None, claude_api_key: str = None, domain_config: Dict[str, Any] = None):
        """
        åˆå§‹åŒ–è¯çº§è¯­ä¹‰åˆ†å‰²å™¨
        
        Args:
            deepseek_api_key: DeepSeek APIå¯†é’¥
            claude_api_key: Claude APIå¯†é’¥  
            domain_config: é¢†åŸŸé…ç½®ï¼ŒåŒ…å«ç±»åˆ«å®šä¹‰ã€å…³é”®è¯ç­‰
        """
        # åŠ è½½ç¯å¢ƒé…ç½®
        load_env_config()
        
        # APIå¯†é’¥é…ç½®
        self.deepseek_api_key = deepseek_api_key or os.getenv('DEEPSEEK_API_KEY')
        self.claude_api_key = claude_api_key or os.getenv('OPENROUTER_API_KEY')
        
        # é¢†åŸŸé…ç½® - æ”¯æŒè‡ªå®šä¹‰æˆ–ä½¿ç”¨é»˜è®¤æ¯å©´é…ç½®
        self.domain_config = domain_config or self._get_default_baby_formula_config()
        
        logger.info("âœ… è¯çº§è¯­ä¹‰åˆ†å‰²å™¨åˆå§‹åŒ–å®Œæˆ")
        logger.info(f"ğŸ¯ é¢†åŸŸ: {self.domain_config.get('domain_name', 'æœªçŸ¥')}")
        logger.info(f"ğŸ“‹ æ”¯æŒç±»åˆ«: {list(self.domain_config.get('categories', {}).keys())}")
        
        # åŠ¨æ€å…³é”®è¯å­¦ä¹ ç¼“å­˜
        self.dynamic_keywords = {
            "ğŸª é’©å­": set(),
            "ğŸ¼ äº§å“ä»‹ç»": set(), 
            "ğŸŒŸ ä½¿ç”¨æ•ˆæœ": set(),
            "ğŸ ä¿ƒé”€æœºåˆ¶": set()
        }
    
    def _update_dynamic_keywords(self, segments: List[Dict[str, Any]]):
        """åŸºäºåˆ†æç»“æœåŠ¨æ€æ›´æ–°å…³é”®è¯åº“"""
        for segment in segments:
            category = segment.get('category', '')
            text = segment.get('text', '')
            confidence = segment.get('confidence', 0)
            
            # åªä»é«˜ç½®ä¿¡åº¦ç‰‡æ®µå­¦ä¹ æ–°å…³é”®è¯
            if confidence > 0.85 and category in self.dynamic_keywords:
                # æå–æ–°è¯æ±‡ï¼ˆç®€å•å®ç°ï¼‰
                words = text.split()
                for word in words:
                    # è¿‡æ»¤æ‰å¸¸è§è¯æ±‡ï¼Œåªä¿ç•™æ½œåœ¨çš„ä¸“ä¸šè¯æ±‡
                    if len(word) > 1 and word not in ['çš„', 'æ˜¯', 'æœ‰', 'åœ¨', 'å’Œ', 'æˆ–', 'ä½†', 'éƒ½', 'å°±', 'ä¹Ÿ', 'è¿˜', 'åˆ', 'äº†', 'ç€', 'è¿‡']:
                        self.dynamic_keywords[category].add(word)
    
    def _get_enhanced_keywords(self, category: str) -> List[str]:
        """è·å–å¢å¼ºçš„å…³é”®è¯åˆ—è¡¨ï¼ˆé™æ€+åŠ¨æ€ï¼‰"""
        static_keywords = self.domain_config.get("categories", {}).get(category, {}).get("keywords", [])
        dynamic_keywords = list(self.dynamic_keywords.get(category, set()))
        return static_keywords + dynamic_keywords
    
    def _calculate_category_confidence(self, text: str, category: str) -> float:
        """è®¡ç®—æ–‡æœ¬å±äºç‰¹å®šç±»åˆ«çš„ç½®ä¿¡åº¦"""
        keywords = self._get_enhanced_keywords(category)
        matches = sum(1 for keyword in keywords if keyword in text)
        
        if not keywords:
            return 0.5  # é»˜è®¤ç½®ä¿¡åº¦
            
        # åŸºäºå…³é”®è¯åŒ¹é…ç‡å’Œæƒé‡è®¡ç®—ç½®ä¿¡åº¦
        base_confidence = min(matches / len(keywords) * 2, 1.0)  # æœ€å¤§1.0
        weight_multiplier = self.domain_config.get("categories", {}).get(category, {}).get("weight_multiplier", 1.0)
        
        return min(base_confidence * weight_multiplier, 0.95)  # æœ€å¤§0.95
    
    def _evaluate_generalization(self, analysis_result: Dict[str, Any], full_text: str) -> float:
        """è¯„ä¼°ç³»ç»Ÿå¯¹æ–°å†…å®¹çš„æ³›åŒ–èƒ½åŠ›"""
        if not analysis_result or not analysis_result.get('semantic_segments'):
            return 0.0
        
        segments = analysis_result['semantic_segments']
        total_score = 0.0
        
        # 1. ç½®ä¿¡åº¦è¯„ä¼° (40%)
        avg_confidence = sum(seg.get('confidence', 0) for seg in segments) / len(segments)
        confidence_score = avg_confidence * 0.4
        
        # 2. è¦†ç›–ç‡è¯„ä¼° (30%) - æ£€æŸ¥æ˜¯å¦æœ‰é—æ¼çš„é‡è¦å†…å®¹
        total_chars = len(full_text)
        covered_chars = sum(len(seg.get('text', '')) for seg in segments)
        coverage_score = min(covered_chars / total_chars, 1.0) * 0.3
        
        # 3. ç±»åˆ«åˆ†å¸ƒåˆç†æ€§ (20%) - æ£€æŸ¥ç±»åˆ«åˆ†å¸ƒæ˜¯å¦åˆç†
        category_counts = {}
        for seg in segments:
            category = seg.get('category', '')
            category_counts[category] = category_counts.get(category, 0) + 1
        
        # ç†æƒ³çš„å¯èµ‹å¹¿å‘Šåº”è¯¥æœ‰é’©å­ã€äº§å“ä»‹ç»ã€ä½¿ç”¨æ•ˆæœ
        expected_categories = {"ğŸª é’©å­", "ğŸ¼ äº§å“ä»‹ç»", "ğŸŒŸ ä½¿ç”¨æ•ˆæœ"}
        found_categories = set(category_counts.keys())
        category_completeness = len(expected_categories & found_categories) / len(expected_categories)
        distribution_score = category_completeness * 0.2
        
        # 4. æ–°è¯æ±‡è¯†åˆ«èƒ½åŠ› (10%) - æ£€æŸ¥æ˜¯å¦è¯†åˆ«å‡ºæ–°çš„ä¸“ä¸šè¯æ±‡
        static_keywords = set()
        for category_config in self.domain_config.get("categories", {}).values():
            static_keywords.update(category_config.get("keywords", []))
        
        text_words = set(full_text.split())
        new_words = text_words - static_keywords
        
        # å¦‚æœè¯†åˆ«å‡ºæ–°çš„ä¸“ä¸šè¯æ±‡ï¼Œç»™äºˆå¥–åŠ±
        professional_new_words = 0
        for word in new_words:
            if len(word) > 2 and any(char.isalpha() for char in word):
                professional_new_words += 1
        
        novelty_score = min(professional_new_words / max(len(text_words), 1), 0.1) * 0.1
        
        total_score = confidence_score + coverage_score + distribution_score + novelty_score
        
        return min(total_score, 1.0)
    
    def export_enhanced_config(self, output_path: str = "enhanced_domain_config.json"):
        """å¯¼å‡ºå¢å¼ºåçš„é¢†åŸŸé…ç½®ï¼ˆåŒ…å«å­¦ä¹ åˆ°çš„æ–°å…³é”®è¯ï¼‰"""
        enhanced_config = self.domain_config.copy()
        
        # åˆå¹¶åŠ¨æ€å­¦ä¹ çš„å…³é”®è¯
        for category, static_config in enhanced_config.get("categories", {}).items():
            if category in self.dynamic_keywords:
                dynamic_words = list(self.dynamic_keywords[category])
                if dynamic_words:
                    static_keywords = static_config.get("keywords", [])
                    enhanced_keywords = static_keywords + dynamic_words
                    static_config["keywords"] = list(set(enhanced_keywords))  # å»é‡
                    static_config["dynamic_learned_count"] = len(dynamic_words)
        
        # æ·»åŠ æ³›åŒ–èƒ½åŠ›ç»Ÿè®¡
        enhanced_config["generalization_stats"] = {
            "total_dynamic_keywords": sum(len(words) for words in self.dynamic_keywords.values()),
            "categories_enhanced": [cat for cat, words in self.dynamic_keywords.items() if words],
            "enhancement_timestamp": str(logger.handlers[0].formatter.formatTime(logger.handlers[0], logger.makeRecord("", 0, "", 0, "", (), None)) if logger.handlers else "unknown")
        }
        
        try:
            import json
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(enhanced_config, f, ensure_ascii=False, indent=2)
            logger.info(f"ğŸ“„ å¢å¼ºé…ç½®å·²å¯¼å‡º: {output_path}")
            return True
        except Exception as e:
            logger.error(f"âŒ é…ç½®å¯¼å‡ºå¤±è´¥: {e}")
            return False
    
    def load_enhanced_config(self, config_path: str):
        """åŠ è½½å¢å¼ºåçš„é¢†åŸŸé…ç½®"""
        try:
            import json
            with open(config_path, 'r', encoding='utf-8') as f:
                self.domain_config = json.load(f)
            logger.info(f"ğŸ“„ å¢å¼ºé…ç½®å·²åŠ è½½: {config_path}")
            return True
        except Exception as e:
            logger.error(f"âŒ é…ç½®åŠ è½½å¤±è´¥: {e}")
            return False
    
    def _analyze_with_retry(self, analyze_func, text: str, model_name: str, max_retries: int = 2) -> Optional[Dict[str, Any]]:
        """å¸¦é‡è¯•æœºåˆ¶çš„AIåˆ†ææ–¹æ³•"""
        import time
        
        for attempt in range(max_retries + 1):
            try:
                logger.info(f"ğŸ”„ {model_name} ç¬¬{attempt + 1}æ¬¡å°è¯•...")
                result = analyze_func(text)
                if result:
                    logger.info(f"âœ… {model_name} ç¬¬{attempt + 1}æ¬¡å°è¯•æˆåŠŸ")
                    return result
            except (requests.exceptions.Timeout, requests.exceptions.ConnectionError) as e:
                logger.warning(f"âš ï¸ {model_name} ç¬¬{attempt + 1}æ¬¡ç½‘ç»œè¶…æ—¶: {e}")
                if attempt < max_retries:
                    wait_time = (attempt + 1) * 2  # 2, 4, 6ç§’ç­‰å¾…
                    logger.info(f"â³ ç­‰å¾…{wait_time}ç§’åé‡è¯•...")
                    time.sleep(wait_time)
                else:
                    raise TimeoutError(f"{model_name} APIè¿ç»­{max_retries + 1}æ¬¡è¶…æ—¶")
            except Exception as e:
                logger.error(f"âŒ {model_name} ç¬¬{attempt + 1}æ¬¡åˆ†æé”™è¯¯: {e}")
                if attempt < max_retries:
                    wait_time = (attempt + 1) * 1  # 1, 2, 3ç§’ç­‰å¾…
                    logger.info(f"â³ ç­‰å¾…{wait_time}ç§’åé‡è¯•...")
                    time.sleep(wait_time)
                else:
                    raise
        
        return None
    
    def _get_proxy_config(self) -> Optional[Dict[str, str]]:
        """è·å–ä»£ç†é…ç½®"""
        # ä»ç¯å¢ƒå˜é‡è¯»å–ä»£ç†é…ç½®
        http_proxy = os.getenv('HTTP_PROXY') or os.getenv('http_proxy')
        https_proxy = os.getenv('HTTPS_PROXY') or os.getenv('https_proxy')
        
        if http_proxy or https_proxy:
            proxies = {}
            if http_proxy:
                proxies['http'] = http_proxy
            if https_proxy:
                proxies['https'] = https_proxy
            logger.info(f"ğŸŒ ä½¿ç”¨ä»£ç†é…ç½®: {proxies}")
            return proxies
        
        return None
    
    def _get_default_baby_formula_config(self) -> Dict[str, Any]:
        """è·å–é»˜è®¤çš„æ¯å©´å¥¶ç²‰é¢†åŸŸé…ç½®ï¼ˆå¢å¼ºæ³›åŒ–ç‰ˆï¼‰"""
        return {
            "domain_name": "æ¯å©´å¥¶ç²‰è¥é”€",
            "categories": {
                "ğŸª é’©å­": {
                    "description": "å®å®å“­é—¹ã€å®¶é•¿ç„¦è™‘ã€å–‚å…»å›°æ‰°ã€ä¸“å®¶å»ºè®®ã€é—®é¢˜è§£å†³ã€æ”¹å–„éœ€æ±‚ã€æ‹…å¿ƒé¡¾è™‘ã€å¯»æ±‚å¸®åŠ©ã€ç§‘å­¦å‘ç°ã€ç ”ç©¶ç»“æœ",
                    "keywords": ["å“­é—¹", "å¤œé†’", "ç¡ä¸å¥½", "ä¸å®‰", "å›°æ‰°", "æ‹…å¿ƒ", "ç„¦è™‘", "é—®é¢˜", "è–„å¼±æœŸ", "æ¶ˆè€—å…‰", "å¦ˆå¦ˆä¸çŸ¥é“", "90%", "ä½ å®¶å®å®", "é¢‘ç¹", "åŸæ¥", "ç ”ç©¶å‘ç°", "ä¸“å®¶è¯´", "ç§‘å­¦è¯æ˜"],
                    "weight_multiplier": 1.0
                },
                "ğŸ¼ äº§å“ä»‹ç»": {
                    "description": "äº§å“å±•ç¤ºã€åŒ…è£…ç‰¹å†™ã€æˆåˆ†ä»‹ç»ã€å†²æ³¡æ¼”ç¤ºã€å“ç‰Œæ ‡è¯†ã€è¥å…»é…æ–¹ã€ä¸“ä¸šæ¨èã€è´¨é‡è®¤è¯ã€ç§‘æŠ€åˆ›æ–°",
                    "keywords": ["æƒ æ°", "å¯èµ‹", "è•´é†‡", "è•´æ·³", "æ°´å¥¶", "æœ‰æœº", "å¥¶ç²‰", "é…æ–¹", "è¥å…»", "æˆåˆ†", "æ¯ä¹³", "ä½èšç³–", "è›‹ç™½", "A2", "å¥¶æº", "HMO", "OPN", "2'-FL", "Î±-ä¹³æ¸…è›‹ç™½", "DHA", "ARA", "ç›Šç”ŸèŒ", "æ ¸è‹·é…¸", "å¶é»„ç´ ", "èƒ†ç¢±", "ç‰›ç£ºé…¸", "é“", "é”Œ", "é’™", "ç»´ç”Ÿç´ ", "äº²å’Œ", "å¤©ç„¶", "çº¯å‡€", "åŸè£…è¿›å£", "ç‘å£«", "çˆ±å°”å…°"],
                    "weight_multiplier": 1.2
                },
                "ğŸŒŸ ä½¿ç”¨æ•ˆæœ": {
                    "description": "å®å®æ´»æ³¼ã€æ•ˆæœå±•ç¤ºã€æ»¡æ„åé¦ˆã€å¥åº·å‘è‚²ã€å¿«ä¹ç©è€ã€æˆé•¿å¯¹æ¯”ã€è¥å…»æ•ˆæœã€å®¶é•¿å¤¸èµã€æ™ºåŠ›å‘å±•ã€ä½“æ ¼å‘è‚²",
                    "keywords": ["è‡ªæ„ˆåŠ›", "å¼€æŒ‚", "å¹³ç¨³", "åº¦è¿‡", "æ›´å¥½", "å¸æ”¶", "å¥åº·", "æ´»æ³¼", "æ•ˆæœ", "æ”¹å–„", "ç›´æ¥", "èƒ½è®©", "å¥½å¸æ”¶", "å¹³ç¨³åº¦è¿‡", "ä¿æŠ¤åŠ›", "æŠµæŠ—åŠ›", "å…ç–«åŠ›", "èªæ˜", "æ™ºåŠ›", "å‘è‚²", "æˆé•¿", "å¼ºå£®", "æ¶ˆåŒ–å¥½", "ä¸ä¸Šç«", "ä¸ä¾¿ç§˜", "ç¡å¾—é¦™", "é•¿å¾—å¿«", "èªæ˜ä¼¶ä¿"],
                    "weight_multiplier": 1.1
                },
                "ğŸ ä¿ƒé”€æœºåˆ¶": {
                    "description": "ä¼˜æƒ æ´»åŠ¨ã€é™æ—¶æŠ˜æ‰£ã€ç¤¼å“èµ é€ã€è¯•ç”¨è£…ã€ä¼šå‘˜ç¦åˆ©ã€è´­ä¹°æ¸ é“ã€å®¢æœå’¨è¯¢ã€å”®åä¿éšœ",
                    "keywords": ["é™æ—¶", "ä¸“äº«", "è¯•å–", "å¤§ä¿ƒ", "å›¤", "ä¼˜æƒ ", "æ´»åŠ¨", "ç¦åˆ©", "æŠ˜æ‰£", "æ»¡å‡", "ä¹°èµ ", "ç¤¼ç›’", "è¯•ç”¨è£…", "æ–°å®¢", "ä¼šå‘˜", "ç§¯åˆ†", "è¿”ç°", "åŒ…é‚®", "å®¢æœ", "å’¨è¯¢", "ä¿éšœ", "æ­£å“", "æˆæƒ", "å®˜æ–¹"],
                    "weight_multiplier": 1.0
                }
            },
            "semantic_boundaries": [
                # é’©å­ â†’ äº§å“ä»‹ç»
                {
                    "pattern": r"(å›°æ‰°|é—®é¢˜|è–„å¼±æœŸ|ç„¦è™‘|æ‹…å¿ƒ|ç ”ç©¶|ä¸“å®¶).{0,20}(æƒ æ°|å¯èµ‹|é…æ–¹|å¥¶ç²‰|é€‰å¥½|è•´é†‡|æ°´å¥¶|æœ‰æœº)",
                    "from_category": "ğŸª é’©å­",
                    "to_category": "ğŸ¼ äº§å“ä»‹ç»",
                    "description": "é’©å­è½¬å‘äº§å“ä»‹ç»"
                },
                # äº§å“ä»‹ç» â†’ ä½¿ç”¨æ•ˆæœ
                {
                    "pattern": r"(é…æ–¹|æˆåˆ†|è¥å…»|æ¯ä¹³|ä½èšç³–|è›‹ç™½|å¥¶æº|HMO|OPN|DHA|ç»´ç”Ÿç´ ).{0,20}(è‡ªæ„ˆåŠ›|å¼€æŒ‚|æ•ˆæœ|æ”¹å–„|å¹³ç¨³|åº¦è¿‡|å¸æ”¶|å¥åº·|å‘è‚²|æˆé•¿|èªæ˜|å¼ºå£®)",
                    "from_category": "ğŸ¼ äº§å“ä»‹ç»", 
                    "to_category": "ğŸŒŸ ä½¿ç”¨æ•ˆæœ",
                    "description": "äº§å“ä»‹ç»è½¬å‘ä½¿ç”¨æ•ˆæœ"
                },
                # ä½¿ç”¨æ•ˆæœ â†’ äº§å“ä»‹ç»
                {
                    "pattern": r"(è‡ªæ„ˆåŠ›|å¼€æŒ‚|æ•ˆæœ|æ”¹å–„|å¹³ç¨³|åº¦è¿‡|å¥åº·|èªæ˜|å‘è‚²).{0,20}(å†åŠ ä¸Š|å¥¶æº|è¥å…»|æˆåˆ†|A2|é…æ–¹|æƒ æ°|å¯èµ‹|è•´é†‡|æ°´å¥¶)",
                    "from_category": "ğŸŒŸ ä½¿ç”¨æ•ˆæœ",
                    "to_category": "ğŸ¼ äº§å“ä»‹ç»",
                    "description": "ä½¿ç”¨æ•ˆæœè½¬å›äº§å“ä»‹ç»"
                },
                # ä½¿ç”¨æ•ˆæœ â†’ ä¿ƒé”€æœºåˆ¶
                {
                    "pattern": r"(æ•ˆæœ|æ”¹å–„|å¥åº·|è¥å…»|å‘è‚²|æˆé•¿|èªæ˜).{0,20}(é™æ—¶|ä¸“äº«|è¯•å–|å¤§ä¿ƒ|å›¤|ç°åœ¨|ä¼˜æƒ |æ´»åŠ¨|æŠ˜æ‰£|æ–°å®¢)",
                    "from_category": "ğŸŒŸ ä½¿ç”¨æ•ˆæœ",
                    "to_category": "ğŸ ä¿ƒé”€æœºåˆ¶",
                    "description": "ä½¿ç”¨æ•ˆæœè½¬å‘ä¿ƒé”€æœºåˆ¶"
                },
                # äº§å“ä»‹ç» â†’ ä¿ƒé”€æœºåˆ¶
                {
                    "pattern": r"(å¥¶æº|è¥å…»|å¸æ”¶|é…æ–¹|æˆåˆ†|æƒ æ°|å¯èµ‹).{0,20}(é™æ—¶|ä¸“äº«|è¯•å–|å¤§ä¿ƒ|å›¤|ç°åœ¨|ä¼˜æƒ |æ´»åŠ¨|æŠ˜æ‰£|æ–°å®¢|ä»·æ ¼|è´­ä¹°)",
                    "from_category": "ğŸ¼ äº§å“ä»‹ç»",
                    "to_category": "ğŸ ä¿ƒé”€æœºåˆ¶",
                    "description": "äº§å“ä»‹ç»è½¬å‘ä¿ƒé”€æœºåˆ¶"
                },
                # é’©å­ â†’ ä½¿ç”¨æ•ˆæœï¼ˆç›´æ¥ï¼‰
                {
                    "pattern": r"(é—®é¢˜|å›°æ‰°|è–„å¼±æœŸ|ä¸å®‰).{0,20}(è‡ªæ„ˆåŠ›|å¼€æŒ‚|æ•ˆæœ|æ”¹å–„|å¹³ç¨³|åº¦è¿‡|å¥åº·|å‘è‚²)",
                    "from_category": "ğŸª é’©å­",
                    "to_category": "ğŸŒŸ ä½¿ç”¨æ•ˆæœ",
                    "description": "é’©å­ç›´æ¥è½¬å‘ä½¿ç”¨æ•ˆæœ"
                }
            ]
        }

    def analyze_srt_with_word_timestamps(self, srt_path: str, transcription_result: Dict[str, Any] = None) -> List[SemanticSegment]:
        """
        åˆ†æSRTæ–‡ä»¶ï¼Œæå–è¯çº§æ—¶é—´æˆ³å¹¶è¿›è¡Œè¯­ä¹‰åˆ†å‰²
        
        Args:
            srt_path: SRTæ–‡ä»¶è·¯å¾„
            transcription_result: è½¬å½•ç»“æœï¼ˆåŒ…å«è¯çº§æ—¶é—´æˆ³ï¼‰
            
        Returns:
            List[SemanticSegment]: è¯­ä¹‰åˆ†å‰²åçš„ç‰‡æ®µåˆ—è¡¨
        """
        try:
            # 1. æå–è¯çº§æ—¶é—´æˆ³
            word_timestamps = self._extract_word_timestamps(transcription_result)
            if not word_timestamps:
                raise ValueError("âŒ æœªæ‰¾åˆ°è¯çº§æ—¶é—´æˆ³ï¼Œæ— æ³•è¿›è¡Œè¯­ä¹‰åˆ†æ")
            
            logger.info(f"ğŸ“Š æå–åˆ° {len(word_timestamps)} ä¸ªè¯çº§æ—¶é—´æˆ³")
            
            # 2. åˆå¹¶å®Œæ•´æ–‡æœ¬
            full_text = ''.join([word.text for word in word_timestamps])
            
            # 3. AIè¯­ä¹‰åˆ†æï¼ˆä¸¥æ ¼æ¨¡å¼ï¼šå¤±è´¥å³æŠ¥é”™ï¼‰
            analysis_result = None
            
            # å°è¯•DeepSeekåˆ†æï¼ˆå¸¦é‡è¯•ï¼‰
            try:
                analysis_result = self._analyze_with_retry(self._analyze_with_deepseek, full_text, "DeepSeek")
                logger.info("âœ… ä½¿ç”¨DeepSeekåˆ†æç»“æœ")
            except Exception as e:
                logger.warning(f"âš ï¸ DeepSeekåˆ†æå¤±è´¥: {e}")
                
                # å¦‚æœDeepSeekå¤±è´¥ï¼Œå°è¯•Claudeï¼ˆå¦‚æœå¯ç”¨ï¼‰
                if self.claude_api_key:
                    try:
                        analysis_result = self._analyze_with_retry(self._analyze_with_claude, full_text, "Claude")
                        logger.info("âœ… ä½¿ç”¨Claudeåˆ†æç»“æœ")
                    except Exception as claude_e:
                        logger.error(f"âŒ Claudeåˆ†æä¹Ÿå¤±è´¥: {claude_e}")
                        raise RuntimeError(f"æ‰€æœ‰AIåˆ†ææ–¹æ³•éƒ½å¤±è´¥ã€‚DeepSeek: {e}, Claude: {claude_e}")
                else:
                    raise RuntimeError(f"DeepSeekåˆ†æå¤±è´¥ä¸”æœªé…ç½®Claude: {e}")
            
            if not analysis_result:
                raise RuntimeError("âŒ æœªè·å¾—æœ‰æ•ˆçš„AIåˆ†æç»“æœ")
            
            # 4. åŸºäºè¯­ä¹‰åˆ†æç»“æœè¿›è¡Œè¯çº§åˆ‡åˆ†
            semantic_segments = self._create_semantic_segments(word_timestamps, analysis_result)
            
            # 5. åŠ¨æ€å­¦ä¹ æ–°å…³é”®è¯ï¼ˆæé«˜æ³›åŒ–èƒ½åŠ›ï¼‰
            if analysis_result and analysis_result.get('semantic_segments'):
                self._update_dynamic_keywords(analysis_result['semantic_segments'])
                logger.info(f"ğŸ§  åŠ¨æ€å­¦ä¹ å®Œæˆï¼Œæ›´æ–°å…³é”®è¯åº“")
            
            # 6. è¯„ä¼°æ³›åŒ–èƒ½åŠ›
            generalization_score = self._evaluate_generalization(analysis_result, full_text)
            logger.info(f"ğŸ“Š æ³›åŒ–èƒ½åŠ›è¯„åˆ†: {generalization_score:.2f}/1.0")
            
            logger.info(f"âœ… è¯­ä¹‰åˆ†å‰²å®Œæˆï¼Œç”Ÿæˆ {len(semantic_segments)} ä¸ªè¯­ä¹‰ç‰‡æ®µ")
            
            return semantic_segments
            
        except Exception as e:
            logger.error(f"âŒ è¯çº§è¯­ä¹‰åˆ†æå¤±è´¥: {e}")
            return []
    
    def _extract_word_timestamps(self, transcription_result: Dict[str, Any]) -> List[WordTimestamp]:
        """ä»è½¬å½•ç»“æœä¸­æå–è¯çº§æ—¶é—´æˆ³"""
        word_timestamps = []
        
        try:
            logger.info("ğŸ” å¼€å§‹æå–è¯çº§æ—¶é—´æˆ³...")
            
            # æ£€æŸ¥æ˜¯å¦æœ‰åŸå§‹è½¬å½•æ•°æ®
            raw_output = transcription_result.get('raw_output', {})
            if not raw_output:
                logger.warning("âš ï¸ è½¬å½•ç»“æœä¸­æ²¡æœ‰raw_outputæ•°æ®")
                return self._extract_from_segments(transcription_result)
            
            # æ–¹æ³•1: ä»resultsä¸­çš„transcription_urlè·å–
            results = raw_output.get('results', [])
            for result in results:
                transcription_url = result.get('transcription_url')
                if transcription_url:
                    logger.info(f"ğŸ“¥ ä¸‹è½½è¯çº§è½¬å½•è¯¦æƒ…: {transcription_url}")
                    detailed_result = self._download_transcription_details(transcription_url)
                    if detailed_result:
                        words = self._parse_word_timestamps_from_result(detailed_result)
                        word_timestamps.extend(words)
                        
            if word_timestamps:
                logger.info(f"âœ… æ–¹æ³•1æˆåŠŸ: æå–åˆ° {len(word_timestamps)} ä¸ªè¯çº§æ—¶é—´æˆ³")
                return word_timestamps
            
            # æ–¹æ³•2: ä»segmentsä¸­æå–ï¼ˆå¤‡ç”¨æ–¹æ¡ˆï¼‰
            logger.info("ğŸ”„ å°è¯•ä»segmentsæå–è¯çº§ä¿¡æ¯...")
            segments = transcription_result.get('segments', [])
            
            for segment in segments:
                text = segment.get('text', '')
                start_time = segment.get('start_time', 0)
                end_time = segment.get('end_time', 0)
                
                # ä¼°ç®—æ¯ä¸ªå­—ç¬¦çš„æ—¶é—´æˆ³
                if text and end_time > start_time:
                    duration = end_time - start_time
                    char_duration = duration / len(text)
                    
                    for i, char in enumerate(text):
                        char_start = start_time + i * char_duration
                        char_end = char_start + char_duration
                        
                        word_timestamps.append(WordTimestamp(
                            text=char,
                            start_time=char_start,
                            end_time=char_end,
                            confidence=0.8  # ä¼°ç®—ç½®ä¿¡åº¦
                        ))
            
            if word_timestamps:
                logger.info(f"âœ… æ–¹æ³•2æˆåŠŸ: ç”Ÿæˆ {len(word_timestamps)} ä¸ªå­—ç¬¦çº§æ—¶é—´æˆ³")
                return word_timestamps
            
            logger.warning("âš ï¸ æ— æ³•æå–è¯çº§æ—¶é—´æˆ³")
            return []
            
        except Exception as e:
            logger.error(f"âŒ æå–è¯çº§æ—¶é—´æˆ³å¤±è´¥: {e}")
            return []
    
    def _extract_from_segments(self, transcription_result: Dict[str, Any]) -> List[WordTimestamp]:
        """ä»segmentsä¸­æå–è¯çº§æ—¶é—´æˆ³ï¼ˆå¤‡ç”¨æ–¹æ¡ˆï¼‰"""
        word_timestamps = []
        
        try:
            segments = transcription_result.get('segments', [])
            logger.info(f"ğŸ“Š å¤„ç† {len(segments)} ä¸ªæ®µè½...")
            
            for segment in segments:
                text = segment.get('text', '')
                start_time = segment.get('start_time', 0)
                end_time = segment.get('end_time', 0)
                
                if not text or end_time <= start_time:
                    continue
                
                # æŒ‰è¯æ±‡åˆ†å‰²ï¼ˆç®€å•çš„ä¸­æ–‡åˆ†è¯ï¼‰
                words = self._simple_word_split(text)
                if not words:
                    continue
                
                # è®¡ç®—æ¯ä¸ªè¯çš„æ—¶é—´æˆ³
                duration = end_time - start_time
                word_duration = duration / len(words)
                
                for i, word in enumerate(words):
                    word_start = start_time + i * word_duration
                    word_end = word_start + word_duration
                    
                    word_timestamps.append(WordTimestamp(
                        text=word,
                        start_time=word_start,
                        end_time=word_end,
                        confidence=0.7
                    ))
            
            return word_timestamps
            
        except Exception as e:
            logger.error(f"âŒ ä»segmentsæå–å¤±è´¥: {e}")
            return []
    
    def _simple_word_split(self, text: str) -> List[str]:
        """ç®€å•çš„ä¸­æ–‡åˆ†è¯"""
        import re
        
        # æŒ‰æ ‡ç‚¹ç¬¦å·å’Œç©ºæ ¼åˆ†å‰²
        words = re.findall(r'[^ï¼Œã€‚ï¼ï¼Ÿï¼›ï¼šã€\s]+', text)
        
        # è¿›ä¸€æ­¥åˆ†å‰²é•¿è¯
        result = []
        for word in words:
            if len(word) > 4:
                # é•¿è¯æŒ‰2-3å­—ç¬¦åˆ†å‰²
                for i in range(0, len(word), 2):
                    result.append(word[i:i+2])
            else:
                result.append(word)
        
        return result
    
    def _download_transcription_details(self, transcription_url: str) -> Optional[Dict[str, Any]]:
        """ä¸‹è½½è¯¦ç»†çš„è½¬å½•ç»“æœ"""
        try:
            response = requests.get(transcription_url, timeout=30)
            response.raise_for_status()
            return response.json()
        except Exception as e:
            logger.error(f"âŒ ä¸‹è½½è½¬å½•è¯¦æƒ…å¤±è´¥: {e}")
            return None
    
    def _parse_word_timestamps_from_result(self, transcription_data: Dict[str, Any]) -> List[WordTimestamp]:
        """ä»è½¬å½•æ•°æ®ä¸­è§£æè¯çº§æ—¶é—´æˆ³"""
        word_timestamps = []
        
        try:
            logger.info("ğŸ“Š å¼€å§‹è§£æè¯çº§æ—¶é—´æˆ³æ•°æ®...")
            logger.info(f"ğŸ” è½¬å½•æ•°æ®é”®: {list(transcription_data.keys())}")
            
            # æ–¹æ³•1: æŸ¥æ‰¾transcripts -> sentences -> wordsç»“æ„ (DashScopeæ ‡å‡†æ ¼å¼)
            transcripts = transcription_data.get('transcripts', [])
            if transcripts:
                logger.info(f"ğŸ“ æ‰¾åˆ° {len(transcripts)} ä¸ªè½¬å½•ï¼Œè§£æè¯çº§ä¿¡æ¯...")
                for transcript in transcripts:
                    # è·å–å¥å­çº§æ•°æ®
                    sentences = transcript.get('sentences', [])
                    if sentences:
                        logger.info(f"ğŸ“ è½¬å½•ä¸­åŒ…å« {len(sentences)} ä¸ªå¥å­")
                        for sentence in sentences:
                            words = sentence.get('words', [])
                            logger.info(f"ğŸ“ å¥å­åŒ…å« {len(words)} ä¸ªè¯")
                            for word_data in words:
                                word_timestamps.append(WordTimestamp(
                                    text=word_data.get('text', ''),
                                    start_time=word_data.get('begin_time', 0) / 1000,  # è½¬æ¢ä¸ºç§’
                                    end_time=word_data.get('end_time', 0) / 1000,
                                    confidence=word_data.get('confidence', 1.0)
                                ))
                    
                    # å¦‚æœsentencesä¸ºç©ºï¼Œå°è¯•ç›´æ¥ä»transcriptè·å–words
                    if not sentences:
                        words = transcript.get('words', [])
                        if words:
                            logger.info(f"ğŸ“ è½¬å½•ç›´æ¥åŒ…å« {len(words)} ä¸ªè¯")
                            for word_data in words:
                                word_timestamps.append(WordTimestamp(
                                    text=word_data.get('text', ''),
                                    start_time=word_data.get('begin_time', 0) / 1000,
                                    end_time=word_data.get('end_time', 0) / 1000,
                                    confidence=word_data.get('confidence', 1.0)
                                ))
                
                if word_timestamps:
                    logger.info(f"âœ… transcriptsæ–¹æ³•æˆåŠŸ: æå–åˆ° {len(word_timestamps)} ä¸ªè¯")
                    return word_timestamps
            
            # æ–¹æ³•2: æŸ¥æ‰¾sentences -> wordsç»“æ„
            sentences = transcription_data.get('sentences', [])
            if sentences:
                logger.info(f"ğŸ“ æ‰¾åˆ° {len(sentences)} ä¸ªå¥å­ï¼Œè§£æè¯çº§ä¿¡æ¯...")
                for sentence in sentences:
                    words = sentence.get('words', [])
                    for word_data in words:
                        word_timestamps.append(WordTimestamp(
                            text=word_data.get('text', ''),
                            start_time=word_data.get('begin_time', 0) / 1000,  # è½¬æ¢ä¸ºç§’
                            end_time=word_data.get('end_time', 0) / 1000,
                            confidence=word_data.get('confidence', 1.0)
                        ))
                if word_timestamps:
                    logger.info(f"âœ… sentencesæ–¹æ³•æˆåŠŸ: æå–åˆ° {len(word_timestamps)} ä¸ªè¯")
                    return word_timestamps
            
            # æ–¹æ³•3: ç›´æ¥æŸ¥æ‰¾wordsæ•°ç»„
            words = transcription_data.get('words', [])
            if words:
                logger.info(f"ğŸ“ æ‰¾åˆ°ç›´æ¥wordsæ•°ç»„ï¼ŒåŒ…å« {len(words)} ä¸ªè¯...")
                for word_data in words:
                    word_timestamps.append(WordTimestamp(
                        text=word_data.get('text', ''),
                        start_time=word_data.get('begin_time', 0) / 1000,
                        end_time=word_data.get('end_time', 0) / 1000,
                        confidence=word_data.get('confidence', 1.0)
                    ))
                if word_timestamps:
                    logger.info(f"âœ… ç›´æ¥wordsæ–¹æ³•æˆåŠŸ: æå–åˆ° {len(word_timestamps)} ä¸ªè¯")
                    return word_timestamps
            
            # å¦‚æœéƒ½å¤±è´¥äº†ï¼Œæ‰“å°è°ƒè¯•ä¿¡æ¯
            logger.warning("âš ï¸ æœªæ‰¾åˆ°æ ‡å‡†çš„è¯çº§æ—¶é—´æˆ³ç»“æ„")
            logger.info(f"ğŸ” è½¬å½•æ•°æ®æ ·æœ¬: {str(transcription_data)[:500]}...")
            
            return []
            
        except Exception as e:
            logger.error(f"âŒ è§£æè¯çº§æ—¶é—´æˆ³å¤±è´¥: {e}")
            import traceback
            logger.error(traceback.format_exc())
            return []
    
    def _analyze_with_deepseek(self, text: str) -> Optional[Dict[str, Any]]:
        """ä½¿ç”¨DeepSeekè¿›è¡Œè¯­ä¹‰åˆ†æ"""
        if not self.deepseek_api_key or self.deepseek_api_key in ['your_deepseek_api_key_here', 'placeholder']:
            raise ValueError("âŒ DeepSeek APIå¯†é’¥æœªé…ç½®ï¼Œæ— æ³•è¿›è¡Œè¯­ä¹‰åˆ†æ")
        
        try:
            # ğŸ¯ ä½¿ç”¨é¢†åŸŸé…ç½®ç”Ÿæˆæç¤ºè¯
            prompt = self._generate_domain_analysis_prompt(text)
            
            # DeepSeek APIè°ƒç”¨ - ä¿®å¤URL
            headers = {
                'Authorization': f'Bearer {self.deepseek_api_key}',
                'Content-Type': 'application/json'
            }
            
            payload = {
                'model': 'deepseek-chat',
                'messages': [
                    {'role': 'system', 'content': f'ä½ æ˜¯ä¸“ä¸šçš„{self.domain_config["domain_name"]}å†…å®¹è¯­ä¹‰åˆ†æä¸“å®¶ã€‚è¯·ä¸¥æ ¼æŒ‰ç…§JSONæ ¼å¼è¿”å›åˆ†æç»“æœã€‚'},
                    {'role': 'user', 'content': prompt}
                ],
                'temperature': 0.1,
                'max_tokens': 2000
            }
            
            logger.info("ğŸš€ è°ƒç”¨DeepSeek APIè¿›è¡Œè¯­ä¹‰åˆ†æ...")
            
            # å¤„ç†ä»£ç†é…ç½®
            proxies = self._get_proxy_config()
            
            response = requests.post(
                'https://api.deepseek.com/chat/completions',
                headers=headers,
                json=payload,
                timeout=60,  # å¢åŠ åˆ°60ç§’
                proxies=proxies
            )
            
            if response.status_code == 200:
                result = response.json()
                content = result['choices'][0]['message']['content']
                
                # è§£æJSONç»“æœ - æ”¯æŒmarkdownä»£ç å—
                try:
                    analysis_result = json.loads(content)
                    analysis_result['model'] = 'deepseek-chat'
                    analysis_result['raw_response'] = content
                    logger.info("âœ… DeepSeekåˆ†ææˆåŠŸ")
                    return analysis_result
                except json.JSONDecodeError as e:
                    logger.error(f"âŒ DeepSeekè¿”å›å†…å®¹ä¸æ˜¯æœ‰æ•ˆJSON: {e}")
                    logger.error(f"ğŸ“„ åŸå§‹å“åº”: {content}")
                    
                    # ğŸ§  å°è¯•æ™ºèƒ½æå–markdownä»£ç å—ä¸­çš„JSON
                    try:
                        analysis_result = self._extract_analysis_from_text(content, 'DeepSeek')
                        logger.info("âœ… DeepSeekæ™ºèƒ½æå–æˆåŠŸ")
                        return analysis_result
                    except Exception as extract_e:
                        logger.error(f"âŒ DeepSeekæ™ºèƒ½æå–ä¹Ÿå¤±è´¥: {extract_e}")
                        raise ValueError(f"DeepSeek APIè¿”å›æ ¼å¼é”™è¯¯: {e}")
            else:
                logger.error(f"âŒ DeepSeek APIè°ƒç”¨å¤±è´¥: {response.status_code} - {response.text}")
                raise RuntimeError(f"DeepSeek APIè°ƒç”¨å¤±è´¥: {response.status_code}")
                
        except requests.exceptions.Timeout:
            logger.error("âŒ DeepSeek APIè¯·æ±‚è¶…æ—¶")
            raise TimeoutError("DeepSeek APIè¯·æ±‚è¶…æ—¶")
        except Exception as e:
            logger.error(f"âŒ DeepSeekè¯­ä¹‰åˆ†æå¤±è´¥: {e}")
            raise
    
    def _analyze_with_claude(self, text: str) -> Optional[Dict[str, Any]]:
        """ä½¿ç”¨Claudeè¿›è¡Œè¯­ä¹‰åˆ†æ"""
        if not self.claude_api_key or self.claude_api_key in ['your_openrouter_api_key_here', 'placeholder']:
            raise ValueError("âŒ Claude APIå¯†é’¥æœªé…ç½®ï¼Œæ— æ³•è¿›è¡Œè¯­ä¹‰åˆ†æ")
        
        try:
            # ğŸ¯ ä½¿ç”¨é¢†åŸŸé…ç½®ç”Ÿæˆæç¤ºè¯
            prompt = self._generate_domain_analysis_prompt(text)
            
            # Claude APIè°ƒç”¨ (é€šè¿‡OpenRouter)
            headers = {
                'Authorization': f'Bearer {self.claude_api_key}',
                'Content-Type': 'application/json'
            }
            
            payload = {
                'model': 'anthropic/claude-4-sonnet-20250522',
                'messages': [
                    {'role': 'system', 'content': f'ä½ æ˜¯ä¸“ä¸šçš„{self.domain_config["domain_name"]}å†…å®¹è¯­ä¹‰åˆ†æä¸“å®¶ã€‚è¯·ä¸¥æ ¼æŒ‰ç…§JSONæ ¼å¼è¿”å›åˆ†æç»“æœã€‚'},
                    {'role': 'user', 'content': prompt}
                ],
                'temperature': 0.1,
                'max_tokens': 2000
            }
            
            logger.info("ğŸš€ è°ƒç”¨Claude APIè¿›è¡Œè¯­ä¹‰åˆ†æ...")
            
            # å¤„ç†ä»£ç†é…ç½®
            proxies = self._get_proxy_config()
            
            response = requests.post(
                'https://openrouter.ai/api/v1/chat/completions',
                headers=headers,
                json=payload,
                timeout=60,  # å¢åŠ åˆ°60ç§’
                proxies=proxies
            )
            
            if response.status_code == 200:
                result = response.json()
                content = result['choices'][0]['message']['content']
                
                # è§£æJSONç»“æœ - æ”¯æŒmarkdownä»£ç å—
                try:
                    analysis_result = json.loads(content)
                    analysis_result['model'] = 'claude-4-sonnet'
                    analysis_result['raw_response'] = content
                    logger.info("âœ… Claudeåˆ†ææˆåŠŸ")
                    return analysis_result
                except json.JSONDecodeError as e:
                    logger.error(f"âŒ Claudeè¿”å›å†…å®¹ä¸æ˜¯æœ‰æ•ˆJSON: {e}")
                    logger.error(f"ğŸ“„ åŸå§‹å“åº”: {content}")
                    
                    # ğŸ§  å°è¯•æ™ºèƒ½æå–markdownä»£ç å—ä¸­çš„JSON
                    try:
                        analysis_result = self._extract_analysis_from_text(content, 'Claude')
                        logger.info("âœ… Claudeæ™ºèƒ½æå–æˆåŠŸ")
                        return analysis_result
                    except Exception as extract_e:
                        logger.error(f"âŒ Claudeæ™ºèƒ½æå–ä¹Ÿå¤±è´¥: {extract_e}")
                        raise ValueError(f"Claude APIè¿”å›æ ¼å¼é”™è¯¯: {e}")
            else:
                logger.error(f"âŒ Claude APIè°ƒç”¨å¤±è´¥: {response.status_code} - {response.text}")
                raise RuntimeError(f"Claude APIè°ƒç”¨å¤±è´¥: {response.status_code}")
                
        except requests.exceptions.Timeout:
            logger.error("âŒ Claude APIè¯·æ±‚è¶…æ—¶")
            raise TimeoutError("Claude APIè¯·æ±‚è¶…æ—¶")
        except Exception as e:
            logger.error(f"âŒ Claudeè¯­ä¹‰åˆ†æå¤±è´¥: {e}")
            raise
    
    def _generate_domain_analysis_prompt(self, text: str) -> str:
        """åŸºäºé¢†åŸŸé…ç½®ç”Ÿæˆåˆ†ææç¤ºè¯"""
        domain_name = self.domain_config["domain_name"]
        categories = self.domain_config["categories"]
        
        # æ„å»ºç±»åˆ«è¯´æ˜
        category_descriptions = []
        for category, config in categories.items():
            desc = config["description"]
            keywords = ", ".join(config["keywords"][:8])  # åªæ˜¾ç¤ºå‰8ä¸ªå…³é”®è¯
            category_descriptions.append(f"**{category}**: {desc}\n   å…³é”®è¯ç¤ºä¾‹: {keywords}")
        
        categories_text = "\n".join(category_descriptions)
        
        # æ„å»ºç±»åˆ«åˆ†å¸ƒç¤ºä¾‹
        category_names = list(categories.keys())
        category_dist_example = ', '.join([f'"{cat}": æ•°å­—' for cat in category_names])
        
        return f"""ğŸ¯ è¯·å¯¹ä»¥ä¸‹{domain_name}å­—å¹•æ–‡æœ¬è¿›è¡Œ**è¶…ç²¾ç»†è¯­ä¹‰åˆ†å‰²åˆ†æ**ï¼ˆé€‚é…æ‰€æœ‰å¯èµ‹äº§å“çº¿ï¼‰ã€‚

## ğŸ“‹ {domain_name}ç±»åˆ«ä½“ç³»

{categories_text}

## ğŸ¯ æ ¸å¿ƒåˆ†æè¦æ±‚ï¼ˆæ³›åŒ–ç‰ˆï¼‰

1. **å¥å­å†…éƒ¨è¯­ä¹‰åˆ‡æ¢æ£€æµ‹**: 
   - ä»”ç»†åˆ†ææ¯ä¸ªå¥å­å†…éƒ¨æ˜¯å¦åŒ…å«å¤šä¸ªè¯­ä¹‰æ¨¡å—
   - è¯†åˆ«è¯­ä¹‰è½¬æ¢ç‚¹ï¼Œå¦‚"äº§å“ä»‹ç»â†’ä½¿ç”¨æ•ˆæœ"ã€"ä½¿ç”¨æ•ˆæœâ†’äº§å“ä»‹ç»"ç­‰
   - ä¸è¦ç®€å•æŒ‰å¥å­åˆ†å‰²ï¼Œè¦æŒ‰è¯­ä¹‰å†…å®¹åˆ†å‰²
   - **é€‚åº”æ€§åˆ†æ**: å³ä½¿é‡åˆ°æ–°çš„è¯æ±‡è¡¨è¾¾ï¼Œä¹Ÿè¦åŸºäºè¯­ä¹‰é€»è¾‘è¿›è¡Œåˆ†ç±»

2. **å…³é”®è½¬æ¢ä¿¡å·è¯†åˆ«**ï¼ˆé€šç”¨æ¨¡å¼ï¼‰:
   - ğŸ¼â†’ğŸŒŸ: "äº§å“/æˆåˆ†/è¥å…»/é…æ–¹" + "æ•ˆæœ/å‘è‚²/å¥åº·/èƒ½åŠ›æå‡"
   - ğŸŒŸâ†’ğŸ¼: "æ•ˆæœ/å‘è‚²/å¥åº·" + "æˆåˆ†/è¥å…»/äº§å“ç‰¹ç‚¹"  
   - ğŸªâ†’ğŸ¼: "é—®é¢˜/å›°æ‰°/éœ€æ±‚" + "å“ç‰Œ/äº§å“/è§£å†³æ–¹æ¡ˆ"
   - ğŸŒŸâ†’ğŸ: "æ•ˆæœ/æ»¡æ„" + "ä¼˜æƒ /æ´»åŠ¨/è´­ä¹°"
   - ğŸ¼â†’ğŸ: "äº§å“ä»‹ç»" + "ä»·æ ¼/ä¼˜æƒ /è´­ä¹°"

3. **ç²¾ç»†è¾¹ç•Œå®šä½**: 
   - æ¯ä¸ªç‰‡æ®µåº”è¯¥è¯­ä¹‰çº¯å‡€ï¼Œå±äºå•ä¸€ç±»åˆ«
   - å¦‚æœä¸€å¥è¯åŒ…å«å¤šä¸ªè¯­ä¹‰ï¼Œå¿…é¡»æ‹†åˆ†æˆå¤šä¸ªç‰‡æ®µ
   - æä¾›ç²¾ç¡®çš„è¯ç´¢å¼•è¾¹ç•Œ
   - **æ–°å†…å®¹é€‚åº”**: å¯¹äºæœªçŸ¥è¯æ±‡ï¼ŒåŸºäºä¸Šä¸‹æ–‡è¯­ä¹‰è¿›è¡Œåˆ†ç±»

4. **å¯èµ‹äº§å“çº¿è‡ªé€‚åº”**:
   - å¯èµ‹è•´é†‡: é‡ç‚¹å…³æ³¨HMOã€OPNã€Î±-ä¹³æ¸…è›‹ç™½ç­‰é«˜ç«¯æˆåˆ†
   - å¯èµ‹æ°´å¥¶: é‡ç‚¹å…³æ³¨ä¾¿æºã€æ–°é²œã€å³å¼€å³é¥®ç­‰ç‰¹ç‚¹
   - å¯èµ‹æœ‰æœº: é‡ç‚¹å…³æ³¨æœ‰æœºè®¤è¯ã€å¤©ç„¶çº¯å‡€ç­‰ç‰¹ç‚¹
   - é€šç”¨ç‰¹å¾: æƒ æ°å“ç‰Œã€ç§‘å­¦é…æ–¹ã€è¥å…»å…¨é¢ç­‰

## ğŸ“ åˆ†æç¤ºä¾‹

å¯¹äºæ–‡æœ¬ï¼š"æƒ æ°å¯èµ‹è•´é†‡è¥å…»ä¸°å¯Œå«HMOï¼Œå®å®è‡ªæ„ˆåŠ›ç›´æ¥å¼€æŒ‚ï¼Œå†åŠ ä¸Šç‘å£«A2å¥¶æºæ›´å¥½å¸æ”¶ï¼Œç°åœ¨é™æ—¶ä¼˜æƒ "
åº”è¯¥åˆ†æä¸ºï¼š
- ç‰‡æ®µ1: "æƒ æ°å¯èµ‹è•´é†‡è¥å…»ä¸°å¯Œå«HMO" â†’ ğŸ¼ äº§å“ä»‹ç»
- ç‰‡æ®µ2: "å®å®è‡ªæ„ˆåŠ›ç›´æ¥å¼€æŒ‚" â†’ ğŸŒŸ ä½¿ç”¨æ•ˆæœ  
- ç‰‡æ®µ3: "å†åŠ ä¸Šç‘å£«A2å¥¶æºæ›´å¥½å¸æ”¶" â†’ ğŸ¼ äº§å“ä»‹ç»
- ç‰‡æ®µ4: "ç°åœ¨é™æ—¶ä¼˜æƒ " â†’ ğŸ ä¿ƒé”€æœºåˆ¶

## ğŸ“ è¾“å‡ºæ ¼å¼

è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹JSONæ ¼å¼è¾“å‡ºï¼Œä¸è¦æ·»åŠ ä»»ä½•è§£é‡Šæ–‡å­—ï¼š

```json
{{
    "semantic_segments": [
        {{
            "text": "è¯­ä¹‰ç‰‡æ®µæ–‡æœ¬",
            "category": "ç±»åˆ«æ ‡ç­¾",
            "confidence": 0.85,
            "start_word_index": 0,
            "end_word_index": 15,
            "reasoning": "åˆ†ç±»ä¾æ®å’Œè½¬æ¢ç‚¹åˆ†æï¼ˆå¦‚æœ‰æ–°è¯æ±‡è¯·è¯´æ˜æ¨ç†è¿‡ç¨‹ï¼‰"
        }}
    ],
    "analysis_summary": {{
        "total_segments": æ•°å­—,
        "category_distribution": {{{category_dist_example}}},
        "overall_confidence": 0.85
    }}
}}
```

## ğŸ“ å¾…åˆ†ææ–‡æœ¬ï¼š

{text}

**è¯·è¿›è¡Œè¶…ç²¾ç»†åˆ†å‰²ï¼Œé€‚åº”å„ç§å¯èµ‹äº§å“å†…å®¹ï¼Œè¯†åˆ«æ‰€æœ‰è¯­ä¹‰è½¬æ¢ç‚¹ï¼š**"""

    def _classify_sentence_by_keywords(self, sentence: str) -> str:
        """åŸºäºå…³é”®è¯åˆ†ç±»å¥å­ï¼ˆå¢å¼ºæ³›åŒ–ç‰ˆï¼‰"""
        
        # ğŸª é’©å­å…³é”®è¯ï¼ˆæ‰©å±•ç‰ˆï¼‰
        hook_keywords = ["å“­é—¹", "å¤œé†’", "ç¡ä¸å¥½", "ä¸å®‰", "å›°æ‰°", "æ‹…å¿ƒ", "ç„¦è™‘", "é—®é¢˜", "è–„å¼±æœŸ", "æ¶ˆè€—å…‰", "å¦ˆå¦ˆä¸çŸ¥é“", "90%", "ä½ å®¶å®å®", "é¢‘ç¹", "åŸæ¥", "ç ”ç©¶å‘ç°", "ä¸“å®¶è¯´", "ç§‘å­¦è¯æ˜", "å‘ç°", "è°ƒæŸ¥", "ç»Ÿè®¡", "æ•°æ®"]
        
        # ğŸ¼ äº§å“ä»‹ç»å…³é”®è¯ï¼ˆæ‰©å±•ç‰ˆï¼‰
        product_keywords = ["æƒ æ°", "å¯èµ‹", "è•´é†‡", "è•´æ·³", "æ°´å¥¶", "æœ‰æœº", "å¥¶ç²‰", "é…æ–¹", "è¥å…»", "æˆåˆ†", "æ¯ä¹³", "ä½èšç³–", "è›‹ç™½", "A2", "å¥¶æº", "HMO", "OPN", "2'-FL", "Î±-ä¹³æ¸…è›‹ç™½", "DHA", "ARA", "ç›Šç”ŸèŒ", "æ ¸è‹·é…¸", "å¶é»„ç´ ", "èƒ†ç¢±", "ç‰›ç£ºé…¸", "é“", "é”Œ", "é’™", "ç»´ç”Ÿç´ ", "äº²å’Œ", "å¤©ç„¶", "çº¯å‡€", "åŸè£…è¿›å£", "ç‘å£«", "çˆ±å°”å…°", "å“ç‰Œ", "ç§‘æŠ€", "åˆ›æ–°", "ä¸“åˆ©"]
        
        # ğŸŒŸ ä½¿ç”¨æ•ˆæœå…³é”®è¯ï¼ˆæ‰©å±•ç‰ˆï¼‰
        effect_keywords = ["è‡ªæ„ˆåŠ›", "å¼€æŒ‚", "å¹³ç¨³", "åº¦è¿‡", "æ›´å¥½", "å¸æ”¶", "å¥åº·", "æ´»æ³¼", "æ•ˆæœ", "æ”¹å–„", "ç›´æ¥", "èƒ½è®©", "å¥½å¸æ”¶", "å¹³ç¨³åº¦è¿‡", "ä¿æŠ¤åŠ›", "æŠµæŠ—åŠ›", "å…ç–«åŠ›", "èªæ˜", "æ™ºåŠ›", "å‘è‚²", "æˆé•¿", "å¼ºå£®", "æ¶ˆåŒ–å¥½", "ä¸ä¸Šç«", "ä¸ä¾¿ç§˜", "ç¡å¾—é¦™", "é•¿å¾—å¿«", "èªæ˜ä¼¶ä¿", "ååº”å¿«", "è®°å¿†åŠ›", "å­¦ä¹ èƒ½åŠ›"]
        
        # ğŸ ä¿ƒé”€æœºåˆ¶å…³é”®è¯ï¼ˆæ‰©å±•ç‰ˆï¼‰
        promotion_keywords = ["é™æ—¶", "ä¸“äº«", "è¯•å–", "å¤§ä¿ƒ", "å›¤", "ä¼˜æƒ ", "æ´»åŠ¨", "ç¦åˆ©", "æŠ˜æ‰£", "æ»¡å‡", "ä¹°èµ ", "ç¤¼ç›’", "è¯•ç”¨è£…", "æ–°å®¢", "ä¼šå‘˜", "ç§¯åˆ†", "è¿”ç°", "åŒ…é‚®", "å®¢æœ", "å’¨è¯¢", "ä¿éšœ", "æ­£å“", "æˆæƒ", "å®˜æ–¹", "è´­ä¹°", "ä¸‹å•", "ç«‹å³", "é©¬ä¸Š", "ç°åœ¨", "ä»·æ ¼"]
        
        sentence_lower = sentence.lower()
        
        # ç»Ÿè®¡å„ç±»å…³é”®è¯å‡ºç°æ¬¡æ•°
        hook_count = sum(1 for kw in hook_keywords if kw in sentence)
        product_count = sum(1 for kw in product_keywords if kw in sentence)
        effect_count = sum(1 for kw in effect_keywords if kw in sentence)
        promotion_count = sum(1 for kw in promotion_keywords if kw in sentence)
        
        # é€‰æ‹©å¾—åˆ†æœ€é«˜çš„ç±»åˆ«
        scores = {
            "ğŸª é’©å­": hook_count,
            "ğŸ¼ äº§å“ä»‹ç»": product_count,
            "ğŸŒŸ ä½¿ç”¨æ•ˆæœ": effect_count,
            "ğŸ ä¿ƒé”€æœºåˆ¶": promotion_count
        }
        
        max_category = max(scores, key=scores.get)
        
        # å¦‚æœæ²¡æœ‰æ˜æ˜¾å…³é”®è¯ï¼Œæ ¹æ®ä½ç½®æ¨æ–­
        if scores[max_category] == 0:
            if "90%" in sentence or "å¦ˆå¦ˆä¸çŸ¥é“" in sentence:
                return "ğŸª é’©å­"
            elif "æƒ æ°" in sentence or "å¥¶ç²‰" in sentence:
                return "ğŸ¼ äº§å“ä»‹ç»"
            elif "é™æ—¶" in sentence or "è¯•å–" in sentence:
                return "ğŸ ä¿ƒé”€æœºåˆ¶"
            else:
                return "ğŸ¼ äº§å“ä»‹ç»"  # é»˜è®¤
        
        return max_category
    
    def _split_by_semantic_keywords(self, text: str) -> List[Dict[str, Any]]:
        """åŸºäºé¢†åŸŸé…ç½®çš„è¯­ä¹‰å…³é”®è¯æ™ºèƒ½åˆ†å‰²æ–‡æœ¬"""
        segments = []
        
        # ğŸ¯ ä»é¢†åŸŸé…ç½®è·å–è¯­ä¹‰è¾¹ç•Œ
        semantic_boundaries = self.domain_config.get("semantic_boundaries", [])
        categories = self.domain_config.get("categories", {})
        
        import re
        
        # æŸ¥æ‰¾æ‰€æœ‰è¯­ä¹‰è¾¹ç•Œç‚¹
        boundary_points = []
        for boundary in semantic_boundaries:
            matches = re.finditer(boundary['pattern'], text, re.IGNORECASE)
            for match in matches:
                boundary_points.append({
                    'position': match.start(),
                    'match_text': match.group(),
                    'from_category': boundary['from_category'],
                    'to_category': boundary['to_category'],
                    'description': boundary.get('description', '')
                })
        
        # æŒ‰ä½ç½®æ’åºè¾¹ç•Œç‚¹
        boundary_points.sort(key=lambda x: x['position'])
        
        # åŸºäºè¾¹ç•Œç‚¹åˆ†å‰²æ–‡æœ¬
        current_pos = 0
        # é»˜è®¤å¼€å§‹ç±»åˆ«ä¸ºç¬¬ä¸€ä¸ªå®šä¹‰çš„ç±»åˆ«
        current_category = list(categories.keys())[0] if categories else 'æœªåˆ†ç±»'
        
        for i, boundary in enumerate(boundary_points):
            # æ·»åŠ è¾¹ç•Œå‰çš„ç‰‡æ®µ
            if boundary['position'] > current_pos:
                segment_text = text[current_pos:boundary['position']].strip()
                if segment_text:
                    segments.append({
                        'text': segment_text,
                        'category': current_category,
                        'confidence': 0.88,
                        'start_word_index': self._text_to_word_index(text, current_pos),
                        'end_word_index': self._text_to_word_index(text, boundary['position']),
                        'reasoning': f'åŸºäºè¯­ä¹‰è¾¹ç•Œè¯†åˆ«ä¸º{current_category}: {boundary.get("description", "")}'
                    })
            
            # æ›´æ–°å½“å‰ç±»åˆ«
            current_category = boundary['to_category']
            current_pos = boundary['position']
        
        # æ·»åŠ æœ€åä¸€ä¸ªç‰‡æ®µ
        if current_pos < len(text):
            segment_text = text[current_pos:].strip()
            if segment_text:
                segments.append({
                    'text': segment_text,
                    'category': current_category,
                    'confidence': 0.86,
                    'start_word_index': self._text_to_word_index(text, current_pos),
                    'end_word_index': self._text_to_word_index(text, len(text)),
                    'reasoning': f'æœ€ç»ˆç‰‡æ®µè¯†åˆ«ä¸º{current_category}'
                })
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°æ˜ç¡®çš„è¾¹ç•Œï¼Œä½¿ç”¨å…³é”®è¯å¯†åº¦åˆ†æ
        if not segments:
            segments = self._analyze_by_keyword_density(text)
        
        logger.info(f"ğŸ¯ åŸºäº{self.domain_config['domain_name']}é…ç½®çš„è¯­ä¹‰åˆ†å‰²å®Œæˆï¼š{len(segments)}ä¸ªç‰‡æ®µ")
        return segments
    
    def _analyze_by_keyword_density(self, text: str) -> List[Dict[str, Any]]:
        """åŸºäºé¢†åŸŸé…ç½®çš„å…³é”®è¯å¯†åº¦åˆ†æè¿›è¡Œåˆ†å‰²"""
        categories = self.domain_config.get("categories", {})
        
        # è®¡ç®—æ¯ä¸ªç±»åˆ«çš„æ€»æƒé‡
        category_scores = {}
        for category, config in categories.items():
            score = 0
            keywords = config.get("keywords", [])
            weight_multiplier = config.get("weight_multiplier", 1.0)
            
            for keyword in keywords:
                count = text.count(keyword)
                score += count * weight_multiplier
            
            category_scores[category] = score
        
        # é€‰æ‹©å¾—åˆ†æœ€é«˜çš„ç±»åˆ«
        if category_scores:
            best_category = max(category_scores, key=category_scores.get)
            best_score = category_scores[best_category]
        else:
            best_category = "æœªåˆ†ç±»"
            best_score = 0
        
        # åˆ›å»ºå•ä¸ªç‰‡æ®µ
        return [{
            'text': text,
            'category': best_category,
            'confidence': 0.82,
            'start_word_index': 0,
            'end_word_index': len(text),
            'reasoning': f'åŸºäº{self.domain_config["domain_name"]}å…³é”®è¯å¯†åº¦åˆ†æè¯†åˆ«ä¸º{best_category}ï¼ˆå¾—åˆ†: {best_score}ï¼‰'
        }]
    
    def _text_to_word_index(self, full_text: str, char_pos: int) -> int:
        """å°†å­—ç¬¦ä½ç½®è½¬æ¢ä¸ºè¯ç´¢å¼•ï¼ˆç®€åŒ–ç‰ˆï¼‰"""
        # ç®€å•åœ°å‡è®¾æ¯2ä¸ªå­—ç¬¦ä¸ºä¸€ä¸ªè¯
        return max(0, char_pos // 2)
    
    def _count_categories(self, segments: List[Dict[str, Any]]) -> Dict[str, int]:
        """ç»Ÿè®¡å„ç±»åˆ«çš„ç‰‡æ®µæ•°é‡"""
        counts = {}
        for segment in segments:
            category = segment.get('category', 'ğŸ¼ äº§å“ä»‹ç»')
            counts[category] = counts.get(category, 0) + 1
        return counts
    

    
    def _extract_analysis_from_text(self, content: str, model: str) -> Dict[str, Any]:
        """ä»æ–‡æœ¬ä¸­æ™ºèƒ½æå–åˆ†æç»“æœï¼ˆå¢å¼ºç‰ˆå¤‡ç”¨æ–¹æ¡ˆï¼‰"""
        logger.info(f"ğŸ§  ä½¿ç”¨æ™ºèƒ½æå–æ¨¡å¼è§£æ{model}å“åº”...")
        
        # è¾“å‡ºå®Œæ•´å“åº”å†…å®¹ç”¨äºè°ƒè¯•
        logger.info(f"ğŸ“„ {model}å®Œæ•´å“åº”å†…å®¹:")
        logger.info(f"   {content}")
        
        # å°è¯•å¤šç§JSONæå–æ–¹æ³•
        analysis_result = None
        
        # æ–¹æ³•1: å¯»æ‰¾JSONä»£ç å—
        import re
        json_pattern = r'```json\s*(.*?)\s*```'
        json_match = re.search(json_pattern, content, re.DOTALL)
        if json_match:
            try:
                analysis_result = json.loads(json_match.group(1))
                logger.info("âœ… æ–¹æ³•1æˆåŠŸï¼šä»```json```ä»£ç å—æå–")
            except json.JSONDecodeError:
                logger.warning("âš ï¸ æ–¹æ³•1å¤±è´¥ï¼šJSONä»£ç å—æ ¼å¼é”™è¯¯")
        
        # æ–¹æ³•2: å¯»æ‰¾å¤§æ‹¬å·åŒ…å›´çš„å†…å®¹
        if not analysis_result:
            bracket_pattern = r'\{.*\}'
            bracket_match = re.search(bracket_pattern, content, re.DOTALL)
            if bracket_match:
                try:
                    analysis_result = json.loads(bracket_match.group(0))
                    logger.info("âœ… æ–¹æ³•2æˆåŠŸï¼šä»å¤§æ‹¬å·å†…å®¹æå–")
                except json.JSONDecodeError:
                    logger.warning("âš ï¸ æ–¹æ³•2å¤±è´¥ï¼šå¤§æ‹¬å·å†…å®¹ä¸æ˜¯æœ‰æ•ˆJSON")
        
        # æ–¹æ³•3: å…³é”®ä¿¡æ¯æå–ï¼ˆåŸºäºæ–‡æœ¬åˆ†æï¼‰
        if not analysis_result:
            logger.info("ğŸ¯ æ–¹æ³•3ï¼šåŸºäºå…³é”®è¯æ™ºèƒ½åˆ†æå“åº”å†…å®¹")
            analysis_result = self._intelligent_text_analysis(content, model)
        
        # å¦‚æœæ‰€æœ‰æ–¹æ³•éƒ½å¤±è´¥ï¼Œç›´æ¥æŠ›å‡ºé”™è¯¯
        if not analysis_result or not analysis_result.get('semantic_segments'):
            raise ValueError(f"âŒ {model}å“åº”è§£æå¤±è´¥ï¼šæ— æ³•æå–æœ‰æ•ˆçš„è¯­ä¹‰åˆ†æç»“æœ")
        
        # è¡¥å……æ¨¡å‹ä¿¡æ¯
        analysis_result['model'] = model
        analysis_result['raw_response'] = content
        analysis_result['extraction_method'] = 'intelligent_extraction'
        
        logger.info(f"âœ… {model}æ™ºèƒ½æå–æˆåŠŸï¼Œè·å¾—{len(analysis_result.get('semantic_segments', []))}ä¸ªç‰‡æ®µ")
        return analysis_result
    
    def _intelligent_text_analysis(self, content: str, model: str) -> Dict[str, Any]:
        """åŸºäºAIå“åº”å†…å®¹è¿›è¡Œæ™ºèƒ½åˆ†æ"""
        # æŸ¥æ‰¾ç±»åˆ«ç›¸å…³çš„å…³é”®è¯
        category_indicators = {
            'ğŸª é’©å­': ['é’©å­', 'å›°æ‰°', 'é—®é¢˜', 'è–„å¼±æœŸ', 'å¤œé†’', 'å“­é—¹', 'ä¸å®‰'],
            'ğŸ¼ äº§å“ä»‹ç»': ['äº§å“', 'ä»‹ç»', 'æƒ æ°', 'å¯èµ‹', 'é…æ–¹', 'æˆåˆ†', 'è¥å…»', 'æ¯ä¹³', 'ä½èšç³–'],
            'ğŸŒŸ ä½¿ç”¨æ•ˆæœ': ['æ•ˆæœ', 'è‡ªæ„ˆåŠ›', 'å¼€æŒ‚', 'å¹³ç¨³', 'åº¦è¿‡', 'æ”¹å–„', 'å¥åº·', 'å¸æ”¶'],
            'ğŸ ä¿ƒé”€æœºåˆ¶': ['ä¿ƒé”€', 'é™æ—¶', 'ä¸“äº«', 'è¯•å–', 'å¤§ä¿ƒ', 'å›¤', 'ä¼˜æƒ ', 'æ´»åŠ¨']
        }
        
        # åˆ†æå“åº”ä¸­æåˆ°çš„ç±»åˆ«
        detected_segments = []
        segment_id = 0
        
        # æŒ‰æ®µè½åˆ†æ
        paragraphs = content.split('\n')
        current_text = ""
        
        for paragraph in paragraphs:
            paragraph = paragraph.strip()
            if not paragraph:
                continue
            
            # æ£€æŸ¥æ˜¯å¦åŒ…å«ç±»åˆ«æŒ‡ç¤ºå™¨
            detected_category = None
            max_score = 0
            
            for category, keywords in category_indicators.items():
                score = sum(1 for keyword in keywords if keyword in paragraph)
                if score > max_score:
                    max_score = score
                    detected_category = category
            
            # å¦‚æœæ£€æµ‹åˆ°ç±»åˆ«æˆ–ç§¯ç´¯äº†è¶³å¤Ÿçš„æ–‡æœ¬ï¼Œåˆ›å»ºç‰‡æ®µ
            if detected_category or len(current_text) > 50:
                if current_text.strip():
                    final_category = detected_category or 'ğŸ¼ äº§å“ä»‹ç»'
                    detected_segments.append({
                        'text': current_text.strip(),
                        'category': final_category,
                        'confidence': 0.78,
                        'start_word_index': segment_id * 30,
                        'end_word_index': (segment_id + 1) * 30,
                        'reasoning': f'AI{model}æ™ºèƒ½åˆ†æè¯†åˆ«ä¸º{final_category}'
                    })
                    segment_id += 1
                
                current_text = paragraph if detected_category else ""
            else:
                current_text += paragraph + " "
        
        # å¤„ç†æœ€åä¸€ä¸ªç‰‡æ®µ
        if current_text.strip():
            detected_segments.append({
                'text': current_text.strip(),
                'category': 'ğŸ¼ äº§å“ä»‹ç»',
                'confidence': 0.75,
                'start_word_index': segment_id * 30,
                'end_word_index': (segment_id + 1) * 30,
                'reasoning': f'AI{model}æ™ºèƒ½åˆ†ææœ€ç»ˆç‰‡æ®µ'
            })
        
        # å¦‚æœæ²¡æœ‰æ£€æµ‹åˆ°æœ‰æ„ä¹‰çš„ç‰‡æ®µï¼Œè¿”å›ç©ºç»“æœ
        if not detected_segments:
            return {}
        
        # æ„å»ºåˆ†æç»“æœ
        category_distribution = {}
        for segment in detected_segments:
            category = segment['category']
            category_distribution[category] = category_distribution.get(category, 0) + 1
        
        return {
            'semantic_segments': detected_segments,
            'analysis_summary': {
                'total_segments': len(detected_segments),
                'category_distribution': category_distribution,
                'overall_confidence': 0.77
            }
        }
    

    

    
    def _create_semantic_segments(self, word_timestamps: List[WordTimestamp], analysis: Dict[str, Any]) -> List[SemanticSegment]:
        """åŸºäºè¯­ä¹‰åˆ†æç»“æœåˆ›å»ºè¯­ä¹‰ç‰‡æ®µï¼ˆä¿®å¤æ—¶é—´æˆ³é‡å é—®é¢˜ï¼‰"""
        semantic_segments = []
        
        try:
            if not analysis or not analysis.get('semantic_segments'):
                raise ValueError("âŒ AIè¯­ä¹‰åˆ†æç»“æœä¸ºç©ºï¼Œæ— æ³•åˆ›å»ºè¯­ä¹‰ç‰‡æ®µ")
            
            segment_id = 1
            last_end_time = 0.0  # è¿½è¸ªä¸Šä¸€ä¸ªç‰‡æ®µçš„ç»“æŸæ—¶é—´ï¼Œé¿å…é‡å 
            
            for segment_data in analysis['semantic_segments']:
                start_idx = segment_data.get('start_word_index', 0)
                end_idx = segment_data.get('end_word_index', len(word_timestamps) - 1)
                
                # ç¡®ä¿ç´¢å¼•æœ‰æ•ˆ
                start_idx = max(0, min(start_idx, len(word_timestamps) - 1))
                end_idx = max(start_idx, min(end_idx, len(word_timestamps) - 1))
                
                segment_words = word_timestamps[start_idx:end_idx + 1]
                if not segment_words:
                    continue
                
                # ğŸ”§ ä¿®å¤æ—¶é—´æˆ³é‡å é—®é¢˜
                start_time = segment_words[0].start_time
                end_time = segment_words[-1].end_time
                
                # ç¡®ä¿ä¸ä¸å‰ä¸€ä¸ªç‰‡æ®µé‡å 
                if start_time < last_end_time:
                    start_time = last_end_time  # è°ƒæ•´å¼€å§‹æ—¶é—´åˆ°å‰ä¸€ä¸ªç‰‡æ®µç»“æŸæ—¶é—´
                
                # ç¡®ä¿ç‰‡æ®µè‡³å°‘æœ‰0.1ç§’çš„æœ€å°é•¿åº¦
                if end_time <= start_time:
                    end_time = start_time + 0.1
                
                semantic_segments.append(SemanticSegment(
                    id=segment_id,
                    start_time=start_time,
                    end_time=end_time,
                    text=segment_data.get('text', ''),
                    semantic_label=segment_data.get('category', 'æœªåˆ†ç±»'),
                    confidence=segment_data.get('confidence', 0.5),
                    words=segment_words
                ))
                
                last_end_time = end_time  # æ›´æ–°æœ€åç»“æŸæ—¶é—´
                segment_id += 1
            
            # ğŸ”§ åå¤„ç†ï¼šè¿›ä¸€æ­¥ä¼˜åŒ–æ—¶é—´æˆ³è¾¹ç•Œ
            semantic_segments = self._optimize_segment_boundaries(semantic_segments)
            
            return semantic_segments
            
        except Exception as e:
            logger.error(f"âŒ åˆ›å»ºè¯­ä¹‰ç‰‡æ®µå¤±è´¥: {e}")
            raise
    
    def _optimize_segment_boundaries(self, segments: List[SemanticSegment]) -> List[SemanticSegment]:
        """ä¼˜åŒ–è¯­ä¹‰ç‰‡æ®µè¾¹ç•Œï¼Œæ¶ˆé™¤é‡å å’Œç¼éš™"""
        if len(segments) <= 1:
            return segments
        
        optimized_segments = []
        
        for i, segment in enumerate(segments):
            # å¤åˆ¶å½“å‰ç‰‡æ®µ
            new_segment = SemanticSegment(
                id=segment.id,
                start_time=segment.start_time,
                end_time=segment.end_time,
                text=segment.text,
                semantic_label=segment.semantic_label,
                confidence=segment.confidence,
                words=segment.words
            )
            
            # å¦‚æœä¸æ˜¯ç¬¬ä¸€ä¸ªç‰‡æ®µï¼Œç¡®ä¿ä¸å‰ä¸€ä¸ªç‰‡æ®µæ— ç¼è¿æ¥
            if i > 0:
                prev_segment = optimized_segments[-1]
                
                # æ¶ˆé™¤é‡å ï¼šå½“å‰ç‰‡æ®µå¼€å§‹æ—¶é—´ä¸èƒ½æ—©äºå‰ä¸€ä¸ªç‰‡æ®µç»“æŸæ—¶é—´
                if new_segment.start_time < prev_segment.end_time:
                    new_segment.start_time = prev_segment.end_time
                
                # æ¶ˆé™¤è¿‡å¤§ç¼éš™ï¼šå¦‚æœé—´éš”å¤§äº0.5ç§’ï¼Œè°ƒæ•´è¾¹ç•Œ
                gap = new_segment.start_time - prev_segment.end_time
                if gap > 0.5:
                    # å°†ç¼éš™å¹³å‡åˆ†é…ç»™ä¸¤ä¸ªç‰‡æ®µ
                    middle_time = prev_segment.end_time + gap / 2
                    prev_segment.end_time = middle_time
                    new_segment.start_time = middle_time
                    # æ›´æ–°å·²æ·»åŠ çš„å‰ä¸€ä¸ªç‰‡æ®µ
                    optimized_segments[-1] = prev_segment
            
            # ç¡®ä¿ç‰‡æ®µæœ€å°é•¿åº¦
            if new_segment.end_time <= new_segment.start_time:
                new_segment.end_time = new_segment.start_time + 0.1
            
            optimized_segments.append(new_segment)
        
        logger.info(f"ğŸ”§ è¾¹ç•Œä¼˜åŒ–å®Œæˆï¼š{len(optimized_segments)}ä¸ªç‰‡æ®µæ— é‡å ")
        return optimized_segments
    
    def _create_default_segments(self, word_timestamps: List[WordTimestamp]) -> List[SemanticSegment]:
        """åˆ›å»ºé»˜è®¤çš„è¯­ä¹‰ç‰‡æ®µï¼ˆå¤‡ç”¨æ–¹æ¡ˆï¼‰"""
        # ç®€å•çš„åŸºäºæ—¶é—´é•¿åº¦çš„åˆ†å‰²
        segments = []
        segment_duration = 5.0  # 5ç§’ä¸€ä¸ªç‰‡æ®µ
        
        current_words = []
        current_start = 0
        segment_id = 1
        
        for word in word_timestamps:
            if not current_words:
                current_start = word.start_time
            
            current_words.append(word)
            
            # æ£€æŸ¥æ˜¯å¦éœ€è¦åˆ†å‰²
            if word.end_time - current_start >= segment_duration:
                text = ''.join([w.text for w in current_words])
                segments.append(SemanticSegment(
                    id=segment_id,
                    start_time=current_start,
                    end_time=word.end_time,
                    text=text,
                    semantic_label='ğŸ¼ äº§å“ä»‹ç»',  # é»˜è®¤æ ‡ç­¾
                    confidence=0.5,
                    words=current_words.copy()
                ))
                
                current_words = []
                segment_id += 1
        
        # å¤„ç†æœ€åä¸€ä¸ªç‰‡æ®µ
        if current_words:
            text = ''.join([w.text for w in current_words])
            segments.append(SemanticSegment(
                id=segment_id,
                start_time=current_start,
                end_time=current_words[-1].end_time,
                text=text,
                semantic_label='ğŸ¼ äº§å“ä»‹ç»',
                confidence=0.5,
                words=current_words
            ))
        
        return segments
    

    
    def export_to_srt(self, semantic_segments: List[SemanticSegment], output_path: str) -> bool:
        """å¯¼å‡ºè¯­ä¹‰ç‰‡æ®µä¸ºSRTæ–‡ä»¶"""
        try:
            srt_lines = []
            
            for segment in semantic_segments:
                # SRTæ ¼å¼ï¼šç¼–å·
                srt_lines.append(str(segment.id))
                
                # SRTæ ¼å¼ï¼šæ—¶é—´æˆ³
                start_time = self._seconds_to_srt_time(segment.start_time)
                end_time = self._seconds_to_srt_time(segment.end_time)
                srt_lines.append(f"{start_time} --> {end_time}")
                
                # SRTæ ¼å¼ï¼šæ–‡æœ¬å†…å®¹ï¼ˆåŒ…å«è¯­ä¹‰æ ‡ç­¾ï¼‰
                labeled_text = f"[{segment.semantic_label}] {segment.text}"
                srt_lines.append(labeled_text)
                
                # SRTæ ¼å¼ï¼šç©ºè¡Œåˆ†éš”ç¬¦
                srt_lines.append("")
            
            # å†™å…¥æ–‡ä»¶
            with open(output_path, 'w', encoding='utf-8') as f:
                f.write('\n'.join(srt_lines))
            
            logger.info(f"âœ… è¯­ä¹‰SRTæ–‡ä»¶å¯¼å‡ºæˆåŠŸ: {output_path}")
            return True
            
        except Exception as e:
            logger.error(f"âŒ å¯¼å‡ºSRTæ–‡ä»¶å¤±è´¥: {e}")
            return False
    
    def export_to_srt_dual_versions(self, semantic_segments: List[SemanticSegment], output_path: str) -> bool:
        """å¯¼å‡ºè¯­ä¹‰ç‰‡æ®µä¸ºåŒç‰ˆæœ¬SRTæ–‡ä»¶ï¼šå¸¦æ ‡æ³¨ç‰ˆæœ¬å’Œå¹²å‡€ç‰ˆæœ¬"""
        try:
            from pathlib import Path
            
            output_path_obj = Path(output_path)
            
            # ç”Ÿæˆä¸¤ä¸ªæ–‡ä»¶è·¯å¾„
            labeled_path = output_path  # å¸¦æ ‡æ³¨ç‰ˆæœ¬ä½¿ç”¨åŸè·¯å¾„
            clean_path = output_path_obj.parent / f"{output_path_obj.stem}_clean{output_path_obj.suffix}"
            
            # --- ç”Ÿæˆå¸¦æ ‡æ³¨ç‰ˆæœ¬ ---
            labeled_srt_lines = []
            for segment in semantic_segments:
                # SRTæ ¼å¼ï¼šç¼–å·
                labeled_srt_lines.append(str(segment.id))
                
                # SRTæ ¼å¼ï¼šæ—¶é—´æˆ³
                start_time = self._seconds_to_srt_time(segment.start_time)
                end_time = self._seconds_to_srt_time(segment.end_time)
                labeled_srt_lines.append(f"{start_time} --> {end_time}")
                
                # SRTæ ¼å¼ï¼šæ–‡æœ¬å†…å®¹ï¼ˆåŒ…å«è¯­ä¹‰æ ‡ç­¾ï¼‰
                labeled_text = f"[{segment.semantic_label}] {segment.text}"
                labeled_srt_lines.append(labeled_text)
                
                # SRTæ ¼å¼ï¼šç©ºè¡Œåˆ†éš”ç¬¦
                labeled_srt_lines.append("")
            
            # --- ç”Ÿæˆå¹²å‡€ç‰ˆæœ¬ ---
            clean_srt_lines = []
            for segment in semantic_segments:
                # SRTæ ¼å¼ï¼šç¼–å·
                clean_srt_lines.append(str(segment.id))
                
                # SRTæ ¼å¼ï¼šæ—¶é—´æˆ³
                start_time = self._seconds_to_srt_time(segment.start_time)
                end_time = self._seconds_to_srt_time(segment.end_time)
                clean_srt_lines.append(f"{start_time} --> {end_time}")
                
                # SRTæ ¼å¼ï¼šæ–‡æœ¬å†…å®¹ï¼ˆä¸åŒ…å«è¯­ä¹‰æ ‡ç­¾ï¼‰
                clean_text = segment.text
                clean_srt_lines.append(clean_text)
                
                # SRTæ ¼å¼ï¼šç©ºè¡Œåˆ†éš”ç¬¦
                clean_srt_lines.append("")
            
            # --- å†™å…¥å¸¦æ ‡æ³¨ç‰ˆæœ¬æ–‡ä»¶ ---
            with open(labeled_path, 'w', encoding='utf-8') as f:
                f.write('\n'.join(labeled_srt_lines))
            
            # --- å†™å…¥å¹²å‡€ç‰ˆæœ¬æ–‡ä»¶ ---
            with open(clean_path, 'w', encoding='utf-8') as f:
                f.write('\n'.join(clean_srt_lines))
            
            logger.info(f"âœ… å¸¦æ ‡æ³¨ç‰ˆæœ¬SRTæ–‡ä»¶å¯¼å‡ºæˆåŠŸ: {labeled_path}")
            logger.info(f"âœ… å¹²å‡€ç‰ˆæœ¬SRTæ–‡ä»¶å¯¼å‡ºæˆåŠŸ: {clean_path}")
            return True
            
        except Exception as e:
            logger.error(f"âŒ å¯¼å‡ºåŒç‰ˆæœ¬SRTæ–‡ä»¶å¤±è´¥: {e}")
            return False
    
    def _seconds_to_srt_time(self, seconds: float) -> str:
        """å°†ç§’æ•°è½¬æ¢ä¸ºSRTæ—¶é—´æ ¼å¼"""
        hours = int(seconds // 3600)
        minutes = int((seconds % 3600) // 60)
        secs = int(seconds % 60)
        milliseconds = int((seconds % 1) * 1000)
        
        return f"{hours:02d}:{minutes:02d}:{secs:02d},{milliseconds:03d}"


def main():
    """å‘½ä»¤è¡Œæ¥å£"""
    import argparse
    
    parser = argparse.ArgumentParser(description="è¯çº§è¯­ä¹‰åˆ†å‰²å™¨")
    parser.add_argument("srt_file", help="è¾“å…¥SRTæ–‡ä»¶è·¯å¾„")
    parser.add_argument("-o", "--output", help="è¾“å‡ºSRTæ–‡ä»¶è·¯å¾„")
    parser.add_argument("--deepseek-key", help="DeepSeek APIå¯†é’¥")
    parser.add_argument("--claude-key", help="Claude APIå¯†é’¥")
    
    args = parser.parse_args()
    
    # é…ç½®æ—¥å¿—
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s'
    )
    
    splitter = WordLevelSemanticSplitter(
        deepseek_api_key=args.deepseek_key,
        claude_api_key=args.claude_key
    )
    
    # åˆ†æSRTæ–‡ä»¶
    segments = splitter.analyze_srt_with_word_timestamps(args.srt_file)
    
    if segments:
        output_path = args.output or args.srt_file.replace('.srt', '_semantic.srt')
        success = splitter.export_to_srt(segments, output_path)
        
        if success:
            print(f"âœ… è¯­ä¹‰åˆ†å‰²å®Œæˆï¼è¾“å‡ºæ–‡ä»¶: {output_path}")
        else:
            print("âŒ å¯¼å‡ºå¤±è´¥ï¼")
            return 1
    else:
        print("âŒ è¯­ä¹‰åˆ†æå¤±è´¥ï¼")
        return 1
    
    return 0


if __name__ == "__main__":
    import sys
    sys.exit(main()) 